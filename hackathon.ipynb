{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to LangChain (CoLab version)\n",
    "\n",
    "LangChain is a Python framework for developing applications using language models. It abstracts the connection between applications and LLMs, allowing a loose coupling between code and specific providers like GoogleLLM, Anthropic, etc.\n",
    "\n",
    "This guide details how to get started with LangChain, and walks-through setting up a Q&A over Documentation example. The files for this guide can be found here.\n",
    "\n",
    "## Creating a knowledge worker\n",
    "Instead of training LLMs using your own data (ie. fine-tuning), it is far easier and more effective to adapt the LLM to your use-case by prompt engineering only (ie. tuning).\n",
    "\n",
    "A custom knowledge worker (an LLM app which only has access to specific knowledge, such as technical documentation), is described in two stages: 1. knowledge embedding and 2. LLM Q&A.\n",
    "\n",
    "![A typical ingestion chain](assets/typical-ingestion-chain.png)\n",
    "\n",
    "First, documents (website pages, Word documents, databases, Powerpoints, etc.) are loaded and split into chunks. Fragmenting is important for three reasons - first, there are technical restrictions on how much data (tokens) can be fed into an LLM at once, meaning all context + prompt engineering + chat history + new query must fit within the token limit (e.g.: ~4096 tokens for GoogleLLM). Second, even if it was possible to provide entire documents as context for a query, most LLM APIs operate on a per-token pricing model, meaning it is cost-effective to limit the size / amount of data provided to the LLM per query. Third, contextually information should be relevant to the user query, meaning it is optimial to provide only relevant snippets from documents, making the answer more relevant whilst saving costs as per (1) and (2).\n",
    "\n",
    "Next, these document shards are embedded within a vector store. Embedding a document means to align it within a mutli-dimension space, which can then be searched according to user queries to find relevant documents. Document relevancy scoring can be as simple as a K-neighbours search, since embedded documents with similarity (as percieved by the LLM embedding model) will be proximate within the search space.\n",
    "\n",
    "![A typical query chain](assets/typical-query-chain.png)\n",
    "\n",
    "Once the vector store is created, a user can query the knowledge base using natural language questions. Relevant documents related to the query are found in the vector store by embedding the user query and finding local documents. These snippets of text are provided to the LLM (alongside the user query, chat history, prompt engineering, etc.) which parses the information to generate an answer.\n",
    "\n",
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from getpass import getpass\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "import gradio as gr\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredHTMLLoader\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from pydantic import BaseModel, root_validator\n",
    "from vertexai.preview.language_models import TextGenerationModel, TextEmbeddingModel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the program\n",
    "\n",
    "Run this notebook using poetry:\n",
    "```bash\n",
    "poetry shell\n",
    "poetry run jupyter notebook langchain_demo.ipynb\n",
    "```\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain chromadb tiktoken gradio unstructured tqdm getpass google-cloud-aiplatform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM API Token\n",
    "To complete this demo you will need an [GoogleLLM API Key](https://platform.GoogleLLM.com/account/api-keys).\n",
    "Run the code block below to set a temporary environment variable containing your API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/zacharysmith/.pyenv/versions/3.10.10/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/zacharysmith/.pyenv/versions/3.10.10/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Prompt notebook user for their API key\n",
    "GoogleLLM_API_KEY = getpass(\"GoogleLLM API Key:\")\n",
    "# Save that key as an environment variable for later use.\n",
    "os.environ[\"GoogleLLM_API_KEY\"] = GoogleLLM_API_KEY\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01: Q&A over Documentation with GoogleLLM\n",
    "\n",
    "Creating a custom knowledge worker is the ‚ÄúHello World!‚Äù of LLMOps. Loading documents, creating embeddings, storing in a vector database and using an LLM to answer queries with knowledge from that database can be achieved in a few lines of Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loading\n",
    "First, we need to select our documentation. LangChain supports [numerous methods](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) for loading documents. As we will be constructing our knowledge worker using website data, we first download the files locally using `wget`. Note: replace `https://datatonic.com/` with any domain to build a knowledge worker specific to your use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -r -A.html https://datatonic.com/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how filepaths to the `.html` files form proper URL paths, we'll use this later to reference our answers with proper hyperlinks.\n",
    "\n",
    "The downloaded website can then be loaded as documents using the LangChain [`DirectoryLoader`](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/directory_loader.html) and [`UnstructuredHTMLLoader`](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/html.html) loaders.\n",
    "\n",
    "We load the entire collection of documents at once using this method, but it is also possible to load and embed documents iteratively, as we will do in task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(source_dir):\n",
    "    # Load the documentation using a HTML parser\n",
    "    loader = DirectoryLoader(\n",
    "        source_dir,\n",
    "        glob=\"**/*.html\",\n",
    "        loader_cls=UnstructuredHTMLLoader,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to build a knowledge worker with another document type, for instance [Microsoft Word](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/microsoft_word.html) documents, you would update the `load_documents()` function according to the documentation for that document type loader.\n",
    "\n",
    "E.g.:\n",
    "```python\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "\n",
    "def load_documents():\n",
    "    # Load the documentation using a Microsoft Word parser\n",
    "    loader = Docx2txtLoader(\"example_data/fake.docx\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "```\n",
    "\n",
    "### Creating or loading embeddings\n",
    "Creating embeddings each time we use our app is time-consuming and expensive. By persisting the vector store database after embedding, we can load the saved embeddings for use in another session.\n",
    "\n",
    "Also note the use of the `GoogleLLMEmbeddings()` model. There are [multiple](https://python.langchain.com/en/latest/modules/models/text_embedding.html?highlight=embedding) text embedding models available, many of which can be directly substituted here. Remember to add additional environment variables for different API keys, as per each model's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "\n",
    "class GooglePalmEmbeddings(BaseModel, Embeddings):\n",
    "    model_name: str = \"textembedding-gecko@001\"\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validates that the python package exists in environment.\"\"\"\n",
    "        \n",
    "        values[\"client\"] = TextEmbeddingModel.from_pretrained(\n",
    "            values[\"model_name\"])\n",
    "        return values\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of strings.\n",
    "        Args:\n",
    "            texts: List[str] The list of strings to embed.\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        logging.info(\n",
    "            \"API calls restricted to 5 instances per call, batching documents to embed...\"\n",
    "        )\n",
    "        texts_batched = [\n",
    "            texts[i: i + BATCH_SIZE] for i in range(0, len(texts), BATCH_SIZE)\n",
    "        ]\n",
    "        embeddings = [self.client.get_embeddings(x) for x in texts_batched]\n",
    "        logging.info(\"Embeddings received!\")\n",
    "        return [el.values for batch in embeddings for el in batch]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a text.\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "        Returns:\n",
    "            Embedding for the text.\n",
    "        \"\"\"\n",
    "        embeddings = self.client.get_embeddings([text])\n",
    "        return embeddings[0].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(persist_dir, texts):\n",
    "    # We use GoogleLLM embeddings model, however other models can be substituted here\n",
    "    embeddings = GooglePalmEmbeddings()()\n",
    "    # We create a vector store database relating documents to embeddings\n",
    "    # This embedding database is used to relate user queries to relevant documentation\n",
    "    vector_store = Chroma.from_documents(\n",
    "        persist_directory=persist_dir,\n",
    "        documents=texts,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "\n",
    "    vector_store.persist()\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(persist_dir):\n",
    "    # We use GoogleLLM embeddings model, however other models can be substituted here\n",
    "    embeddings = GooglePalmEmbeddings()()\n",
    "\n",
    "    # Creating embeddings with each re-run is highly inefficient and costly.\n",
    "    # We instead aim to embed once, then load these embeddings from storage.\n",
    "    vector_store = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=persist_dir,\n",
    "    )\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the vectorstore embedding database has been created, load it\n",
    "persist_dir = \"chromadb\"\n",
    "if os.path.isdir(persist_dir):\n",
    "    print(f\"Loading {persist_dir} as vector store\")\n",
    "    vector_store = load_embeddings(persist_dir=persist_dir)\n",
    "# If it exists, create it\n",
    "else:\n",
    "    print(f\"Creating new vector store in dir {persist_dir}\")\n",
    "    documents = load_documents(source_dir=\"datatonic.com\")\n",
    "\n",
    "    # Individual documents will often exceed the 4096 token limit for GPT-3.\n",
    "    # By splitting documents into chunks of 1000 token\n",
    "    # These chunks fit into the token limit alongside the user prompt\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    vector_store = embed_documents(persist_dir=persist_dir, texts=texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Conversational Q&A Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'k' value indicates the number of sources to use per query.\n",
    "# 'k' as in 'k-nearest-neighbours' to the query in the embedding space.\n",
    "# 'temperature' is the degree of randomness introduced into the LLM response.\n",
    "k = 2\n",
    "temperature = 0.0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt engineering\n",
    "Prompt engineering is a method of zero-shot fine-tuning for large language models. By prompting a LLM with contextual information about its purpose, the model can simulate a variety of situations, such as a customer assistant chatbot, a document summariser, a translator, etc.\n",
    "\n",
    "In this use-case, we prompt our model to respond as a conversational Q&A chatbot. Prompt engineering can be especially useful for introducing guard rails to an application - in this template we tell the model to not respond to queries it lacks the information to answer, as users will trust the application to provide factual replies, so rejecting a query is preferable to outputting false information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "You are a helpful chatbot designed to perform Q&A on a set of documents.\n",
    "Always respond to users with friendly and helpful messages.\n",
    "Your goal is to answer user questions using relevant sources.\n",
    "\n",
    "You were developed by Datatonic, and are powered by Google's PaLM-2 model.\n",
    "\n",
    "In addition to your implicit model world knowledge, you have access to the following data sources:\n",
    "- Company documentation.\n",
    "\n",
    "If a user query is too vague, ask for more information.\n",
    "If insufficient information exists to answer a query, respond with \"I don't know\".\n",
    "NEVER make up information.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleLLM(LLM):\n",
    "    \n",
    "    # Model name options {text-bison-alpha, text-bison@001}\n",
    "    model_name: str=\"text-bison@001\"\n",
    "    _llm = TextGenerationModel.from_pretrained(model_name)\n",
    "    max_output_tokens:int = 256\n",
    "    temperature:float = 0.3\n",
    "    top_p:float = 0.8\n",
    "    top_k:int = 40\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm\"\"\"\n",
    "        return \"google\"\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_output_tokens\": self.max_output_tokens,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"top_k\": self.top_k\n",
    "        }\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        text = str(self._llm.predict(\n",
    "            prompt, \n",
    "            max_output_tokens=self.max_output_tokens, \n",
    "            temperature=self.temperature, \n",
    "            top_p=self.top_p, \n",
    "            top_k=self.top_k,\n",
    "        ))\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vector store retriever relates queries to embedded documents\n",
    "retriever = vector_store.as_retriever(k=k)\n",
    "\n",
    "# The selected GoogleLLM model uses embedded documents related to the query\n",
    "# It parses these documents in order to answer the user question.\n",
    "# We use the GoogleLLM LLM, however other models can be substituted here\n",
    "model = GoogleLLM(temperature=temperature)\n",
    "\n",
    "# A conversation retrieval chain keeps a history of Q&A / conversation\n",
    "# This allows for contextual questions such as \"give an example of that (previous response)\".\n",
    "# The chain is also set to return the source documents used in generating an output\n",
    "# This allows for explainability behind model output.\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    condense_question_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt engineering and Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a(question: str, history: list):\n",
    "    # map history (list of lists) to expected format of chat_history (list of tuples)\n",
    "    chat_history = map(tuple, history)\n",
    "    \n",
    "    # Query the LLM to get a response\n",
    "    # First the Q&A chain will collect documents semantically similar to the question\n",
    "    # Then it will ask the LLM to use this data to answer the user question\n",
    "    # We also provide chat history as further context\n",
    "    response = qa_chain(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"chat_history\": chat_history,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Format source documents (sources of excerpts passed to the LLM) into links the user can validate\n",
    "    sources = [\n",
    "        \"[{0}]({0})\".format(doc.metadata[\"source\"])\n",
    "        for doc in response[\"source_documents\"]\n",
    "    ]\n",
    "\n",
    "    # Return the LLM answer, and list of sources used (formatted as a string)\n",
    "    return response[\"answer\"], \"\\n\\n\".join(sources)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple GradI/O UI\n",
    "As building with GradI/O is outside the scope of this workshop, a template GradI/O app has been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple GradIO app that accepts user input and queries the LLM\n",
    "# Then displays the response in a ChatBot interface, with markdown support.\n",
    "with gr.Blocks(theme=gr.themes.Base()) as demo:\n",
    "\n",
    "    def submit(msg, chatbot):\n",
    "        # First create a new entry in the conversation log\n",
    "        msg, chatbot = user(msg, chatbot)\n",
    "        # Then get the chatbot response to the user question\n",
    "        chatbot = bot(chatbot)\n",
    "        return msg, chatbot\n",
    "\n",
    "    def user(user_message, history):\n",
    "        # Return \"\" to clear the user input, and add the user question to the conversation history\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        # Get the user question from conversation history\n",
    "        user_message = history[-1][0]\n",
    "        # Get the response and sources used to answer the user question\n",
    "        bot_message, bot_sources = q_a(user_message, history[:-1])\n",
    "\n",
    "        # Using a template, format the response and sources together\n",
    "        bot_template = (\n",
    "            \"{0}\\n\\n<details><summary><b>Sources</b></summary>\\n\\n{1}</details>\"\n",
    "        )\n",
    "        # Place the response into the conversation history and return\n",
    "        history[-1][1] = bot_template.format(bot_message, bot_sources)\n",
    "        return history\n",
    "\n",
    "    # Set a page title\n",
    "    gr.Markdown(\"# Custom knowledge worker\")\n",
    "    # Create a chatbot conversation log\n",
    "    chatbot = gr.Chatbot(label=\"ü§ñ knowledge worker\")\n",
    "    # Create a textbox for user questions\n",
    "    msg = gr.Textbox(\n",
    "        label=\"üë©‚Äçüíª user input\", info=\"Query information from the custom knowledge base.\"\n",
    "    )\n",
    "\n",
    "    # Align both buttons on the same row\n",
    "    with gr.Row():\n",
    "        send = gr.Button(value=\"Send\", variant=\"primary\").style(size=\"sm\")\n",
    "        clear = gr.Button(value=\"Clear History\", variant=\"secondary\").style(size=\"sm\")\n",
    "\n",
    "    # Submit message on <enter> or clicking \"Send\" button\n",
    "    msg.submit(submit, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    send.click(submit, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "\n",
    "    # Clear chatbot history on clicking \"Clear History\" button\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# Create a queue system so multiple users can access the page at once\n",
    "demo.queue()\n",
    "# Launch the webserver locally\n",
    "demo.launch()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our demo, we can query our knowledge base for answers (code emphasis added). Since we used a `ConversationalRetrievalChain`, we can also correct the model when it gives the wrong response and prompt it to fix it‚Äôs mistake, or ask for further detail on a previous response.\n",
    "\n",
    "> üë©‚Äçüíª: how to install the gcloud cli\n",
    "> \n",
    "> ü¶ú:  You can install the gcloud CLI by running the command `$ gcloud components update`.\n",
    "> \n",
    "> üë©‚Äçüíª: that command requires gcloud to be installed, how can I install gcloud initially?\n",
    "> \n",
    "> ü¶ú:  You can install gcloud initially by running the command '`pip install google-cloud`' in your terminal.\n",
    "> \n",
    "> üë©‚Äçüíª: how can I set the target project\n",
    "> \n",
    "> ü¶ú:  You can set the target project for the gcloud CLI by using the command `$ gcloud config set project my-new-default-project`.\n",
    "> \n",
    "> üë©‚Äçüíª: what is the gcloud cli?\n",
    "> \n",
    "> ü¶ú:  The gcloud CLI is a command line interface for Google Cloud Platform services.\n",
    "> \n",
    "> üë©‚Äçüíª: explain the above in more detail\n",
    "> \n",
    "> ü¶ú:  The gcloud CLI is a tool used to authenticate and configure credentials for Google Cloud services. It can be used to change the default project ID, update components, and authenticate the CLI itself.\n",
    "\n",
    "## Task 02: Extending the knowledge base\n",
    "As mentioned, it is possible to extend a knowledge base with additional documents. This is useful for updating a knowledge base with new information without having to re-embed established knowledge from scratch.\n",
    "\n",
    "In this example, we will add an additional knowledge source to our worker, allowing it to answer queries on multiple domains. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load another set of documents\n",
    "documents = load_documents(source_dir=\"datatonic.com\")\n",
    "\n",
    "# Individual documents will often exceed the token limit.\n",
    "# By splitting documents into chunks of 1000 token\n",
    "# These chunks fit into the token limit alongside the user prompt\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add new texts to the embedding vector store\n",
    "vector_store.add_texts(texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the Q&A chain\n",
    "\n",
    "# A vector store retriever relates queries to embedded documents\n",
    "retriever = vector_store.as_retriever(k=k)\n",
    "# The selected GoogleLLM model uses embedded documents related to the query\n",
    "# It parses these documents in order to answer the user question.\n",
    "# We use the GoogleLLM LLM, however other models can be substituted here\n",
    "model = GoogleLLM(temperature=temperature)\n",
    "\n",
    "# A conversation retrieval chain keeps a history of Q&A / conversation\n",
    "# This allows for contextual questions such as \"give an example of that (previous response)\".\n",
    "# The chain is also set to return the source documents used in generating an output\n",
    "# This allows for explainability behind model output.\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    condense_question_prompt=SYSTEM_PROMPT\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you're looking to deploy a knowledge worker with several knowledge bases, a [Router Chain](https://python.langchain.com/en/latest/modules/chains/examples/multi_retrieval_qa_router.html), which combines several knowledge workers with discrete knowledge bases into a single chain which selects the best worker for the query.\n",
    "\n",
    "## Task 03: Using a different LLM (?)\n",
    "\n",
    "\n",
    "## Task 04: Loading custom documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
