{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScHDNga8s3Rw"
      },
      "source": [
        "# Generative AI Hackathon\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook walks you through the challenge of implementing a **Knowledge Worker** for your organisation using **Generative AI**!\n",
        "\n",
        "**The challenge:** Decentralized data across internal and external databases results in time wasted as workforce tries to find required information and transform into insights.\n",
        "\n",
        "‚û°Ô∏è **Your task:** Implement a knowledge worker to enable users in your company to perform Q&A, in natural language, upon a knowledge base.\n",
        "In this way, you'll centralise company data for easy access in a user-friendly manner, boosting productivity.\n",
        "As such, you'll create a knowledge worker fine-tuned to your data domain.\n",
        "This app will only have access to specific knowledge such as public data about your company available on your company's website and unstructured documents (PDF, Word, text ...).\n",
        "\n",
        "While solving the tasks as instructed in this notebook, you'll familiarise yourself with common concepts and tools for Generative AI including:\n",
        "\n",
        "- The Open-Source tool LangChain\n",
        "- Large Language Models (LLMs)\n",
        "- Text Embeddings and Vector Databases\n",
        "- Prompts and Prompt Engineering\n",
        "\n",
        "Ultimately, this notebook details how to get started with LangChain, and walks through setting up a knowledge worker on Google Cloud and Vertex AI.\n",
        "\n",
        "## Implementing a knowledge worker\n",
        "\n",
        "When creating a knowledge worker, you recall Large Language Models (LLMs) can be tuned for a variety of tasks such as text summarisation, answering questions, and generating new content (and many more!).\n",
        "When it comes to tuning approaches, you'll have the choice between:\n",
        "\n",
        "**A) Zero-shot learning:** Use LLMs directly without providing additional data or fine-tuning the model.\n",
        "\n",
        "**B) Few-shot learning:** Provide a select number of input examples when using LLM to improve the quality of outputs.\n",
        "\n",
        "**C) Model Fine-tuning:** Fine-tune certain (or additional) layers in the LLM by training the model on provided training data.\n",
        "\n",
        "Instead of training LLMs using your own data (ie. fine-tuning), it is far easier and more effective to adapt the LLM to your use-case by prompt engineering only (ie. tuning).\n",
        "Thus, methods A) and B) are more applicable for creating your first knowledge worker.\n",
        "\n",
        "A knowledge worker can be approached in two stages:\n",
        "\n",
        "1. Embedding knowledge from diverse sources.\n",
        "2. Querying a LLM which is aware of your relevant knowledge to answer questions.\n",
        "\n",
        "![A typical ingestion chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/typical-ingestion-chain.png?raw=true)\n",
        "\n",
        "First, documents (websites, Word documents, databases, Powerpoints, PDFs, etc.) are loaded and split into chunks. Fragmenting is important for three reasons:\n",
        "\n",
        "1. There are technical restrictions on how much data (tokens) can be fed into an LLM at once, meaning the context + system prompt + chat history + user prompt must fit within the token limit.\n",
        "2. Most LLM APIs operate on a per-token pricing model, meaning it is cost-effective to limit the size / amount of data provided to the LLM per query.\n",
        "3. Contextual information should be relevant to the user query, meaning it is optimal to provide only relevant snippets from documents, making the answer more relevant whilst saving costs as per (1) and (2).\n",
        "\n",
        "Next, these document shards are embedded within a vector store. Embedding a document means to align it within a mutli-dimension space, which can then be searched according to user queries to find relevant documents. Document relevancy scoring can be as simple as a K-neighbours search, since embedded documents with similarity (as percieved by the LLM embedding model) will be proximate within the search space.\n",
        "\n",
        "![A typical query chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/typical-query-chain.png?raw=true)\n",
        "\n",
        "Once the vector store is created, a user can query the knowledge base using natural language questions. Relevant documents related to the query are found in the vector store by embedding the user query and finding local documents. These snippets of text are provided to the LLM (alongside the user query, chat history, prompt engineering, etc.) which parses the information to generate an answer.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**Authenticate with Google Cloud:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wsGQRwC1s3R1"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Python dependencies:**"
      ],
      "metadata": {
        "id": "Jwkq5e5QqLRy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "02VnG-Zhs3R2"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet langchain chromadb tiktoken gradio unstructured tqdm google-cloud-aiplatform google-cloud-core"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart Python Kernel:**\n",
        "Ensure that your environment can access the newly installed dependencies."
      ],
      "metadata": {
        "id": "saLQmyJeqSM3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sVJ8C3zws3R3",
        "outputId": "d91687bd-a530-431e-f8a4-bc9f7e53680e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo3B7BfIs3R3"
      },
      "source": [
        "**Note:** If your kernel doesn't restart automatically, click the \"Restart Runtime\" button above your notebook.\n",
        "If you dont see a restart button, go to the \"Runtime\" toolbar tab then \"Restart Runtime\". After restarting, continue executing the project from below this cell.\n",
        "\n",
        "## Setup Projects and Region\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "geQ9BdrDs3R3"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"dt-langchain-apps-dev\" # @param {type:\"string\"}\n",
        "LOCATION = \"europe-west2\"  # @param {type:\"string\"}\n",
        "BUCKET = \"dt-langchain-apps-dev-zurich-hackathon\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RxkFX1ies3R3",
        "outputId": "7a8ad905-6ab3-4555-ddde-75c762a7141c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oZ4FMsxIs3R4"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TId4ggR6s3R4"
      },
      "source": [
        "## Task 01: Q&A over Documentation with GoogleLLM\n",
        "\n",
        "Creating a custom knowledge worker is similar to your first step when learning a new programming language.\n",
        "As such your first challenge is to create a ‚ÄúHello World‚Äù program, however, adapted to LLMs which is way more exciting!\n",
        "\n",
        "With a few lines of code, you'll:\n",
        "- Load documents with information about your company\n",
        "- Create text embeddings from documents\n",
        "- Storing embedding in a local database\n",
        "- Use an LLM to answer queries about your company knowledge\n",
        "\n",
        "**All of these steps can be achieved in a few lines of Python.**\n",
        "\n",
        "### Download documents\n",
        "\n",
        "First, we need to collect our documentation.\n",
        "As we will be constructing our knowledge worker using website data in this first task, we'll first download the files locally.\n",
        "\n",
        "To get started fast, we've already downloaded some sample website data upfront.\n",
        "Let's copy the website data from a public Cloud Storage bucket to your local file system:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://{BUCKET}/datatonic.com.tar.gz . && tar -xzf datatonic.com.tar.gz"
      ],
      "metadata": {
        "id": "3ahQGIVwyyNZ",
        "outputId": "9e8a81c8-75db-46ab-cc46-938a95be99f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://dt-langchain-apps-dev-zurich-hackathon/datatonic.com.tar.gz...\n",
            "/ [0 files][    0.0 B/  9.2 MiB]                                                \r/ [1 files][  9.2 MiB/  9.2 MiB]                                                \r\n",
            "Operation completed over 1 objects/9.2 MiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚û°Ô∏è Your task:**  Use `wget` to download your company's public website data your local file system.\n",
        "You can use the shell command below to download the data."
      ],
      "metadata": {
        "id": "zSZVHFyrusUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WEBSITE = \"https://datatonic.com/\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "C7Txd5EKvA9B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -r -A.html {WEBSITE}"
      ],
      "metadata": {
        "id": "9iVCFEFmu8R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure that you remember where you've stored your company's website data:"
      ],
      "metadata": {
        "id": "OasGh0hVxBjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOCAL_FOLDER = \"datatonic.com\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "q82yJPNmw1nB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MkTNas0s3R5"
      },
      "source": [
        "Note how filepaths to the `.html` files form proper URL paths, we'll use this later to reference our answers with proper hyperlinks.\n",
        "\n",
        "**üéâ Congratulations! üéâ** You've downloaded your company data. Now, you'll create create text embeddings as a next\n",
        "\n",
        "\n",
        "### Introduction to LangChain\n",
        "\n",
        "LangChain is a Python framework for developing applications using language models.\n",
        "It abstracts the connection between applications and LLMs, allowing a loose coupling between code and specific providers like Google PaLM.\n",
        "\n",
        "LangChain supports [numerous methods](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) for loading documents.\n",
        "\n",
        "The downloaded website can then be loaded as documents using the LangChain [`DirectoryLoader`](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/directory_loader.html) and [`UnstructuredHTMLLoader`](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/html.html) loaders.\n",
        "\n",
        "**‚û°Ô∏è Your task:** Read the linked resources in the *Introduction to LangChain* and study the following code cells as they provide reusable LangChain code for your knowledge worker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kjaBy8CSs3R5"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader, UnstructuredHTMLLoader\n",
        "\n",
        "def load_documents(source_dir):\n",
        "    # Load the documentation using a HTML parser\n",
        "    loader = DirectoryLoader(\n",
        "        source_dir,\n",
        "        glob=\"**/*.html\",\n",
        "        loader_cls=UnstructuredHTMLLoader,\n",
        "        show_progress=True,\n",
        "    )\n",
        "    documents = loader.load()\n",
        "\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXhTVS9ws3R5"
      },
      "source": [
        "### Creating or loading embeddings\n",
        "\n",
        "Creating embeddings each time we use our app is time-consuming and expensive.\n",
        "By persisting the vector store database after embedding, we can load the saved embeddings for use in another session.\n",
        "\n",
        "Also note the use of the `GoogleLLMEmbeddings()` model.\n",
        "There are [multiple](https://python.langchain.com/en/latest/modules/models/text_embedding.html?highlight=embedding) text embedding models available, many of which can be directly substituted here.\n",
        "Remember to add additional environment variables for different API keys, as per each model's documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Njknbfazs3R6"
      },
      "outputs": [],
      "source": [
        "PERSIST_DIR = \"chromadb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fy4mX1JCs3R6"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "\n",
        "def load_embeddings():\n",
        "    # We use GoogleLLM embeddings model, however other models can be substituted here\n",
        "    embeddings = VertexAIEmbeddings()\n",
        "\n",
        "    # Creating embeddings with each re-run is highly inefficient and costly.\n",
        "    # We instead aim to embed once, then load these embeddings from storage.\n",
        "    vector_store = Chroma(\n",
        "        embedding_function=embeddings,\n",
        "        persist_directory=PERSIST_DIR,\n",
        "    )\n",
        "\n",
        "    return vector_store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XkO3NV73s3R6"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def embed_documents(embedding, documents):\n",
        "    # Individual documents will often exceed the token limit.\n",
        "    # By splitting documents into chunks of 1000 token\n",
        "    # These chunks fit into the token limit alongside the user prompt\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=texts, embedding=embedding, persist_directory=PERSIST_DIR\n",
        "    )\n",
        "\n",
        "    # Persist the ChromaDB locally, so we can reload the script without expensively re-embedding the database\n",
        "    vector_store.persist()\n",
        "\n",
        "    return vector_store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "I6WVbC8xs3R6"
      },
      "outputs": [],
      "source": [
        "def create_embeddings(source_dir):\n",
        "    documents = load_documents(source_dir=source_dir)\n",
        "\n",
        "    # We use GoogleLLM embeddings model, however other models can be substituted here\n",
        "    embedding = VertexAIEmbeddings()\n",
        "\n",
        "    vector_store = embed_documents(embedding, documents)\n",
        "\n",
        "    return vector_store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚û°Ô∏è Your task:** Run the following cell to create the text embeddings based on your downloaded data."
      ],
      "metadata": {
        "id": "1bU-8KUjxPOH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "z2r7APATs3R6",
        "outputId": "9253dc49-e6ca-48d8-fba7-c6dd336c8c21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new vector store in dir chromadb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/157 [00:00<?, ?it/s][nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [01:05<00:00,  2.39it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# If the vectorstore embedding database has been created, load it\n",
        "if os.path.isdir(PERSIST_DIR):\n",
        "    print(f\"Loading {PERSIST_DIR} as vector store\")\n",
        "    vector_store = load_embeddings()\n",
        "# If it doesn't exist, create it\n",
        "else:\n",
        "    print(f\"Creating new vector store in dir {PERSIST_DIR}\")\n",
        "    vector_store = create_embeddings(source_dir=LOCAL_FOLDER)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üéâ Congratulations! üéâ** You've created text embeddings from your company data and stored them successfully in a local vector database.\n",
        "Now, you'll shift your focus to implementing the actual LLM by creating a chain using LangChain."
      ],
      "metadata": {
        "id": "9mXJKt22xc6N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSsi93lgs3R7"
      },
      "source": [
        "### Creating the Conversational Q&A Chain\n",
        "\n",
        "In this section, you'll create a chain which will be able to provide an answer given a question from a user.\n",
        "To understand the purpose of chains, you can read about chains in the [LangChain documentation](https://docs.langchain.com/docs/).\n",
        "\n",
        "At first, we'll initialise a few hyperparameters for the LLM models which we'll reference later on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MnGD1Mxvs3R7"
      },
      "outputs": [],
      "source": [
        "# The 'k' value indicates the number of sources to use per query.\n",
        "# 'k' as in 'k-nearest-neighbours' to the query in the embedding space.\n",
        "# 'temperature' is the degree of randomness introduced into the LLM response.\n",
        "k = 2\n",
        "temperature = 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KEEzB0ks3R8"
      },
      "source": [
        "#### Prompt engineering\n",
        "\n",
        "As outlined before, the creation of prompts is essential to adapt LLMs for your given use case.\n",
        "**Prompt engineering** is a method of zero-shot fine-tuning for large language models.\n",
        "By prompting a LLM with contextual information about its purpose, the model can simulate a variety of situations, such as a customer assistant chatbot, a document summariser, a translator, etc.\n",
        "\n",
        "In this use case, we prompt our model to respond as a conversational Q&A chatbot.\n",
        "Prompt engineering can be especially useful for introducing guard rails to an application - in this template we tell the model to not respond to queries it lacks the information to answer, as users will trust the application to provide factual replies, so rejecting a query is preferable to outputting false information.\n",
        "\n",
        "You can use the prompt and code cells below for your knowledge worker.\n",
        "\n",
        "**‚û°Ô∏è Your task:** Study the following code cells as they provide reusable LangChain code for your knowledge worker.\n",
        "Pay attention to the prompt in the `template` variable.\n",
        "What elements do you notice in the prompt?\n",
        "How is the prompt used in the chain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3XR2N31ms3R8"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\\\n",
        "You are a helpful chatbot designed to perform Q&A on a set of documents.\n",
        "Always respond to users with friendly and helpful messages.\n",
        "Your goal is to answer user questions using relevant sources.\n",
        "\n",
        "You were developed by Datatonic, and are powered by Google's PaLM-2 model.\n",
        "\n",
        "In addition to your implicit model world knowledge, you have access to the following data sources:\n",
        "- Company documentation.\n",
        "\n",
        "If a user query is too vague, ask for more information.\n",
        "If insufficient information exists to answer a query, respond with \"I don't know\".\n",
        "NEVER make up information.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# The PromptTemplate reads input variables (i.e.: 'chat_history', 'question') from the template\n",
        "SYSTEM_PROMPT = PromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "h_8UWPous3R8"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import VertexAI\n",
        "\n",
        "def qa_chain():\n",
        "    # A vector store retriever relates queries to embedded documents\n",
        "    retriever = vector_store.as_retriever(k=k)\n",
        "\n",
        "    # The selected GoogleLLM model uses embedded documents related to the query\n",
        "    # It parses these documents in order to answer the user question.\n",
        "    # We use the GoogleLLM LLM, however other models can be substituted here\n",
        "    model = VertexAI(temperature=temperature)\n",
        "\n",
        "    # A conversation retrieval chain keeps a history of Q&A / conversation\n",
        "    # This allows for contextual questions such as \"give an example of that (previous response)\".\n",
        "    # The chain is also set to return the source documents used in generating an output\n",
        "    # This allows for explainability behind model output.\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=model,\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        condense_question_prompt=SYSTEM_PROMPT,\n",
        "    )\n",
        "\n",
        "    return chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "IifTS5ags3R8"
      },
      "outputs": [],
      "source": [
        "def q_a(question: str, history: list):\n",
        "    # map history (list of lists) to expected format of chat_history (list of tuples)\n",
        "    chat_history = map(tuple, history)\n",
        "\n",
        "    # Query the LLM to get a response\n",
        "    # First the Q&A chain will collect documents semantically similar to the question\n",
        "    # Then it will ask the LLM to use this data to answer the user question\n",
        "    # We also provide chat history as further context\n",
        "    response = qa_chain()(\n",
        "        {\n",
        "            \"question\": question,\n",
        "            \"chat_history\": chat_history,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Format source documents (sources of excerpts passed to the LLM) into links the user can validate\n",
        "    sources = [\n",
        "        \"[{0}]({0})\".format(doc.metadata[\"source\"])\n",
        "        for doc in response[\"source_documents\"]\n",
        "    ]\n",
        "\n",
        "    # Return the LLM answer, and list of sources used (formatted as a string)\n",
        "    return response[\"answer\"], \"\\n\\n\".join(sources)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SG5IawNs3R9"
      },
      "source": [
        "### Building a user interface\n",
        "\n",
        "As building a UI is outside of the scope for this hackathon, a templateed UI using [Gradio](https://gradio.app/) is provided for your knowledge worker.\n",
        "\n",
        "**‚û°Ô∏è Your task:** Execute the cells below to launch the user interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "68mnxD75s3R9"
      },
      "outputs": [],
      "source": [
        "def submit(msg, chatbot):\n",
        "    # First create a new entry in the conversation log\n",
        "    msg, chatbot = user(msg, chatbot)\n",
        "    # Then get the chatbot response to the user question\n",
        "    chatbot = bot(chatbot)\n",
        "    return msg, chatbot\n",
        "\n",
        "\n",
        "def user(user_message, history):\n",
        "    # Return \"\" to clear the user input, and add the user question to the conversation history\n",
        "    return \"\", history + [[user_message, None]]\n",
        "\n",
        "\n",
        "def bot(history):\n",
        "    # Get the user question from conversation history\n",
        "    user_message = history[-1][0]\n",
        "    # Get the response and sources used to answer the user question\n",
        "    bot_message, bot_sources = q_a(user_message, history[:-1])\n",
        "\n",
        "    # Using a template, format the response and sources together\n",
        "    bot_template = (\n",
        "        \"{0}\\n\\n<details><summary><b>Sources</b></summary>\\n\\n{1}</details>\"\n",
        "    )\n",
        "    # Place the response into the conversation history and return\n",
        "    history[-1][1] = bot_template.format(bot_message, bot_sources)\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erPj4_Sxs3R9"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Build a simple GradIO app that accepts user input and queries the LLM\n",
        "# Then displays the response in a ChatBot interface, with markdown support.\n",
        "with gr.Blocks(theme=gr.themes.Base()) as demo:\n",
        "    # Set a page title\n",
        "    gr.Markdown(\"# Custom knowledge worker\")\n",
        "    # Create a chatbot conversation log\n",
        "    chatbot = gr.Chatbot(label=\"ü§ñ knowledge worker\")\n",
        "    # Create a textbox for user questions\n",
        "    msg = gr.Textbox(\n",
        "        label=\"üë©‚Äçüíª user input\", info=\"Query information from the custom knowledge base.\"\n",
        "    )\n",
        "\n",
        "    # Align both buttons on the same row\n",
        "    with gr.Row():\n",
        "        send = gr.Button(value=\"Send\", variant=\"primary\", size=\"sm\")\n",
        "        clear = gr.Button(value=\"Clear History\", variant=\"secondary\", size=\"sm\")\n",
        "\n",
        "    # Submit message on <enter> or clicking \"Send\" button\n",
        "    msg.submit(submit, [msg, chatbot], [msg, chatbot], queue=False)\n",
        "    send.click(submit, [msg, chatbot], [msg, chatbot], queue=False)\n",
        "\n",
        "    # Clear chatbot history on clicking \"Clear History\" button\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Create a queue system so multiple users can access the page at once\n",
        "demo.queue()\n",
        "# Launch the webserver locally\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrsqbDyOs3R9"
      },
      "source": [
        "**‚û°Ô∏è Your task:** Use the user interface above (which you can also open in a separate tab given the shareable link above), to query your knowledge base.\n",
        "\n",
        "Try out a few questions below:\n",
        "\n",
        "> üë©‚Äçüíª: tell me about company x!\n",
        ">\n",
        "> üë©‚Äçüíª: how can I apply for company x?\n",
        "> ü¶ú:  You can install the gcloud CLI by running the command `$ gcloud components update`.\n",
        "\n",
        "\n",
        "You can also ask general questions since LLMs have been trained on massive amounts of public data:\n",
        "\n",
        "> üë©‚Äçüíª: how to install the gcloud cli\n",
        ">\n",
        "> ü¶ú:  You can install the gcloud CLI by running the command `$ gcloud components update`.\n",
        ">\n",
        "> üë©‚Äçüíª: that command requires gcloud to be installed, how can I install gcloud initially?\n",
        ">\n",
        "> ü¶ú:  You can install gcloud initially by running the command '`pip install google-cloud`' in your terminal.\n",
        ">\n",
        "> üë©‚Äçüíª: how can I set the target project\n",
        ">\n",
        "> ü¶ú:  You can set the target project for the gcloud CLI by using the command `$ gcloud config set project my-new-default-project`.\n",
        ">\n",
        "> üë©‚Äçüíª: what is the gcloud cli?\n",
        ">\n",
        "> ü¶ú:  The gcloud CLI is a command line interface for Google Cloud Platform services.\n",
        ">\n",
        "> üë©‚Äçüíª: explain the above in more detail\n",
        ">\n",
        "> ü¶ú:  The gcloud CLI is a tool used to authenticate and configure credentials for Google Cloud services. It can be used to change the default project ID, update components, and authenticate the CLI itself.\n",
        "\n",
        "Since we used a `ConversationalRetrievalChain`, we can also correct the model when it gives the wrong response and prompt it to fix it‚Äôs mistake, or ask for further detail on a previous response.\n",
        "\n",
        "**üéâ Congratulations! üéâ** You've created your first chain using LangChain which you can query for general questions in a user interface.\n",
        "Let's continue extending your knowledge worker in the next task.\n",
        "\n",
        "**Note:** Stop the cell which runs the user interface before contuining with the next task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Extending your knowledge base\n",
        "\n",
        "As mentioned, it is possible to extend a knowledge base with additional documents.\n",
        "This is useful for updating a knowledge base with new information without having to re-embed established knowledge from scratch.\n",
        "\n",
        "If you wanted to build a knowledge worker with another document type, for instance [Microsoft Word](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/microsoft_word.html) documents, you would update the `load_documents()` function according to the documentation for that document type loader."
      ],
      "metadata": {
        "id": "3Ua3ttLS1_sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the following example for loading Word documents:\n",
        "```python\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "\n",
        "def load_documents():\n",
        "    # Load the documentation using a Microsoft Word parser\n",
        "    loader = Docx2txtLoader(\"example_data/fake.docx\")\n",
        "    documents = loader.load()\n",
        "\n",
        "    return documents\n",
        "```\n",
        "\n",
        "**Note:** if you're looking to deploy a knowledge worker with several knowledge bases, a [Router Chain](https://python.langchain.com/en/latest/modules/chains/examples/multi_retrieval_qa_router.html), which combines several knowledge workers with discrete knowledge bases into a single chain which selects the best worker for the query.\n",
        "\n",
        "**‚û°Ô∏è Your task:** Load at least one additional document type such as PDF or Doc.\n",
        "For that update the `load_documents` function to load `.pdf` or `.pptx` files, or write some logic to load any of these file types depending on an argument `document_type` (try using an `if-elif-else` switch to change which document loader is used) and embed multiple document types within the same vector store."
      ],
      "metadata": {
        "id": "l0Ad_MOh21zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings(source_dir):\n",
        "    # TODO: update the code below\n",
        "\n",
        "    documents = load_documents(source_dir=source_dir)\n",
        "\n",
        "    # We use GoogleLLM embeddings model, however other models can be substituted here\n",
        "    embedding = VertexAIEmbeddings()\n",
        "\n",
        "    vector_store = embed_documents(embedding, documents)\n",
        "\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "Ah-anCDQ2eaj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you've different sources of data, you'll need to update the way of embedding data as you'll need to loop over a sequence of folders instead of processing a single folder only.\n",
        "\n",
        "**‚û°Ô∏è Your task:** Extend the `create_embeddings()` function to load multiple knowledge sources.\n",
        "Update the `source_dir` to `source_dirs`, which accepts a list of folder paths, then iteratively load these folders as documents and embed them in the database."
      ],
      "metadata": {
        "id": "Dl39Eix33Xs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(source_dir):\n",
        "    # Load the documentation using a HTML parser\n",
        "\n",
        "    # TODO: update the code below\n",
        "    loader = DirectoryLoader(\n",
        "        source_dir,\n",
        "        glob=\"**/*.html\",\n",
        "        loader_cls=UnstructuredHTMLLoader,\n",
        "        show_progress=True,\n",
        "    )\n",
        "    documents = loader.load()\n",
        "\n",
        "    return documents"
      ],
      "metadata": {
        "id": "G-Eg8OGk2ije"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚û°Ô∏è Your task:** Load your text embeddings again after updating the two functions `create_embeddings` and `load_documents` and test your changes directly in the UI.\n",
        "You can run the cells below to load your embeddings and launch the UI."
      ],
      "metadata": {
        "id": "Mkpb3xkq4nmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# If the vectorstore embedding database has been created, load it\n",
        "if os.path.isdir(PERSIST_DIR):\n",
        "    print(f\"Loading {PERSIST_DIR} as vector store\")\n",
        "    vector_store = load_embeddings()\n",
        "# If it doesn't exist, create it\n",
        "else:\n",
        "    print(f\"Creating new vector store in dir {PERSIST_DIR}\")\n",
        "    vector_store = create_embeddings(source_dir=LOCAL_FOLDER)\n"
      ],
      "metadata": {
        "id": "yqzVeRnq4U26",
        "outputId": "b1eadbaa-4a65-409f-fae5-d4f89945aca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading chromadb as vector store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a queue system so multiple users can access the page at once\n",
        "demo.queue()\n",
        "# Launch the webserver locally\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "IwUgYAAT4j-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üéâ Congratulations! üéâ** You've extended your knowledge to creating text embedding from a variety of sources - whether it's public data from your company's website or unstructured documents!\n",
        "\n",
        "**Note:** Stop the cell which runs the user interface before contuining with the next task."
      ],
      "metadata": {
        "id": "WOkmXLdT31yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Generating text over a vector index\n",
        "\n",
        "We can utilise our embedded documents for more than just Q&A.\n",
        "In tasks 1 and 2, we used the embedded documents as context for answering user queries, but in this task we will use it to generate original content using this knowledge base as a source of information and style.\n",
        "\n",
        "The concept of this use case is to generate ideas for new blogs, utilising knowledge and style information contained in the existing company website data.\n",
        "We can use Generativr AI for creative ideation, too!\n",
        "Let's demonstrates the possibilities for human-computer interaction (HCI) apps in this task."
      ],
      "metadata": {
        "id": "bsGLS0Xw2BKg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NmFU3kyns3R-"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0PzurMaEs3R-"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\\\n",
        "Using the provided context, write the outline of a company blog post.\n",
        "Include a bullet-point list of the main talking points, and a brief summary of the overall blog.\n",
        "Context: {context}\n",
        "Topic: {topic}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"topic\"]\n",
        ")\n",
        "\n",
        "model = VertexAI(temperature=0.7)\n",
        "\n",
        "chain = LLMChain(llm=model, prompt=PROMPT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5kVmZBg-s3R-"
      },
      "outputs": [],
      "source": [
        "def generate_blog_outline(topic: str, k: int):\n",
        "    # search for 'k' nearest documents related to our topic.\n",
        "    docs = vector_store.similarity_search(topic, k=k)\n",
        "\n",
        "    # associate topic with the content of each document to generate inputs\n",
        "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
        "    # generate blog outline\n",
        "    output = chain.apply(inputs)\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚û°Ô∏è Your task:** Create ideas for a new blog post.\n",
        "Try adjusting the title of the post to generate new ideas!"
      ],
      "metadata": {
        "id": "JJc3FB8WLs2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BLOG_TITLE = \"Greentonic: Make cloud projects more sustainable\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "10THi9L2Kvzv"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho7Euwhks3R-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# generate variations of blog posts on the topic \"Greentonic initiative\" based on the 4 most relevant documents\n",
        "output = generate_blog_outline(BLOG_TITLE, k=4)\n",
        "markdown = \"\"\n",
        "for i, blog in enumerate(output):\n",
        "  markdown += f\"# #{i} {BLOG_TITLE}\\n{blog['text']}\\n\\n\"\n",
        "display(Markdown(markdown))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚û°Ô∏è Your task:** Now it's time to change the prompt template to create new content based on your liking.\n",
        "For that update the `prompt_template` and `temperature`."
      ],
      "metadata": {
        "id": "Rt6qQQwELhm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: setup the prompt, model, and chain to create new types of content.\n"
      ],
      "metadata": {
        "id": "6hBV3zbLMDfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üéâ Congratulations! üéâ** You've completed the last task of this hackathon!\n",
        "Continue with the next section to explore next steps for *your* Generative AI journey on Google Cloud."
      ],
      "metadata": {
        "id": "wIlaipnMMOMk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXCL6Ri6s3R-"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "## What have we built?\n",
        "\n",
        "In this session, we have built a knowledge worker use-case for accessing your complex information using Generative AI.\n",
        "This concept can be extended into a fully-fledged tool that can unlock the value of your data for customers or internal use.\n",
        "\n",
        "## Going further..\n",
        "\n",
        "This workshop has introduced all the LangChain knowledge required to create a knowledge worker.\n",
        "The next steps for moving this project from development to production are discussed below.\n",
        "\n",
        "## Decoupling LangChain from Gradio\n",
        "\n",
        "It is not necessary to run LangChain within a GradI/O app.\n",
        "Decoupling LangChain into a separate API has several benefits:\n",
        "1. We can deploy scalable servers / Docker containers\n",
        "2. Simplified code - a frontend-backend loose coupling can lead to simpler code, which is ease to update and maintain.\n",
        "3. If a more professional user interface is needed, such as a native React app.\n",
        "Replacing GradI/O is a straightforward process - FastAPI can be called from javascript, etc., allowing you to move beyond Python frontend frameworks.\n",
        "\n",
        "An example of this separation can be found on the GitHub repository, using FastAPI to create a simple LangChain API server and Poetry to manage separate server environments.\n",
        "\n",
        "## Deploying on Google Cloud\n",
        "\n",
        "Once we have decoupled our frontend and backend code, we can deploy the project onto Google Cloud.\n",
        "\n",
        "This reference architecture diagrams mirror the flow diagrams we first introducted in the workshop introduction. Using Google Cloud, we can create production pipelines for creating / updating vector databases, and deploy a knowledge worker API (which can be connected to a web UI, Slack bot, etc.).\n",
        "\n",
        "**Example architecture: Ingestion**\n",
        "![A typical ingestion chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/knowledge-worker-gcp-ingestion-pipeline.png?raw=true)\n",
        "\n",
        "By creating a pipeline for data ingestion, we can continue to extend the knowledge base of our knowledge worker as you produce new documents and documentation.\n",
        "\n",
        "**Example architecture: Inference**\n",
        "![A typical ingestion chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/knowledge-worker-gcp-inference-pipeline.png?raw=true)\n",
        "\n",
        "By creating a pipeline for inference, we can leverage the power of Google Cloud to provide a highly reliable and scalable API that can power a variety of applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üéâ Congratulations! üéâ** You've completed this notebook!\n",
        "Now it's time to embark your Generative AI journey and ideate about use cases which can benefit your company in conjunction or in addition to your first knowledge worker."
      ],
      "metadata": {
        "id": "TehVT3X_OelY"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
