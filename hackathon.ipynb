{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Generative AI Hackathon</h1>\n",
    "<table align=\"center\">\n",
    "    <td>\n",
    "        <a href=\"https://colab.research.google.com/github/teamdatatonic/gen-ai-hackathon/blob/main/hackathon.ipynb\">\n",
    "            <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\">\n",
    "            <span style=\"vertical-align: middle;\">Run in Colab</span>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a href=\"https://github.com/teamdatatonic/gen-ai-hackathon/blob/main/hackathon.ipynb\">\n",
    "            <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "            <span style=\"vertical-align: middle;\">View on GitHub</span>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/teamdatatonic/gen-ai-hackathon/main/hackathon.ipynb\">\n",
    "            <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"> \n",
    "            <span style=\"vertical-align: middle;\">Open in Vertex AI Workbench</span>\n",
    "        </a>\n",
    "    </td>\n",
    "</table>\n",
    "<hr>\n",
    "\n",
    "**‚û°Ô∏è Your task:** Learn about Generative AI by building your own Knowledge Worker using Python and LangChain!\n",
    "\n",
    "**‚ùó Note:** This workshop has been designed to be run in Google CoLab. Support for running the workshop locally or using VertexAI Workbench is provided, but we heavily recommend CoLab for the best experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScHDNga8s3Rw"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook walks you through the challenge of implementing a **Knowledge Worker** for your organisation using **Generative AI**!\n",
    "\n",
    "**Why a knowledge worker?** Decentralized data across internal and external databases results in time wasted as workforce tries to find required information and transform into insights. A knowledge worker can consolidate this information, then answer queries in natural language providing summarisation and sources.\n",
    "\n",
    "![Q&A Chain intro](https://github.com/teamdatatonic/gen-ai-hackathon/blob/bca8120f1408be1895309517a7a4d693035b940b/assets/qa_intro.png?raw=true)\n",
    "\n",
    "‚û°Ô∏è **Your task:** Implement a knowledge worker to enable users in your company to perform Q&A, in natural language, upon a knowledge base.\n",
    "In this way, you'll centralise company data for easy access in a user-friendly manner, boosting productivity.\n",
    "As such, you'll create a knowledge worker fine-tuned to your data domain.\n",
    "This app will only have access to specific knowledge such as public data about your company available on your company's website and unstructured documents (websites, PDF, Word, text ...).\n",
    "\n",
    "While solving the tasks as instructed in this notebook, you'll familiarise yourself with common concepts and tools for Generative AI including:\n",
    "\n",
    "- The Open-Source tool LangChain\n",
    "- Large Language Models (LLMs)\n",
    "- Vertex AI Search\n",
    "- Prompts and Prompt Engineering\n",
    "- Text Embeddings and Vector Databases\n",
    "\n",
    "\n",
    "Ultimately, this notebook details how to get started with LangChain, and walks through setting up a knowledge worker on Google Cloud and Vertex AI.\n",
    "\n",
    "## Implementing a knowledge worker\n",
    "\n",
    "When creating a knowledge worker, you'll recall that Large Language Models (LLMs) can be tuned for a variety of tasks such as text summarization, answering questions, and generating new content (and many more!).\n",
    "When it comes to tuning approaches, you'll have the choice between:\n",
    "\n",
    "**A) Zero-shot learning:** Use LLMs directly without providing additional data or fine-tuning the model.\n",
    "\n",
    "**B) Few-shot learning:** Provide a select number of input examples when using LLM to improve the quality of outputs.\n",
    "\n",
    "**C) Model Fine-tuning:** Fine-tune certain (or additional) layers in the LLM by training the model on provided training data.\n",
    "\n",
    "Instead of training LLMs using your own data (ie. fine-tuning), it is far easier and more effective to adapt the LLM to your use-case by prompt engineering only (ie. tuning).\n",
    "Thus, methods A) and B) are more applicable for creating your first knowledge worker.\n",
    "\n",
    "A knowledge worker can be approached in two stages:\n",
    "\n",
    "1. Embedding knowledge from diverse sources.\n",
    "    * Load our dataset.\n",
    "    * Shard our documents (e.g.: by paragraph, per 1000 tokens, etc.)\n",
    "    * Embed the documents in Vertex AI Search or a vector store.\n",
    "2. Querying a LLM which is aware of your relevant knowledge to answer questions.\n",
    "    * Locating relevant documents from Vertex AI Search or a vector store.\n",
    "    * Asking the LLM our query, providing relevant knowledge as context to generate an answer.\n",
    "\n",
    "![Q&A Chain Flow](https://github.com/teamdatatonic/gen-ai-hackathon/blob/bca8120f1408be1895309517a7a4d693035b940b/assets/qa_flow.jpeg?raw=true)\n",
    "\n",
    "First, documents (websites, Word documents, databases, Powerpoints, PDFs, etc.) are loaded and split into chunks. Fragmenting is important for three reasons:\n",
    "\n",
    "1. There are technical restrictions on how much data (tokens) can be fed into an LLM at once, meaning the context + system prompt + chat history + user prompt must fit within the token limit.\n",
    "2. Most LLM APIs operate on a per-token pricing model, meaning it is cost-effective to limit the size / amount of data provided to the LLM per query.\n",
    "3. Contextual information should be relevant to the user query, meaning it is optimal to provide only relevant snippets from documents, making the answer more relevant whilst saving costs as per (1) and (2).\n",
    "\n",
    "Next, these document shards are embedded within a vector store. Embedding a document means to align it within a multi-dimension space, which can then be searched according to user queries to find relevant documents. Document relevancy scoring can be as simple as a K-neighbours search, since embedded documents with similarity (as percieved by the LLM embedding model) will be proximate within the search space.\n",
    "\n",
    "![A typical ingestion chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/typical-ingestion-chain.png?raw=true)\n",
    "\n",
    "Once the vector store is created, a user can query the knowledge base using natural language questions. Relevant documents related to the query are found in the vector store by embedding the user query and finding local documents. These snippets of text are provided to the LLM (alongside the user query, chat history, prompt engineering, etc.) which parses the information to generate an answer.\n",
    "\n",
    "![A typical query chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/typical-query-chain.png?raw=true)\n",
    "\n",
    "## Running this workshop\n",
    "* Execute each code snippet sequentially. This lab is designed so certain steps (like defining functions or importing modules) are performed once, so each new code cell builds upon previous cells.\n",
    "* If the notebook crashes or you want to restart the notebook later, make sure you execute all cells prior to where you left off.\n",
    "* Executing cells out of order can lead to errors, so your first step when debugging should be to ensure all previous code cells have been run.\n",
    "* This workshop can be completed independently, but Datatonic workshop leaders are available to discuss tasks, debug issues, or have a chat about generative AI!\n",
    "\n",
    "## Prerequisites\n",
    "### Install Python dependencies\n",
    "We've developed a python module specifically for this workshop. Installing this one package also installs other dependencies, such as LangChain (a LLM framework) and GradIO (a web UI framework)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02VnG-Zhs3R2"
   },
   "outputs": [],
   "source": [
    "%pip install --quiet \"git+https://github.com/teamdatatonic/gen-ai-hackathon.git@feat/add-vertex-ai-search#subdirectory=dt-gen-ai-hackathon-helper\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saLQmyJeqSM3"
   },
   "source": [
    "**‚ùó Restart the Python kernel:** Ensure that your environment can access the newly installed dependencies. Continue after the restart from the `Setup cloud project` step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVJ8C3zws3R3"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo3B7BfIs3R3"
   },
   "source": [
    "**‚ùó Note:** If your kernel doesn't restart automatically, click the \"Restart Runtime\" button above your notebook.\n",
    "If you dont see a restart button, go to the \"Runtime\" toolbar tab then \"Restart Runtime\". After restarting, continue executing the project from below this cell.\n",
    "\n",
    "## Accessing the Vertex AI Endpoint\n",
    "\n",
    "Currently, Vertex AI LLMs are accessible via Google Cloud projects. We will access the Vertex AI endpoint via a service account.\n",
    "\n",
    "1. Upload the Google Application Credentials `.json` file sent to your email to the notebook filesystem.\n",
    "2. Set the variable `GOOGLE_APPLICATION_CREDENTIALS` with the filepath (**‚ùó Note:** the `/content/` folder is where uploaded files are stored by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RxkFX1ies3R3",
    "ExecuteTime": {
     "end_time": "2023-10-06T08:10:49.032630Z",
     "start_time": "2023-10-06T08:10:46.848733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [dt-gen-ai-hackathon-sa@dt-gen-ai-hackathon-dev.iam.gserviceaccount.com]\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# @title Set project credentials. { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown Set the filepath to the `.json` credentials file.\n",
    "GOOGLE_APPLICATION_CREDENTIALS = \"secrets/credentials.json\"  # @param {type:\"string\"}\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "!gcloud auth activate-service-account --key-file={GOOGLE_APPLICATION_CREDENTIALS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:10:52.948043Z",
     "start_time": "2023-10-06T08:10:49.808257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "# @markdown Set the Google Cloud project ID.\n",
    "PROJECT_ID = \"dt-gen-ai-hackathon-dev\"  # @param {type:\"string\"}\n",
    "\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure notebook environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "MODEL = \"text-bison@001\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T08:10:53.640299Z",
     "start_time": "2023-10-06T08:10:53.599352Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1: Implementing a knowledge worker on Vertex AI Search\n",
    "\n",
    "Creating a custom knowledge worker is similar to your first step when learning a new programming language.\n",
    "As such your first challenge is to create a ‚ÄúHello World‚Äù program, however, adapted to LLMs which is way more exciting!\n",
    "\n",
    "With a few lines of code, you'll:\n",
    "- Load documents with information about your company\n",
    "- Store documents in Vertex AI Search\n",
    "- Use an LLM to answer queries about your company knowledge\n",
    "\n",
    "**‚ùó All of these steps can be achieved in a few lines of Python.**\n",
    "\n",
    "## Introduction to LangChain\n",
    "\n",
    "LangChain is a Python framework for developing applications using language models.\n",
    "It abstracts the connection between applications and LLMs, allowing a loose coupling between code and specific providers like Google PaLM.\n",
    "\n",
    "LangChain supports [numerous methods](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) for loading documents.\n",
    "\n",
    "We will be using the `DirectoryLoader` and `UnstructuredHTMLLoader` in order to load a pre-compiled archive of your website. This method is similar to the [`RecursiveUrlLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/recursive_url_loader). This document loader searches for subpages of a website and loads each pages content as a document. Additionally, if we only wanted to download a list of URLs without searching for subpages, we could use a [`UnstructuredURLLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/url).\n",
    "\n",
    "**‚ùó Note:** We're using pre-compiled archives to avoid hitting rate limits during this session, however this content can also be programatically gathered routinely to collect new blog posts / press releases.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Read the linked resources in the `Introduction to LangChain` step and study the following code cells as they provide reusable LangChain code for your knowledge worker.\n",
    "\n",
    "## Setting up Vertex AI Search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a Vertex AI Search data store\n",
    "Create a Vertex AI Search data store to host unstructured data (pdf documents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dt_gen_ai_hackathon_helper.vertex_ai_search import vertex_ai_search\n",
    "\n",
    "display_name = \"data_store_test_from_notebook\"\n",
    "data_store_name = \"alphabet_investor_pdfs\"\n",
    "hackathon_team_name = \"team_1\"\n",
    "\n",
    "vertex_ai_search.create_data_store(PROJECT_ID, data_store_name + \"_\" + hackathon_team_name, display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest data to the data store\n",
    "Ingest data to the data store previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dt_gen_ai_hackathon_helper.vertex_ai_search import vertex_ai_search\n",
    "\n",
    "data_store_id = \"DATA_STORE_ID_FROM_UI\"\n",
    "location = \"global\"\n",
    "gcs_uri = \"gs://dt-gen-ai-hackathon-pdf-datasets/alphabet-investor-pdfs/\"\n",
    "\n",
    "vertex_ai_search.import_documents(PROJECT_ID, location, data_store_id, gcs_uri=gcs_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Vertex AI Search application\n",
    "Create a Vertex AI Search application to host your knowledge worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dt_gen_ai_hackathon_helper.vertex_ai_search import vertex_ai_search\n",
    "\n",
    "display_name = \"app_team_1\"\n",
    "solution_type_search = \"SOLUTION_TYPE_SEARCH\"\n",
    "\n",
    "vertex_ai_search.create_search_ai_app(PROJECT_ID, display_name, data_store_id, solution_type_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain retrieval Q&A chains\n",
    "We will demonstrate the use of three types of LangChain retrieval Q&A chains:\n",
    "\n",
    "- RetrievalQA\n",
    "- RetrievalQAWithSourcesChain\n",
    "- ConversationalRetrievalChain\n",
    "\n",
    "First, we initialize a Vertex AI Language Model (LLM) and a LangChain 'retriever' to fetch documents from our Enterprise Search engine.\n",
    "\n",
    "In the case of Q&A chains, our retriever is directly passed to the chain, enabling it to function automatically without requiring any additional configuration.\n",
    "\n",
    "Behind the scenes, the search query is initially passed to the retriever. The retriever performs a search and returns relevant document snippets. These snippets are then used as context for the prompt executed by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:11:52.353402Z",
     "start_time": "2023-10-06T08:11:48.320039Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "from langchain.retrievers import GoogleCloudEnterpriseSearchRetriever\n",
    "\n",
    "data_store_id = \"alphabet-investor-pdfs-tea_1696409737598\"\n",
    "\n",
    "llm = VertexAI(model_name=MODEL, temperature=0.0)\n",
    "\n",
    "retriever = GoogleCloudEnterpriseSearchRetriever(\n",
    "    project_id=PROJECT_ID, search_engine_id=data_store_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RetrievalQA Chain\n",
    "This is the simplest document Q&A chain offered by LangChain.\n",
    "\n",
    "Several different chain types are available, as listed here.\n",
    "\n",
    "In these examples, we use the 'stuff' type, which simply inserts all the document snippets into the prompt. This approach has the advantage of requiring only a single LLM call, making it faster and more cost-efficient.\n",
    "\n",
    "However, this method comes with a drawback: if we have a large number of search results, we run the risk of exceeding the token limit for our prompt or truncating useful information.\n",
    "\n",
    "Other chain types, such as 'map_reduce' and 'refine,' employ an iterative process. These types make multiple LLM calls, taking individual document snippets one at a time and refining the answer iteratively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "search_query = \"Who is the CEO of DeepMind?\"\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "retrieval_qa.run(search_query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we set `return_source_documents=True` as an optional parameter when constructing the chain, we can examine the document snippets returned by the retriever. This feature is particularly useful for debugging, as the relevance of these snippets to the answer may not always be immediately obvious."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "results = retrieval_qa({\"query\": search_query})\n",
    "counter = 1\n",
    "print(\"*\" * 79)\n",
    "print(f\"Answer: {results['result']}\")\n",
    "print(f\"Used {len(results['source_documents'])} relevant documents.\")\n",
    "print(\"*\" * 79)\n",
    "for doc in results[\"source_documents\"]:\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"Document {counter}\")\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"Source of content: {doc.metadata['source']}\")\n",
    "    print(\"-\" * 79)\n",
    "    print(doc.page_content)\n",
    "    counter += 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### RetrievalQAWithSourcesChain\n",
    "This variant delivers both the answer to the question and the source documents used for generating that answer, doing so in a simpler manner than using `return_source_documents=True`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "retrieval_qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_qa_with_sources({\"question\": search_query}, return_only_outputs=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ConversationalRetrievalChain\n",
    "The ConversationalRetrievalChain remembers and uses previous questions to enable a chat-like discovery process. To utilize this chain, we need to provide a memory class that stores and passes the previous messages to the LLM as context. For this purpose, we use the ConversationBufferMemory class that comes with Langchain."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversational_retrieval = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, retriever=retriever, memory=memory\n",
    ")\n",
    "\n",
    "search_query = \"What were alphabet revenues in 2022?\"\n",
    "\n",
    "result = conversational_retrieval({\"question\": search_query})\n",
    "print(result[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_query = \"What about costs and expenses?\"\n",
    "result = conversational_retrieval({\"question\": new_query})\n",
    "print(result[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_query = \"Is this more than in 2021?\"\n",
    "\n",
    "result = conversational_retrieval({\"question\": new_query})\n",
    "print(result[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt engineering\n",
    "\n",
    "![Q&A Chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/bca8120f1408be1895309517a7a4d693035b940b/assets/stuff-chain.jpeg?raw=true)\n",
    "\n",
    "As outlined before, the creation of prompts is essential to adapt LLMs for your given use case.\n",
    "**Prompt engineering** is a method of zero-shot fine-tuning for large language models.\n",
    "By prompting a LLM with contextual information about its purpose, the model can simulate a variety of situations, such as a customer assistant chatbot, a document summariser, a translator, etc.\n",
    "\n",
    "In this use case, we prompt our model to respond as a conversational Q&A chatbot.\n",
    "Prompt engineering can be especially useful for introducing guard rails to an application - in this template we tell the model to not respond to queries it lacks the information to answer, as users will trust the application to provide factual replies, so rejecting a query is preferable to outputting false information.\n",
    "\n",
    "You can use the prompt and code cells below for your knowledge worker.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Execute and study the code cell below. Pay attention to the prompt being defined.\n",
    "What elements do you notice in the prompt?\n",
    "How is the prompt used in the chain?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "TASK_01_TEMPLATE = \"\"\"\\\n",
    "You are a helpful chatbot designed to perform Q&A on a set of documents.\n",
    "Always respond to users with friendly and helpful messages.\n",
    "Your goal is to answer user questions using relevant sources.\n",
    "\n",
    "You were developed by Datatonic, and are powered by Google's PaLM-2 model.\n",
    "\n",
    "In addition to your implicit model world knowledge, you have access to the following data sources:\n",
    "- Company documentation.\n",
    "\n",
    "If a user query is too vague, ask for more information.\n",
    "If insufficient information exists to answer a query, respond with \"I don't know\".\n",
    "NEVER make up information.\n",
    "\n",
    "The answer should consist of only 1 word and not a sentence. Don't more words than 1.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T09:29:10.171957Z",
     "start_time": "2023-10-06T09:29:10.127525Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have connected our Vertex AI Search data store and prompt, we can define our LangChain.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Execute and study the following code cells - they provide reusable LangChain code for your knowledge worker."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In all of the previous examples we used the default prompt that comes with langchain.\n",
    "\n",
    "We can inspect our chain object to discover the wording of the prompt template being used.\n",
    "\n",
    "We may find that this is not suitable for our purposes, and we may wish to customise the prompt, for example to present our results in a different format, or to specify additional constraints."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "print(qa.combine_documents_chain.llm_chain.prompt.template)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's modify the prompt to return an answer in a single word (useful for yes/no questions). We will constrain the LLM to say 'I don't know' if it cannot answer.\n",
    "\n",
    "We create a new prompt_template and pass this in using the `template` argument."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RetrievalQA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprompts\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PromptTemplate\n\u001B[1;32m      3\u001B[0m prompt \u001B[38;5;241m=\u001B[39m PromptTemplate(\n\u001B[1;32m      4\u001B[0m     template\u001B[38;5;241m=\u001B[39mTASK_01_TEMPLATE, input_variables\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      5\u001B[0m )\n\u001B[0;32m----> 6\u001B[0m qa_chain \u001B[38;5;241m=\u001B[39m \u001B[43mRetrievalQA\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_llm(\n\u001B[1;32m      7\u001B[0m     llm\u001B[38;5;241m=\u001B[39mllm, prompt\u001B[38;5;241m=\u001B[39mprompt, retriever\u001B[38;5;241m=\u001B[39mretriever, return_source_documents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m      8\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'RetrievalQA' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=TASK_01_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=llm, prompt=prompt, retriever=retriever, return_source_documents=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T09:33:41.030151Z",
     "start_time": "2023-10-06T09:33:40.946379Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_query = \"Were 2020 EMEA revenues higher than 2020 APAC revenues?\"\n",
    "\n",
    "results = qa_chain({\"query\": search_query})\n",
    "counter = 1\n",
    "\n",
    "print(\"*\" * 79)\n",
    "print(f\"Answer: {results['result']}\")\n",
    "print(f\"Used {len(results['source_documents'])} relevant documents.\")\n",
    "print(\"*\" * 79)\n",
    "for doc in results[\"source_documents\"]:\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"Document {counter}\")\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"Source of content: {doc.metadata['source']}\")\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    counter += 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the user interface\n",
    "\n",
    "As building a UI is outside of the scope for this hackathon, a templated GUI using [Gradio](https://gradio.app/) is provided for your knowledge worker.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Execute the cells below to launch the user interface.\n",
    "\n",
    "**‚ùó Note:** The following cell will run until manually stopped. Remember to halt it before moving onto the next task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "\n",
    "def create_qa_vertex_ai_search_chain(condense_question_prompt, k=4, temperature=0.0):\n",
    "    \"\"\"Create a Q&A conversation chain using the VertexAI LLM.\n",
    "\n",
    "    Arguments:\n",
    "        vector_store (object): The vectorstore containing our knowledge.\n",
    "        condense_question_prompt (PromptTemplate): The prompt template used to prompt engineer our LLM to respond in a certain tone, etc.\n",
    "        k (int): the 'k' value indicates the number of sources to use per query. 'k' as in 'k-nearest-neighbours' to the query in the embedding space.\n",
    "        temperature (float): the degree of randomness introduced into the LLM response.\n",
    "    \"\"\"\n",
    "    retriever = GoogleCloudEnterpriseSearchRetriever(\n",
    "        project_id=PROJECT_ID, search_engine_id=data_store_id\n",
    "    )\n",
    "\n",
    "    # The selected Google model uses embedded documents related to the query\n",
    "    # It parses these documents in order to answer the user question.\n",
    "    # We use the VertexAI LLM, however other models can be substituted here\n",
    "    llm = VertexAI(model_name=MODEL, k=k, temperature=temperature)\n",
    "\n",
    "    # A conversation retrieval chain keeps a history of Q&A / conversation\n",
    "    # This allows for contextual questions such as \"give an example of that (previous response)\".\n",
    "    # The chain is also set to return the source documents used in generating an output\n",
    "    # This allows for explainability behind model output.\n",
    "    conversational_retrieval = ConversationalRetrievalChain.from_llm(condense_question_prompt=condense_question_prompt,\n",
    "                                                                     llm=llm, retriever=retriever,\n",
    "                                                                     return_source_documents=True\n",
    "                                                                     )\n",
    "    return conversational_retrieval"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T09:29:16.105284Z",
     "start_time": "2023-10-06T09:29:16.016553Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://6960d76cb667ca3d1a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"https://6960d76cb667ca3d1a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dt_gen_ai_packages.dt_gen_ai_hackathon_helper.view.view as demo_view\n",
    "\n",
    "qa_chain = create_qa_vertex_ai_search_chain(prompt)\n",
    "demo = demo_view.View(qa_chain=qa_chain)\n",
    "demo.launch_interface()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-06T09:33:55.644410Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 0: Preview a knowledge worker\n",
    "\n",
    "Before we look at the underlying code supporting the knowledge worker demo, let's look at a pre-built example using the Datatonic website as a knowledge source.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Download the pre-embedded vector store and explore the capabilities of a knowledge worker."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @title Set the demo bucket name { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown This variable can be left as default for this task.\n",
    "DEMO_BUCKET = \"dt-gen-ai-hackathon-demo\"  # @param {type:\"string\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dt_gen_ai_hackathon_helper.widgets.widgets as widgets\n",
    "\n",
    "DEMO_DROPDOWN = widgets.gcp_bucket_dropdown(\n",
    "    DEMO_BUCKET, \"Select a demo from this dropdown: \"\n",
    ")\n",
    "display(DEMO_DROPDOWN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!gsutil cp gs: // {DEMO_BUCKET} / {DEMO_DROPDOWN.value}.tar.gz.& & tar -xzf {DEMO_DROPDOWN.value}.tar.gz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import dt_gen_ai_hackathon_helper.view.view as demo_view\n",
    "import dt_gen_ai_hackathon_helper.embeddings.embeddings as demo_embeddings\n",
    "\n",
    "demo_vector_store = demo_embeddings.load_embeddings(\n",
    "    DEMO_DROPDOWN.value\n",
    ")  # loads the vector DB from the local file system.\n",
    "\n",
    "demo = demo_view.View(vector_store=demo_vector_store)\n",
    "demo.launch_interface()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**‚ùó Note:** The following cell will run until manually stopped. Remember to halt it before moving onto the next task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Implementing a knowledge worker\n",
    "Here, you'll implement a knowledge worker for your company using a local vector store.\n",
    "\n",
    "We will:\n",
    "- Load documents with information about your company\n",
    "- Create text embeddings from documents\n",
    "- Storing embedding in a local database\n",
    "- Use an LLM to answer queries about your company knowledge\n",
    "\n",
    "## Collecting documents\n",
    "First, we need to collect our data. To get started fast, we've already downloaded some sample website data upfront. Let's copy the website data from a public Cloud Storage bucket to your local file system\n",
    "\n",
    "**‚ùó Note:** Although PaLM supports multiple languages, text embeddings currently work best with English documents.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Select a pre-compiled website from our list and download from the public bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Select a webarchive { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown This variable can be left as default for this task.\n",
    "BUCKET = \"dt-gen-ai-hackathon-webarchive\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_FOLDER_DROPDOWN = widgets.gcp_bucket_dropdown(\n",
    "    BUCKET,\n",
    "    \"Choose any of these web archives as the base knowledge of your worker: \",\n",
    ")\n",
    "display(LOCAL_FOLDER_DROPDOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs: // {BUCKET} / {LOCAL_FOLDER_DROPDOWN.value}.tar.gz.& & tar -xzf {LOCAL_FOLDER_DROPDOWN.value}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Congratulations! üéâ You've downloaded your website data.\n",
    "\n",
    "If we browse the files we just downloaded, we can see the file structure contains folders and `HTML` files. This is because it is a local replica of the target website, meaning the file paths correlate with real webpages.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Run the following cells to *create* the text embeddings based on your downloaded data.\n",
    "\n",
    "Now, lets embed these files so we can use them in our knowledge worker.\n",
    "\n",
    "In the code cell below, we parse the directory to find `HTML` files and load their contents using `UnstructuredHTMLLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjaBy8CSs3R5"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, UnstructuredHTMLLoader\n",
    "\n",
    "\n",
    "def load_documents(source_dir):\n",
    "    # Load the documentation using a HTML parser\n",
    "    loader = DirectoryLoader(\n",
    "        source_dir,\n",
    "        glob=\"**/*.html\",\n",
    "        loader_cls=UnstructuredHTMLLoader,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    documents = loader.load()\n",
    "\n",
    "    print(f\"Loaded: {len(documents)} documents from {source_dir}.\")\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXhTVS9ws3R5"
   },
   "source": [
    "### Creating or loading embeddings\n",
    "\n",
    "Creating embeddings each time we use our app is time-consuming and expensive.\n",
    "By persisting the vector store database after embedding, we can load the saved embeddings for use in another session.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Study and execute the following code cells. Note that after the documents have been loaded, they are split into shards using the `RecursiveCharacterTextSplitter` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Njknbfazs3R6"
   },
   "outputs": [],
   "source": [
    "# @title Set the name of your vectorstore { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown This variable can be left as default for this task.\n",
    "PERSIST_DIR = \"chromadb\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkO3NV73s3R6"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "\n",
    "def create_embeddings(source_dir):\n",
    "    documents = load_documents(source_dir=source_dir)\n",
    "\n",
    "    # We use Google embeddings model, however other models can be substituted here\n",
    "    embedding = VertexAIEmbeddings()\n",
    "\n",
    "    # Individual documents will often exceed the token limit.\n",
    "    # By splitting documents into chunks of 1000 token\n",
    "    # These chunks fit into the token limit alongside the user prompt\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=texts, embedding=embedding, persist_directory=PERSIST_DIR\n",
    "    )\n",
    "\n",
    "    # Persist the ChromaDB locally, so we can reload the script without expensively re-embedding the database\n",
    "    vector_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings(\n",
    "    source_dir=LOCAL_FOLDER_DROPDOWN.value\n",
    ")  # creates the vector DB and saves it locally.\n",
    "print(f\"Created new vectorstore in dir {PERSIST_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è Your task:** Run the following cells to *load* the text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy4mX1JCs3R6"
   },
   "outputs": [],
   "source": [
    "def load_embeddings(persist_directory):\n",
    "    # We use VertexAI embeddings model, however other models can be substituted here\n",
    "    embeddings = VertexAIEmbeddings()\n",
    "\n",
    "    # Creating embeddings with each re-run is highly inefficient and costly.\n",
    "    # We instead aim to embed once, then load these embeddings from storage.\n",
    "    vector_store = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2r7APATs3R6"
   },
   "outputs": [],
   "source": [
    "vector_store = load_embeddings(\n",
    "    PERSIST_DIR\n",
    ")  # loads the vector DB from the local file system.\n",
    "print(f\"Loaded {PERSIST_DIR} as vectorstore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mXJKt22xc6N"
   },
   "source": [
    "**üéâ Congratulations! üéâ** You've created text embeddings from your company data and stored them successfully in a local vector database.\n",
    "Now, you'll shift your focus to implementing the actual LLM by creating a chain using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSsi93lgs3R7"
   },
   "source": [
    "## Creating the Conversational Q&A Chain\n",
    "\n",
    "In this section, you'll create a chain which will be able to provide an answer given a question from a user.\n",
    "To understand the purpose of chains, you can read about chains in the [LangChain documentation](https://docs.langchain.com/docs/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the user interface\n",
    "\n",
    "As building a UI is outside of the scope for this hackathon, a templated GUI using [Gradio](https://gradio.app/) is provided for your knowledge worker.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Execute the cells below to launch the user interface.\n",
    "\n",
    "**‚ùó Note:** The following cell will run until manually stopped. Remember to halt it before moving onto the next task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qa_chain = create_qa_chain(\n",
    "    vector_store=vector_store, condense_question_prompt=TASK_01_PROMPT\n",
    ")\n",
    "\n",
    "demo = demo_view.View(qa_chain=qa_chain)\n",
    "demo.launch_interface()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrsqbDyOs3R9"
   },
   "source": [
    "**‚û°Ô∏è Your task:** Use the user interface above (which you can also open in a separate tab given the shareable link above) to query your knowledge base.\n",
    "\n",
    "Try out a few questions from this example Q&A (using the Datatonic web archive):\n",
    "\n",
    "> üë©‚Äçüíª: What is Datatonic?\n",
    "> \n",
    "> ü¶ú: Datatonic is a data consultancy enabling companies to make better business decisions with the power of Modern Data Stack and MLOps.\n",
    "> \n",
    "> üë©‚Äçüíª: Summarise the web article on Greentonic.\n",
    "> \n",
    "> ü¶ú: Greentonic is Datatonic's sustainability initiative.\n",
    "> \n",
    "> üë©‚Äçüíª: How is Datatonic being sustainable?\n",
    "> \n",
    "> ü¶ú: Datatonic is committed to sustainability and has a number of initiatives in place to reduce its environmental impact. These include:\n",
    ">    * Using renewable energy sources\n",
    ">    * Reducing our carbon footprint\n",
    ">    * Promoting sustainable practices in our supply chain\n",
    ">    * Supporting environmental charities\n",
    ">\n",
    "> We believe that sustainability is essential for the future of our planet and we are committed to doing our part to make a difference.\n",
    "\n",
    "Since we used a `ConversationalRetrievalChain`, we can also correct the model when it gives the wrong response and prompt it to fix it‚Äôs mistake, or ask for further detail on a previous response.\n",
    "\n",
    "**üéâ Congratulations! üéâ** You've created your first chain using LangChain which you can query for general questions in a user interface.\n",
    "Let's continue extending your knowledge worker in the next task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ua3ttLS1_sO"
   },
   "source": [
    "# Task 2: Extending the knowledge base\n",
    "\n",
    "As mentioned, it is possible to extend the knowledge base with additional documents.\n",
    "This is useful for updating a knowledge base with new information without having to re-embed established knowledge from scratch.\n",
    "\n",
    "If you wanted to build a knowledge worker with another document type, for instance [Microsoft Word](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/microsoft_word.html) documents, you would update the `load_documents()` function according to the documentation for that document type loader.\n",
    "\n",
    "**‚ùó Note:** if you're looking to deploy a knowledge worker with several knowledge bases, an [Embedding Router Chain](https://python.langchain.com/docs/modules/chains/foundational/router#embeddingrouterchain), which combines several knowledge workers with discrete knowledge bases into a single chain which selects the best worker for the query.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Extend the knowledge worker with new documents.\n",
    "1. Load the new documents.\n",
    "2. Add new documents to the existing vector store using the `.add_documents(documents=...)` method.\n",
    "\n",
    "See the following example for loading Word documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ah-anCDQ2eaj"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader\n",
    "\n",
    "\n",
    "def load_docx_documents(filepath):\n",
    "    if filepath:\n",
    "        # Load the documentation using a Microsoft Word parser\n",
    "        loader = Docx2txtLoader(filepath)\n",
    "        documents = loader.load()\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word document by filepath\n",
    "word_documents = load_docx_documents(\n",
    "    filepath=None\n",
    ")  # ‚ùó TODO: update this function to your own file type + file path\n",
    "\n",
    "# add this document(s) to the vector store\n",
    "vector_store.add_documents(word_documents)\n",
    "\n",
    "# \"save\" the new vector store back to the file system\n",
    "vector_store.persist()\n",
    "\n",
    "print(\"Finished adding new documents to the vectorstore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùó TODO: replicate the code above to add more documents to the vector store.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkpb3xkq4nmu"
   },
   "source": [
    "**‚û°Ô∏è Your task:** Rerun the app and try asking questions using knowledge from your newly added documents.\n",
    "\n",
    "**‚ùó Note:** The following cell will run until manually stopped. Remember to halt it before moving onto the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwUgYAAT4j-6"
   },
   "outputs": [],
   "source": [
    "qa_chain = create_qa_chain(\n",
    "    vector_store=vector_store, condense_question_prompt=TASK_01_PROMPT\n",
    ")\n",
    "\n",
    "demo = demo_view.View(qa_chain=qa_chain)\n",
    "demo.launch_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOkmXLdT31yE"
   },
   "source": [
    "**üéâ Congratulations! üéâ** You've extended your knowledge to creating text embedding from a variety of sources - whether it's public data from your company's website or unstructured documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsGLS0Xw2BKg"
   },
   "source": [
    "# Task 3: Generating text over a vector index\n",
    "\n",
    "We can utilise our embedded documents for more than just Q&A.\n",
    "In tasks 1 and 2, we used the embedded documents as context for answering user queries, but in this task we will use it to generate original content using this knowledge base as a source of information and style.\n",
    "\n",
    "The concept of this use case is to generate ideas for new blogs, utilising knowledge and style information contained in the existing company website data.\n",
    "We can use Generative AI for creative ideation, too!\n",
    "Let's demonstrates the possibilities for human-computer interaction (HCI) apps in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set LLM temperature { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown Temperature controls the degree of creativity / randomness introduced into the LLM.\n",
    "temperature = 0.7  # @param {type:\"slider\", min:0, max:1, step:0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_03_TEMPLATE = \"\"\"\\\n",
    "Using the provided context, write the outline of a company blog post.\n",
    "Include a bullet-point list of the main talking points, and a brief summary of the overall blog.\n",
    "\n",
    "Context: {context}\n",
    "Topic: {topic}\n",
    "\"\"\"\n",
    "\n",
    "TASK_03_PROMPT = PromptTemplate.from_template(TASK_03_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PzurMaEs3R-"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "model = VertexAI(temperature=temperature)\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=TASK_03_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kVmZBg-s3R-"
   },
   "outputs": [],
   "source": [
    "def generate_blog_outline(topic: str, k: int):\n",
    "    # search for 'k' nearest documents related to our topic.\n",
    "    docs = vector_store.similarity_search(topic, k=k)\n",
    "\n",
    "    # associate topic with the content of each document to generate inputs\n",
    "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
    "\n",
    "    # generate blog outline\n",
    "    output = chain.apply(inputs)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJc3FB8WLs2-"
   },
   "source": [
    "**‚û°Ô∏è Your task:** Create ideas for a new blog post.\n",
    "Try adjusting the title of the post to generate new ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10THi9L2Kvzv"
   },
   "outputs": [],
   "source": [
    "# @title Set blog prompt / title { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown Be descriptive, as the LLM will collect semantically similar sources as inspiration.\n",
    "BLOG_TITLE = \"How we're making our business more sustainable\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho7Euwhks3R-"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# generate variations of blog posts on the topic provided, based on the 4 most relevant documents\n",
    "output = generate_blog_outline(BLOG_TITLE, k=4)\n",
    "markdown = \"\"\n",
    "\n",
    "for i, blog in enumerate(output):\n",
    "    markdown += f\"# #{i} {BLOG_TITLE}\\n{blog['text']}\\n\\n\"\n",
    "\n",
    "display(Markdown(markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt6qQQwELhm8"
   },
   "source": [
    "**‚û°Ô∏è Your task:** Update the `TASK_03_PROMPT`, `temperature` and `BLOG_TITLE` variables to create new types of content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIlaipnMMOMk"
   },
   "source": [
    "**üéâ Congratulations! üéâ** You've completed task 3 and generated ideas for future blog posts!\n",
    "Continue with the next section to explore more possibilities and ideas using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xooDGVw_Jwlg"
   },
   "source": [
    "# Task 4: Extending the chain\n",
    "\n",
    "So far you've created two types of chains:\n",
    "\n",
    "### LLMChain\n",
    "\n",
    "The `LLMChain` is a simple chain that adds some functionality around language models.\n",
    "It is used widely throughout LangChain, including in other chains and agents.\n",
    "\n",
    "An LLMChain consists of a **PromptTemplate** and a **language model** (either an LLM or chat model).\n",
    "It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "```python\n",
    "chain = LLMChain(llm=model, prompt=PROMPT)\n",
    "```\n",
    "\n",
    "### ConversationalRetrievalChain\n",
    "\n",
    "The `ConversationalRetrievalQA` chain builds on RetrievalQAChain to provide a chat history component.\n",
    "\n",
    "It first combines the chat history (either explicitly passed in or retrieved from the provided memory) and the question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question answering chain to return a response.\n",
    "\n",
    "To create one, you will need a retriever.\n",
    "In the below example, we will create one from a vector store, which can be created from embeddings.\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(k=k)\n",
    "model = VertexAI(temperature=temperature)\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    condense_question_prompt=TASK_01_PROMPT,\n",
    ")\n",
    "```\n",
    "\n",
    "### Explore more chains\n",
    "\n",
    "**‚û°Ô∏è Your task:** Firm up your knowledge about the two chains used in this notebook [here](https://python.langchain.com/docs/modules/chains/foundational/llm_chain) and [here](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db).\n",
    "In which scenarios should you apply either of them?\n",
    "What are their limitations?\n",
    "\n",
    "*The LLMChain is useful when ...*\n",
    "\n",
    "*It's limitations are ...*\n",
    "\n",
    "*The ConversationalRetrievalChain is useful when ...*\n",
    "\n",
    "*It's limitations are ...*\n",
    "\n",
    "**‚û°Ô∏è Your task:** Read about more types of chains in the [official LangChain documentation](https://python.langchain.com/docs/modules/chains/additional/).\n",
    "We recommend the **Sequential chain** and **Self-critique chain with constitutional AI**.\n",
    "How can you extend your conversational knowledge worker which is currently based on the `ConversationalRetrievalChain`?\n",
    "Summarise your idea either using pseudo code or actual code if you've time!\n",
    "Overall we would like to you to consider:\n",
    "\n",
    "**Idea + idea description:**\n",
    "\n",
    "- *The idea is ...*\n",
    "- *What it is ...*\n",
    "\n",
    "**Problem it solves + impact:**\n",
    "\n",
    "- *It would solve the following challenge ...*\n",
    "- *The volume or value of the impact would be ...*\n",
    "\n",
    "**Approach + Next steps:**\n",
    "\n",
    "- *Next steps would be ...*\n",
    "\n",
    "**‚ùó Note:** Do you have any other ideas (even outside of implementing a knowledge worker)?\n",
    "Feel free to ideate about another use case which is relevant to your industry or company!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jq_fu6wXKFNf"
   },
   "outputs": [],
   "source": [
    "# ‚ùó TODO: create pseudo code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy_oGwjBKFi2"
   },
   "source": [
    "**üéâ Congratulations! üéâ** You've completed the last task of this hackathon! Now lets get a sneak peek of Gen App Builder and Google Enterprise Search.\n",
    "\n",
    "## Bonus Track - Using Gen App Builder Within your Knowledge Worker\n",
    "Google Cloud has released a tool called Enterprise Search within the Gen App Builder service. Using Enterprise Search, you can ingest and retrieve websites, internal structured and unstructured data in a search engine and then use a Knowledge Worker to retrieve information in natural language. This is analogous to an internal Google search engine for your documents.\n",
    "\n",
    "Using a custom implementation of a Knowledge Worker, you can combine the power of Gen App Builder with the customisation ability of a Knowledge Worker and build applications simpler, faster and more robustly. \n",
    "\n",
    "Let's have a sneak peak at how to do this. \n",
    "\n",
    "**‚ùó Note** : This is not publicly available so you will need a whitelisted Google Cloud project. We have already created an Enterprise Search for you, so you can go ahead and try it out.\n",
    "\n",
    "**‚û°Ô∏è Your task:** Obtain the credentials from one of the workshop leaders for the `dt-vertex-gen-ai-dev` project, in order to access a preview of Enterprise Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set project credentials. { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown Set the filepath to the `.json` credentials file.\n",
    "\n",
    "# @markdown **‚ùó Note** : This overwrites your previous credentials. If you want to return to previous tasks, reset the credentials filepath using the original `GOOGLE_APPLICATION_CREDENTIALS` assignment.\n",
    "GOOGLE_APPLICATION_CREDENTIALS = \"/content/es_credentials.json\"  # @param {type:\"string\"}\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "# @markdown These values can be kept as default for this task.\n",
    "SEARCH_ENGINE_PROJECT_ID = \"dt-vertex-gen-ai-dev\"  # @param {type:\"string\"}\n",
    "SEARCH_ENGINE_ID = \"wpp-genai-day_1689017718091\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è Your task:** Run the code cell below to update the Q&A Chain to use the new retriever.\n",
    "\n",
    "**‚ùó Note how the only code snippet we need to update is the `retriever` variable. Enterprise Search integrates directly into our existing app with minimal code changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dt_gen_ai_hackathon_helper.enterprise_search.enterprise_search as enterprise_search\n",
    "\n",
    "\n",
    "def create_qa_chain(condense_question_prompt, k=4, temperature=0.0):\n",
    "    # Using Enterprise Search as a retriever\n",
    "    retriever = enterprise_search.EnterpriseSearchRetriever(\n",
    "        project_id=SEARCH_ENGINE_PROJECT_ID, search_engine_id=SEARCH_ENGINE_ID, k=4\n",
    "    )\n",
    "\n",
    "    # The selected Google model uses embedded documents related to the query\n",
    "    # It parses these documents in order to answer the user question.\n",
    "    # We use the Google LLM, however other models can be substituted here\n",
    "    model = VertexAI(temperature=temperature)\n",
    "\n",
    "    # A conversation retrieval chain keeps a history of Q&A / conversation\n",
    "    # This allows for contextual questions such as \"give an example of that (previous response)\".\n",
    "    # The chain is also set to return the source documents used in generating an output\n",
    "    # This allows for explainability behind model output.\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=model,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        condense_question_prompt=condense_question_prompt,\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è Your task:** Rerun the app using the new retriever.\n",
    "\n",
    "**‚ùó Note:** The following cell will run until manually stopped. Remember to halt it before moving onto the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = create_qa_chain(condense_question_prompt=TASK_01_PROMPT)\n",
    "\n",
    "demo = demo_view.View(qa_chain=qa_chain)\n",
    "demo.launch_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXCL6Ri6s3R-"
   },
   "source": [
    "Try asking the same questions you used to query your knowledge worker. Does it answer the question in the same way? Does it give more or less detail? What are the immediate differences between solutions?\n",
    "\n",
    "**üéâ Congratulations! üéâ** You've gotten started with Enterprise Search. \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "## What have we built?\n",
    "\n",
    "In this session, we have built a knowledge worker use-case for accessing your complex information using Generative AI.\n",
    "This concept can be extended into a fully-fledged tool that can unlock the value of your data for customers or internal use.\n",
    "\n",
    "## Going further..\n",
    "\n",
    "This workshop has introduced all the LangChain knowledge required to create a knowledge worker.\n",
    "The next steps for moving this project from development to production are discussed below.\n",
    "\n",
    "## Decoupling LangChain from Gradio\n",
    "\n",
    "It is not necessary to run LangChain within a GradI/O app.\n",
    "Decoupling LangChain into a separate API has several benefits:\n",
    "1. We can deploy scalable servers / Docker containers\n",
    "2. Simplified code - a frontend-backend loose coupling can lead to simpler code, which is ease to update and maintain.\n",
    "3. If a more professional user interface is needed, such as a native React app.\n",
    "Replacing GradI/O is a straightforward process - FastAPI can be called from javascript, etc., allowing you to move beyond Python frontend frameworks.\n",
    "\n",
    "An example of this separation can be found on the GitHub repository, using FastAPI to create a simple LangChain API server and Poetry to manage separate server environments.\n",
    "\n",
    "## Deploying on Google Cloud\n",
    "\n",
    "Once we have decoupled our frontend and backend code, we can deploy the project onto Google Cloud.\n",
    "\n",
    "This reference architecture diagrams mirror the flow diagrams we first introducted in the workshop introduction. Using Google Cloud, we can create production pipelines for creating / updating vector databases, and deploy a knowledge worker API (which can be connected to a web UI, Slack bot, etc.).\n",
    "\n",
    "**Example architecture: Ingestion**\n",
    "![A typical ingestion chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/knowledge-worker-gcp-ingestion-pipeline.png?raw=true)\n",
    "\n",
    "By creating a pipeline for data ingestion, we can continue to extend the knowledge base of our knowledge worker as you produce new documents and documentation.\n",
    "\n",
    "**Example architecture: Inference**\n",
    "![A typical ingestion chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/knowledge-worker-gcp-inference-pipeline.png?raw=true)\n",
    "\n",
    "By creating a pipeline for inference, we can leverage the power of Google Cloud to provide a highly reliable and scalable API that can power a variety of applications.\n",
    "\n",
    "**üéâ Congratulations! üéâ** You've completed this notebook!\n",
    "Now it's time to embark your Generative AI journey and ideate about use cases which can benefit your company in conjunction or in addition to your first knowledge worker."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
