{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Generative AI Hackathon</h1>\n",
    "<table align=\"center\">\n",
    "    <td>\n",
    "        <a href=\"https://colab.research.google.com/github/teamdatatonic/gen-ai-hackathon/blob/main/hackathon.ipynb\">\n",
    "            <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\">\n",
    "            <span style=\"vertical-align: middle;\">Run in Colab</span>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a href=\"https://github.com/teamdatatonic/gen-ai-hackathon/blob/main/hackathon.ipynb\">\n",
    "            <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "            <span style=\"vertical-align: middle;\">View on GitHub</span>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/teamdatatonic/gen-ai-hackathon/main/hackathon.ipynb\">\n",
    "            <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"> \n",
    "            <span style=\"vertical-align: middle;\">Open in Vertex AI Workbench</span>\n",
    "        </a>\n",
    "    </td>\n",
    "</table>\n",
    "<hr>\n",
    "\n",
    "**➡️ Your task:** Learn about Generative AI by building your own Knowledge Worker using Python and LangChain!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScHDNga8s3Rw"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook walks you through the challenge of implementing a **Knowledge Worker** for your organisation using **Generative AI**!\n",
    "\n",
    "**Why a knowledge worker?** Decentralized data across internal and external databases results in time wasted as workforce tries to find required information and transform into insights. A knowledge worker can consolidate this information, then answer queries in natural language providing summarisation and sources.\n",
    "\n",
    "![Q&A Chain intro](https://github.com/teamdatatonic/gen-ai-hackathon/blob/bca8120f1408be1895309517a7a4d693035b940b/assets/qa_intro.png?raw=true)\n",
    "\n",
    "➡️ **Your task:** Implement a knowledge worker to enable users in your company to perform Q&A, in natural language, upon a knowledge base.\n",
    "In this way, you'll centralise company data for easy access in a user-friendly manner, boosting productivity.\n",
    "As such, you'll create a knowledge worker fine-tuned to your data domain.\n",
    "This app will only have access to specific knowledge such as public data about your company available on your company's website and unstructured documents (websites, PDF, Word, text ...).\n",
    "\n",
    "While solving the tasks as instructed in this notebook, you'll familiarise yourself with common concepts and tools for Generative AI including:\n",
    "\n",
    "- The Open-Source tool LangChain\n",
    "- Large Language Models (LLMs)\n",
    "- Vertex AI Search (a.k.a Enterprise Search)\n",
    "- Prompts and Prompt Engineering\n",
    "- Text Embeddings and Vector Databases\n",
    "\n",
    "\n",
    "Ultimately, this notebook details how to get started with LangChain and Vertex AI to do Retrieval Augmented Generation (RAG), and walks through setting up a knowledge worker on Google Cloud and Vertex AI.\n",
    "\n",
    "## Implementing a knowledge worker\n",
    "\n",
    "When creating a knowledge worker, you'll recall that Large Language Models (LLMs) can be tuned for a variety of tasks such as text summarization, answering questions, and generating new content (and many more!).\n",
    "When it comes to tuning approaches, you'll have the choice between:\n",
    "\n",
    "**A) Zero-shot learning:** Use LLMs directly without providing additional data or fine-tuning the model.\n",
    "\n",
    "**B) Few-shot learning:** Provide a select number of input examples when using LLM to improve the quality of outputs.\n",
    "\n",
    "**C) Model Fine-tuning:** Fine-tune certain (or additional) layers in the LLM by training the model on provided training data.\n",
    "\n",
    "Instead of training LLMs using your own data (ie. fine-tuning), it is far easier and more effective to adapt the LLM to your use-case by prompt engineering only (ie. tuning).\n",
    "Thus, methods A) and B) are more applicable for creating your first knowledge worker.\n",
    "\n",
    "A knowledge worker can be approached in two stages:\n",
    "\n",
    "1. Embedding knowledge from diverse sources.\n",
    "    * Load our dataset.\n",
    "    * Shard our documents (e.g.: by paragraph, per 1000 tokens, etc.)\n",
    "    * Embed the documents in Vertex AI Search or a vector store.\n",
    "2. Querying a LLM which is aware of your relevant knowledge to answer questions.\n",
    "    * Locating relevant documents from Vertex AI Search or a vector store.\n",
    "    * Asking the LLM our query, providing relevant knowledge as context to generate an answer.\n",
    "\n",
    "![Q&A Chain Flow](https://github.com/teamdatatonic/gen-ai-hackathon/blob/bca8120f1408be1895309517a7a4d693035b940b/assets/qa_flow.jpeg?raw=true)\n",
    "First, documents (websites, Word documents, databases, Powerpoints, PDFs, etc.) are loaded and split into chunks. Fragmenting is important for three reasons:\n",
    "\n",
    "1. There are technical restrictions on how much data (tokens) can be fed into an LLM at once, meaning the context + system prompt + chat history + user prompt must fit within the token limit.\n",
    "2. Most LLM APIs operate on a per-token pricing model, meaning it is cost-effective to limit the size / amount of data provided to the LLM per query.\n",
    "3. Contextual information should be relevant to the user query, meaning it is optimal to provide only relevant snippets from documents, making the answer more relevant whilst saving costs as per (1) and (2).\n",
    "\n",
    "Next, these document shards are embedded within a vector store. Embedding a document means to align it within a multi-dimension space, which can then be searched according to user queries to find relevant documents. Document relevancy scoring can be as simple as a K-neighbours search, since embedded documents with similarity (as percieved by the LLM embedding model) will be proximate within the search space.\n",
    "\n",
    "![A typical ingestion chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/typical-ingestion-chain.png?raw=true)\n",
    "\n",
    "Once the vector store is created, a user can query the knowledge base using natural language questions. Relevant documents related to the query are found in the vector store by embedding the user query and finding local documents. These snippets of text are provided to the LLM (alongside the user query, chat history, prompt engineering, etc.) which parses the information to generate an answer.\n",
    "\n",
    "![A typical query chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/typical-query-chain.png?raw=true)\n",
    "\n",
    "## Running this workshop\n",
    "* Execute each code snippet sequentially. This lab is designed so certain steps (like defining functions or importing modules) are performed once, so each new code cell builds upon previous cells.\n",
    "* If the notebook crashes or you want to restart the notebook later, make sure you execute all cells prior to where you left off.\n",
    "* Executing cells out of order can lead to errors, so your first step when debugging should be to ensure all previous code cells have been run.\n",
    "* This workshop can be completed independently, but Datatonic workshop leaders are available to discuss tasks, debug issues, or have a chat about generative AI!\n",
    "\n",
    "## Prerequisites\n",
    "### Install Python dependencies\n",
    "We've developed a python module specifically for this workshop. Installing this one package also installs other dependencies, such as LangChain (a LLM framework) and GradIO (a web UI framework)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02VnG-Zhs3R2"
   },
   "outputs": [],
   "source": [
    "#%pip install --quiet \"git+https://github.com/teamdatatonic/gen-ai-hackathon.git@feat/add-vertex-ai-search#subdirectory=dt_gen_ai_hackathon_helper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring colorama: markers 'python_full_version >= \"3.9.16\" and python_full_version < \"4.0.0\" and platform_system == \"Windows\" or python_full_version >= \"3.9.16\" and python_full_version < \"4.0.0\" and sys_platform == \"win32\"' don't match your environment\n",
      "Ignoring exceptiongroup: markers 'python_full_version >= \"3.9.16\" and python_version < \"3.11\"' don't match your environment\n",
      "Ignoring importlib-resources: markers 'python_full_version >= \"3.9.16\" and python_version < \"3.10\"' don't match your environment\n",
      "Ignoring pyreadline3: markers 'sys_platform == \"win32\" and python_full_version >= \"3.9.16\" and python_full_version < \"4.0.0\"' don't match your environment\n",
      "Ignoring pywin32: markers 'sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" and python_full_version >= \"3.9.16\" and python_full_version < \"4.0.0\"' don't match your environment\n",
      "Ignoring pywinpty: markers 'python_full_version >= \"3.9.16\" and python_full_version < \"4.0.0\" and os_name == \"nt\"' don't match your environment\n",
      "Ignoring tomli: markers 'python_full_version >= \"3.9.16\" and python_version < \"3.11\"' don't match your environment\n",
      "Requirement already satisfied: aiofiles==22.1.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1)) (22.1.0)\n",
      "Requirement already satisfied: aiohttp==3.8.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 4)) (3.8.4)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 92)) (1.3.1)\n",
      "Requirement already satisfied: aiosqlite==0.19.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 95)) (0.19.0)\n",
      "Requirement already satisfied: altair==5.0.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 98)) (5.0.1)\n",
      "Requirement already satisfied: anyio==3.7.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 101)) (3.7.1)\n",
      "Requirement already satisfied: appnope==0.1.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 104)) (0.1.3)\n",
      "Requirement already satisfied: argilla==0.0.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 107)) (0.0.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 110)) (21.2.0)\n",
      "Requirement already satisfied: argon2-cffi==21.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 132)) (21.3.0)\n",
      "Requirement already satisfied: arrow==1.2.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 135)) (1.2.3)\n",
      "Requirement already satisfied: asttokens==2.2.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 138)) (2.2.1)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 141)) (4.0.2)\n",
      "Requirement already satisfied: attrs==23.1.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 144)) (23.1.0)\n",
      "Requirement already satisfied: babel==2.12.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 147)) (2.12.1)\n",
      "Requirement already satisfied: backcall==0.2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 150)) (0.2.0)\n",
      "Requirement already satisfied: backoff==2.2.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 153)) (2.2.1)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 156)) (4.12.2)\n",
      "Requirement already satisfied: bleach==6.0.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 159)) (6.0.0)\n",
      "Requirement already satisfied: cachetools==5.3.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 162)) (5.3.1)\n",
      "Requirement already satisfied: certifi==2023.5.7 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 165)) (2023.5.7)\n",
      "Requirement already satisfied: cffi==1.15.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 168)) (1.15.1)\n",
      "Requirement already satisfied: chardet==5.1.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 233)) (5.1.0)\n",
      "Requirement already satisfied: charset-normalizer==3.2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 236)) (3.2.0)\n",
      "Requirement already satisfied: chromadb==0.3.26 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 312)) (0.3.26)\n",
      "Requirement already satisfied: click==8.1.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 315)) (8.1.4)\n",
      "Requirement already satisfied: clickhouse-connect==0.6.6 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 318)) (0.6.6)\n",
      "Requirement already satisfied: coloredlogs==15.0.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 387)) (15.0.1)\n",
      "Requirement already satisfied: comm==0.1.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 390)) (0.1.3)\n",
      "Requirement already satisfied: contourpy==1.1.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 393)) (1.1.0)\n",
      "Requirement already satisfied: cryptography==41.0.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 433)) (41.0.2)\n",
      "Requirement already satisfied: cycler==0.11.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 457)) (0.11.0)\n",
      "Collecting dataclasses-json==0.5.9\n",
      "  Using cached dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: debugpy==1.6.7 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 463)) (1.6.7)\n",
      "Requirement already satisfied: decorator==5.1.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 482)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 485)) (0.7.1)\n",
      "Requirement already satisfied: duckdb==0.8.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 488)) (0.8.1)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 541)) (1.1.0)\n",
      "Requirement already satisfied: executing==1.2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 547)) (1.2.0)\n",
      "Requirement already satisfied: fastapi==0.85.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 550)) (0.85.1)\n",
      "Requirement already satisfied: fastjsonschema==2.17.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 553)) (2.17.1)\n",
      "Requirement already satisfied: ffmpy==0.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 556)) (0.3.0)\n",
      "Requirement already satisfied: filelock==3.12.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 558)) (3.12.2)\n",
      "Requirement already satisfied: flatbuffers==23.5.26 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 561)) (23.5.26)\n",
      "Requirement already satisfied: fonttools==4.40.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 564)) (4.40.0)\n",
      "Requirement already satisfied: fqdn==1.5.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 599)) (1.5.1)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 602)) (1.3.3)\n",
      "Requirement already satisfied: fsspec==2023.6.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 677)) (2023.6.0)\n",
      "Requirement already satisfied: google-api-core==2.11.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 680)) (2.11.1)\n",
      "Requirement already satisfied: google-auth==2.22.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 686)) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform==1.28.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 689)) (1.28.0)\n",
      "Requirement already satisfied: google-cloud-bigquery==3.11.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 692)) (3.11.3)\n",
      "Requirement already satisfied: google-cloud-core==2.3.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 695)) (2.3.3)\n",
      "Requirement already satisfied: google-cloud-discoveryengine==0.9.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 698)) (0.9.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager==1.10.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 701)) (1.10.2)\n",
      "Requirement already satisfied: google-cloud-storage==2.10.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 704)) (2.10.0)\n",
      "Requirement already satisfied: google-crc32c==1.5.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 707)) (1.5.0)\n",
      "Requirement already satisfied: google-resumable-media==2.5.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 776)) (2.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos==1.59.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 779)) (1.59.1)\n",
      "Requirement already satisfied: gradio-client==0.2.9 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 785)) (0.2.9)\n",
      "Requirement already satisfied: gradio==3.36.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 788)) (3.36.1)\n",
      "Collecting greenlet==2.0.2\n",
      "  Downloading greenlet-2.0.2-cp311-cp311-macosx_10_9_universal2.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.0/243.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpc-google-iam-v1==0.12.6 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 852)) (0.12.6)\n",
      "Requirement already satisfied: grpcio-status==1.56.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 855)) (1.56.0)\n",
      "Requirement already satisfied: grpcio==1.56.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 858)) (1.56.0)\n",
      "Requirement already satisfied: h11==0.14.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 904)) (0.14.0)\n",
      "Requirement already satisfied: hnswlib==0.7.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 907)) (0.7.0)\n",
      "Requirement already satisfied: httpcore==0.17.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 909)) (0.17.3)\n",
      "Requirement already satisfied: httptools==0.6.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 912)) (0.6.0)\n",
      "Requirement already satisfied: httpx==0.24.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 948)) (0.24.1)\n",
      "Requirement already satisfied: huggingface-hub==0.16.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 951)) (0.16.4)\n",
      "Requirement already satisfied: humanfriendly==10.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 954)) (10.0)\n",
      "Requirement already satisfied: idna==3.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 957)) (3.4)\n",
      "Requirement already satisfied: importlib-metadata==6.8.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 960)) (6.8.0)\n",
      "Requirement already satisfied: ipykernel==6.24.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 966)) (6.24.0)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 969)) (0.2.0)\n",
      "Requirement already satisfied: ipython==8.14.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 972)) (8.14.0)\n",
      "Requirement already satisfied: isoduration==20.11.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 975)) (20.11.0)\n",
      "Requirement already satisfied: jedi==0.18.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 978)) (0.18.2)\n",
      "Requirement already satisfied: jinja2==3.1.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 981)) (3.1.2)\n",
      "Requirement already satisfied: joblib==1.3.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 984)) (1.3.1)\n",
      "Requirement already satisfied: json5==0.9.14 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 987)) (0.9.14)\n",
      "Requirement already satisfied: jsonpointer==2.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 990)) (2.4)\n",
      "Requirement already satisfied: jsonschema-specifications==2023.6.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 993)) (2023.6.1)\n",
      "Requirement already satisfied: jsonschema==4.18.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 996)) (4.18.2)\n",
      "Requirement already satisfied: jupyter-client==8.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1002)) (8.3.0)\n",
      "Requirement already satisfied: jupyter-core==5.3.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1005)) (5.3.1)\n",
      "Requirement already satisfied: jupyter-events==0.6.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1008)) (0.6.3)\n",
      "Requirement already satisfied: jupyter-server-fileid==0.9.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1011)) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals==0.4.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1014)) (0.4.4)\n",
      "Requirement already satisfied: jupyter-server-ydoc==0.8.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1017)) (0.8.0)\n",
      "Requirement already satisfied: jupyter-server==2.7.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1020)) (2.7.0)\n",
      "Requirement already satisfied: jupyter-ydoc==0.2.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1023)) (0.2.4)\n",
      "Requirement already satisfied: jupyterlab-pygments==0.2.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1026)) (0.2.2)\n",
      "Requirement already satisfied: jupyterlab-server==2.23.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1029)) (2.23.0)\n",
      "Requirement already satisfied: jupyterlab==3.6.5 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1032)) (3.6.5)\n",
      "Requirement already satisfied: kiwisolver==1.4.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1035)) (1.4.4)\n",
      "Collecting langchain==0.0.225\n",
      "  Using cached langchain-0.0.225-py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: langchainplus-sdk==0.0.20 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1107)) (0.0.20)\n",
      "Requirement already satisfied: linkify-it-py==2.0.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1110)) (2.0.2)\n",
      "Requirement already satisfied: lxml==4.9.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1113)) (4.9.3)\n",
      "Requirement already satisfied: lz4==4.3.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1195)) (4.3.2)\n",
      "Requirement already satisfied: markdown-it-py==2.2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1231)) (2.2.0)\n",
      "Requirement already satisfied: markdown==3.4.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1237)) (3.4.3)\n",
      "Requirement already satisfied: markupsafe==2.1.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1240)) (2.1.3)\n",
      "Requirement already satisfied: marshmallow-enum==1.5.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1291)) (1.5.1)\n",
      "Collecting marshmallow==3.19.0\n",
      "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1297)) (0.1.6)\n",
      "Requirement already satisfied: matplotlib==3.7.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1300)) (3.7.2)\n",
      "Requirement already satisfied: mdit-py-plugins==0.3.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1342)) (0.3.3)\n",
      "Requirement already satisfied: mdurl==0.1.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1345)) (0.1.2)\n",
      "Requirement already satisfied: mistune==3.0.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1348)) (3.0.1)\n",
      "Requirement already satisfied: monotonic==1.6 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1351)) (1.6)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1354)) (1.3.0)\n",
      "Requirement already satisfied: msg-parser==1.2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1357)) (1.2.0)\n",
      "Requirement already satisfied: multidict==6.0.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1360)) (6.0.4)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1435)) (1.0.0)\n",
      "Requirement already satisfied: nbclassic==1.0.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1438)) (1.0.0)\n",
      "Requirement already satisfied: nbclient==0.8.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1441)) (0.8.0)\n",
      "Requirement already satisfied: nbconvert==7.6.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1444)) (7.6.0)\n",
      "Requirement already satisfied: nbformat==5.9.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1447)) (5.9.1)\n",
      "Requirement already satisfied: nest-asyncio==1.5.6 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1450)) (1.5.6)\n",
      "Requirement already satisfied: nltk==3.8.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1453)) (3.8.1)\n",
      "Requirement already satisfied: notebook-shim==0.2.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1456)) (0.2.3)\n",
      "Requirement already satisfied: notebook==6.5.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1459)) (6.5.4)\n",
      "Requirement already satisfied: numexpr==2.8.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1462)) (2.8.4)\n",
      "Requirement already satisfied: numpy==1.25.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1493)) (1.25.1)\n",
      "Requirement already satisfied: olefile==0.46 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1519)) (0.46)\n",
      "Requirement already satisfied: onnxruntime==1.15.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1521)) (1.15.1)\n",
      "Requirement already satisfied: openapi-schema-pydantic==1.2.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1546)) (1.2.4)\n",
      "Requirement already satisfied: openpyxl==3.1.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1549)) (3.1.2)\n",
      "Requirement already satisfied: orjson==3.9.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1552)) (3.9.2)\n",
      "Requirement already satisfied: overrides==7.3.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1599)) (7.3.1)\n",
      "Requirement already satisfied: packaging==23.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1602)) (23.1)\n",
      "Requirement already satisfied: pandas==2.0.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1605)) (2.0.3)\n",
      "Requirement already satisfied: pandocfilters==1.5.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1631)) (1.5.0)\n",
      "Requirement already satisfied: parso==0.8.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1634)) (0.8.3)\n",
      "Requirement already satisfied: pdfminer-six==20221105 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1637)) (20221105)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1640)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1643)) (0.7.5)\n",
      "Requirement already satisfied: pillow==10.0.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1646)) (10.0.0)\n",
      "Requirement already satisfied: platformdirs==3.8.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1701)) (3.8.1)\n",
      "Requirement already satisfied: posthog==3.0.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1704)) (3.0.1)\n",
      "Requirement already satisfied: prometheus-client==0.17.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1707)) (0.17.1)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.39 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1710)) (3.0.39)\n",
      "Requirement already satisfied: proto-plus==1.22.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1713)) (1.22.3)\n",
      "Requirement already satisfied: protobuf==4.23.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1716)) (4.23.4)\n",
      "Requirement already satisfied: psutil==5.9.5 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1730)) (5.9.5)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1745)) (0.7.0)\n",
      "Requirement already satisfied: pulsar-client==3.2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1748)) (3.2.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1779)) (0.2.2)\n",
      "Requirement already satisfied: pyasn1-modules==0.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1782)) (0.3.0)\n",
      "Requirement already satisfied: pyasn1==0.5.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1785)) (0.5.0)\n",
      "Requirement already satisfied: pycparser==2.21 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1788)) (2.21)\n",
      "Requirement already satisfied: pydantic==1.10.8 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1791)) (1.10.8)\n",
      "Requirement already satisfied: pydub==0.25.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1828)) (0.25.1)\n",
      "Requirement already satisfied: pygments==2.15.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1831)) (2.15.1)\n",
      "Requirement already satisfied: pypandoc==1.11 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1834)) (1.11)\n",
      "Requirement already satisfied: pyparsing==3.0.9 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1837)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1843)) (2.8.2)\n",
      "Requirement already satisfied: python-docx==0.8.11 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1846)) (0.8.11)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1848)) (1.0.0)\n",
      "Requirement already satisfied: python-json-logger==2.0.7 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1851)) (2.0.7)\n",
      "Requirement already satisfied: python-magic==0.4.27 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1854)) (0.4.27)\n",
      "Requirement already satisfied: python-multipart==0.0.6 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1857)) (0.0.6)\n",
      "Requirement already satisfied: python-pptx==0.6.21 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1860)) (0.6.21)\n",
      "Requirement already satisfied: pytz==2023.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1862)) (2023.3)\n",
      "Requirement already satisfied: pyyaml==6.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1887)) (6.0)\n",
      "Requirement already satisfied: pyzmq==25.1.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 1928)) (25.1.0)\n",
      "Requirement already satisfied: referencing==0.29.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2006)) (0.29.1)\n",
      "Requirement already satisfied: regex==2023.6.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2009)) (2023.6.3)\n",
      "Requirement already satisfied: requests==2.31.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2098)) (2.31.0)\n",
      "Requirement already satisfied: rfc3339-validator==0.1.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2101)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator==0.1.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2104)) (0.1.1)\n",
      "Requirement already satisfied: rpds-py==0.8.10 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2107)) (0.8.10)\n",
      "Requirement already satisfied: rsa==4.9 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2205)) (4.9)\n",
      "Requirement already satisfied: semantic-version==2.10.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2208)) (2.10.0)\n",
      "Requirement already satisfied: send2trash==1.8.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2211)) (1.8.2)\n",
      "Requirement already satisfied: shapely==1.8.5.post1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2214)) (1.8.5.post1)\n",
      "Requirement already satisfied: six==1.16.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2257)) (1.16.0)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2260)) (1.3.0)\n",
      "Requirement already satisfied: soupsieve==2.4.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2263)) (2.4.1)\n",
      "Collecting sqlalchemy==2.0.18\n",
      "  Using cached SQLAlchemy-2.0.18-cp311-cp311-macosx_10_9_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: stack-data==0.6.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2308)) (0.6.2)\n",
      "Requirement already satisfied: starlette==0.20.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2311)) (0.20.4)\n",
      "Requirement already satisfied: sympy==1.12 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2314)) (1.12)\n",
      "Collecting tenacity==8.2.2\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: terminado==0.17.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2320)) (0.17.1)\n",
      "Requirement already satisfied: tiktoken==0.3.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2323)) (0.3.3)\n",
      "Requirement already satisfied: tinycss2==1.2.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2353)) (1.2.1)\n",
      "Requirement already satisfied: tokenizers==0.13.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2356)) (0.13.3)\n",
      "Requirement already satisfied: toolz==0.12.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2400)) (0.12.0)\n",
      "Requirement already satisfied: tornado==6.3.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2403)) (6.3.2)\n",
      "Requirement already satisfied: tqdm==4.65.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2415)) (4.65.0)\n",
      "Requirement already satisfied: traitlets==5.9.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2418)) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions==4.5.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2421)) (4.5.0)\n",
      "Requirement already satisfied: typing-inspect==0.8.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2424)) (0.8.0)\n",
      "Requirement already satisfied: tzdata==2023.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2427)) (2023.3)\n",
      "Requirement already satisfied: uc-micro-py==1.0.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2430)) (1.0.2)\n",
      "Requirement already satisfied: unstructured==0.6.11 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2433)) (0.6.11)\n",
      "Requirement already satisfied: uri-template==1.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2436)) (1.3.0)\n",
      "Requirement already satisfied: urllib3==1.26.16 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2439)) (1.26.16)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2442)) (0.22.0)\n",
      "Requirement already satisfied: uvloop==0.17.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2448)) (0.17.0)\n",
      "Requirement already satisfied: watchfiles==0.19.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2479)) (0.19.0)\n",
      "Requirement already satisfied: wcwidth==0.2.6 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2502)) (0.2.6)\n",
      "Requirement already satisfied: webcolors==1.13 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2505)) (1.13)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2508)) (0.5.1)\n",
      "Requirement already satisfied: websocket-client==1.6.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2511)) (1.6.1)\n",
      "Requirement already satisfied: websockets==11.0.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2514)) (11.0.3)\n",
      "Requirement already satisfied: xlrd==2.0.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2585)) (2.0.1)\n",
      "Requirement already satisfied: xlsxwriter==3.1.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2588)) (3.1.2)\n",
      "Requirement already satisfied: y-py==0.5.9 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2591)) (0.5.9)\n",
      "Requirement already satisfied: yarl==1.9.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2658)) (1.9.2)\n",
      "Requirement already satisfied: ypy-websocket==0.8.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2733)) (0.8.2)\n",
      "Requirement already satisfied: zipp==3.16.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2736)) (3.16.0)\n",
      "Requirement already satisfied: zstandard==0.21.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from -r requirements_225.txt (line 2739)) (0.21.0)\n",
      "Installing collected packages: tenacity, marshmallow, greenlet, sqlalchemy, dataclasses-json, langchain\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.3\n",
      "    Uninstalling tenacity-8.2.3:\n",
      "      Successfully uninstalled tenacity-8.2.3\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 3.20.1\n",
      "    Uninstalling marshmallow-3.20.1:\n",
      "      Successfully uninstalled marshmallow-3.20.1\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 3.0.0\n",
      "    Uninstalling greenlet-3.0.0:\n",
      "      Successfully uninstalled greenlet-3.0.0\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.21\n",
      "    Uninstalling SQLAlchemy-2.0.21:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.21\n",
      "  Attempting uninstall: dataclasses-json\n",
      "    Found existing installation: dataclasses-json 0.6.1\n",
      "    Uninstalling dataclasses-json-0.6.1:\n",
      "      Successfully uninstalled dataclasses-json-0.6.1\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.310\n",
      "    Uninstalling langchain-0.0.310:\n",
      "      Successfully uninstalled langchain-0.0.310\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sqlalchemy-bigquery 1.8.0 requires sqlalchemy<2.0.0dev,>=1.2.0, but you have sqlalchemy 2.0.18 which is incompatible.\n",
      "langchain-experimental 0.0.17 requires langchain>=0.0.239, but you have langchain 0.0.225 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.5.9 greenlet-2.0.2 langchain-0.0.225 marshmallow-3.19.0 sqlalchemy-2.0.18 tenacity-8.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saLQmyJeqSM3"
   },
   "source": [
    "**❗ Restart the Python kernel:** Ensure that your environment can access the newly installed dependencies. Continue after the restart from the `Setup cloud project` step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sVJ8C3zws3R3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo3B7BfIs3R3"
   },
   "source": [
    "**❗ Note:** If your kernel doesn't restart automatically, click the \"Restart Runtime\" button above your notebook.\n",
    "If you dont see a restart button, go to the \"Runtime\" toolbar tab then \"Restart Runtime\". After restarting, continue executing the project from below this cell.\n",
    "\n",
    "## Accessing the Vertex AI Endpoint\n",
    "\n",
    "Currently, Vertex AI LLMs are accessible via Google Cloud projects. We will access the Vertex AI endpoint via a service account.\n",
    "\n",
    "1. Upload the Google Application Credentials `.json` file sent to your email to the notebook filesystem.\n",
    "2. Set the variable `GOOGLE_APPLICATION_CREDENTIALS` with the filepath (**❗ Note:** the `/content/` folder is where uploaded files are stored by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxkFX1ies3R3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# @title Set project credentials. { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown Set the filepath to the `.json` credentials file.\n",
    "GOOGLE_APPLICATION_CREDENTIALS = \"secrets/credentials.json\"  # @param {type:\"string\"}\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "!gcloud auth activate-service-account --key-file={GOOGLE_APPLICATION_CREDENTIALS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# @markdown Set the Google Cloud project ID.\n",
    "from google.cloud import aiplatform\n",
    "PROJECT_ID = \"dt-gen-ai-hackathon-dev\"  # @param {type:\"string\"}\n",
    "aiplatform.init(project=PROJECT_ID)\n",
    "\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Implementing a knowledge worker on Vertex AI Search\n",
    "\n",
    "Creating a custom knowledge worker is similar to your first step when learning a new programming language.\n",
    "As such your first challenge is to create a “Hello World” program, however, adapted to LLMs which is way more exciting!\n",
    "\n",
    "With a few lines of code, you'll:\n",
    "- Load documents with information about your company\n",
    "- Store documents in Vertex AI Search\n",
    "- Use an LLM to answer queries about Alphabet's investors reports\n",
    "\n",
    "**❗ All of these steps can be achieved in a few lines of Python, or can be equally achieved through the UI.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setting up Vertex AI Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Vertex AI Search data store with documents\n",
    "Create a Vertex AI Search data store to host unstructured data (pdf documents)\n",
    "\n",
    "1 - On the Search & Conversation page, click `NEW DATA STORE`.\n",
    "\n",
    "\n",
    "![Select NEW DATA STORE](https://github.com/teamdatatonic/gen-ai-hackathon/blob/ba86ffb203a0f6963958cf41244a2af7a3187684/assets/new_data_store.png?raw=true)\n",
    "\n",
    "2 - Select a data source of type Cloud Storage. \n",
    "\n",
    "3 - Add the folder path `dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2004_2021`and select the data kind `Unstructured documents`. gs:// is not required.\n",
    "\n",
    "![Select kind of data](https://github.com/teamdatatonic/gen-ai-hackathon/blob/b0d89b56d4e6063d7a7de8c590ca6e886c9a0883/assets/select_kind_of_data.png?raw=true)\n",
    "\n",
    "4 - Location should be `global (Global)` and the data store name `dt-gen-ai-hackathon-<TEAM NUMBER>`.e.g. `dt-gen-ai-hackathon-t1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Vertex AI Search application\n",
    "Create a Vertex AI Search application to host your knowledge worker.\n",
    "\n",
    "1 - On the Search & Conversation page, click `NEW APP`.\n",
    "\n",
    "2 - Type: Select app type Search.\n",
    "\n",
    "3 - Configuration: Maintain the default values with `gobal (Global)` and both features activated. Add the app name `app_<TEAM NUMBER>`.e.g. `app_t1` and click `CONTINUE`.\n",
    "\n",
    "![App configuration](https://github.com/teamdatatonic/gen-ai-hackathon/blob/fc930a5159026ff7ee7ecd8d9440cd1bde0f4a22/assets/app_configuration.png?raw=true)\n",
    "\n",
    "4 - Data: Select the data store created in the previous step and click `CREATE`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LangChain retrieval Q&A chains\n",
    "\n",
    "LangChain is a Python framework for developing applications using language models.\n",
    "It abstracts the connection between applications and LLMs, allowing a loose coupling between code and specific providers like Google PaLM.\n",
    "\n",
    "We will demonstrate the use of three types of LangChain retrieval Q&A chains:\n",
    "\n",
    "- RetrievalQA\n",
    "- RetrievalQAWithSourcesChain\n",
    "- ConversationalRetrievalChain\n",
    "\n",
    "First, we initialize a Vertex AI Language Model (LLM) and a LangChain `retriever` to fetch documents from our Vertex AI Search engine.\n",
    "\n",
    "In the case of Q&A chains, our retriever is directly passed to the chain, enabling it to function automatically without requiring any additional configuration.\n",
    "\n",
    "Behind the scenes, the search query is initially passed to the retriever. The retriever performs a search and returns relevant document snippets. These snippets are then used as context for the prompt executed by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.0.236\n",
      "  Downloading langchain-0.0.236-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (2.0.18)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (0.5.9)\n",
      "Collecting langsmith<0.0.11,>=0.0.10\n",
      "  Downloading langsmith-0.0.10-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (1.25.1)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from langchain==0.0.236) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.236) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.236) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.236) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pydantic<2,>=1->langchain==0.0.236) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.236) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.236) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.236) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.236) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.236) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/alvaroazabal/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.236) (1.0.0)\n",
      "Installing collected packages: langsmith, langchain\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.43\n",
      "    Uninstalling langsmith-0.0.43:\n",
      "      Successfully uninstalled langsmith-0.0.43\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.225\n",
      "    Uninstalling langchain-0.0.225:\n",
      "      Successfully uninstalled langchain-0.0.225\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dt-gen-ai-hackathon-helper 0.1.0 requires langchain<0.0.226,>=0.0.225, but you have langchain 0.0.236 which is incompatible.\n",
      "langchain-experimental 0.0.17 requires langchain>=0.0.239, but you have langchain 0.0.236 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.0.236 langsmith-0.0.10\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.0.236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.236\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "from langchain.retrievers import GoogleCloudEnterpriseSearchRetriever\n",
    "\n",
    "# Get this value from the Vertex AI Search UI\n",
    "data_store_id = \"dt-gen-ai-hackathon-t1_1696592331863\"\n",
    "MODEL = \"text-bison@001\"\n",
    "\n",
    "llm = VertexAI(model_name=MODEL, temperature=0.0)\n",
    "\n",
    "retriever = GoogleCloudEnterpriseSearchRetriever(\n",
    "    project_id=PROJECT_ID, search_engine_id=data_store_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrievalQA Chain\n",
    "This is the simplest document Q&A chain offered by LangChain.\n",
    "\n",
    "Several different chain types are available, as listed [here](https://docs.langchain.com/docs/components/chains/index_related_chains).\n",
    "\n",
    "In these examples, we use the 'stuff' type, which simply inserts all the document snippets into the prompt. This approach has the advantage of requiring only a single LLM call, making it faster and more cost-efficient.\n",
    "\n",
    "However, this method comes with a drawback: if we have a large number of search results, we run the risk of exceeding the token limit for our prompt or truncating useful information.\n",
    "\n",
    "Other chain types, such as 'map_reduce' and 'refine,' employ an iterative process. These types make multiple LLM calls, taking individual document snippets one at a time and refining the answer iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The CEO of DeepMind is Demis Hassabis.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "search_query = \"Give me the name of the CEO of DeepMind?\"\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "retrieval_qa.run(search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set `return_source_documents=True` as an optional parameter when constructing the chain, we can examine the document snippets returned by the retriever. This feature is particularly useful for debugging, as the relevance of these snippets to the answer may not always be immediately obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Answer: The CEO of DeepMind is Demis Hassabis.\n",
      "Used 5 relevant documents.\n",
      "*******************************************************************************\n",
      "-------------------------------------------------------------------------------\n",
      "Document 1\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2004_2021/2015_google_annual_report.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "to implement segment reporting for our Q4 results,\n",
      "where Google financials will be provided separately\n",
      "than those for the rest of Alphabet businesses as\n",
      "a whole.\n",
      "This new structure will allow us to keep tremendous\n",
      "focus on the extraordinary opportunities we have\n",
      "inside of Google. A key part of this is Sundar Pichai.\n",
      "Sundar has been saying the things I would have\n",
      "said (and sometimes better!) for quite some time\n",
      "now, and I’ve been tremendously enjoying our work\n",
      "together. He has really stepped up since October\n",
      "of last year, when he took on product and engi\n",
      "neering responsibility for our Internet businesses.\n",
      "Sergey and I have been super excited about his\n",
      "progress and dedication to the company. And it is\n",
      "clear to us and our board that it is time for Sundar\n",
      "to be CEO of Google. I feel very fortunate to have\n",
      "someone as talented as he is to run the slightly\n",
      "slimmed down Google and this frees up time for\n",
      "me to continue to scale our aspirations. I have been\n",
      "spending quite a bit of time with Sundar, helping\n",
      "him and the company in any way I can, and I will\n",
      "of course continue to do that. Google itself is also\n",
      "making all sorts of new products, and I know Sundar\n",
      "will always be focused on innovation—continuing\n",
      "to stretch boundaries. I know he deeply cares that\n",
      "we can continue to make big strides on our core\n",
      "mission to organize the world’s information. Recent\n",
      "launches like Google Photos and Google Now using\n",
      "machine learning are amazing progress. Google\n",
      "also has some services that are run with their\n",
      "own identity, like YouTube. Susan is doing a great\n",
      "job as CEO, running a strong brand and driving\n",
      "incredible growth.\n",
      "Sergey and I are seriously in the business of starting\n",
      "new things. Alphabet will also include our X lab,\n",
      "which incubates new efforts like Wing, our drone\n",
      "delivery effort. We are also stoked about growing\n",
      "our investment arms, Ventures and Capital, as\n",
      "part of this new structure.\n",
      "Alphabet Inc. will replace Google Inc. as the publicly\n",
      "traded entity and all shares of Google will automat\n",
      "ically convert into the same number of shares of\n",
      "Alphabet, with all of the same rights. Google will\n",
      "become a wholly-owned subsidiary of Alphabet.\n",
      "Our two classes of shares will continue to trade\n",
      "on Nasdaq as GOOGL and GOOG.\n",
      "-------------------------------------------------------------------------------\n",
      "Document 2\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2004_2021/2016_google_annual_report.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "So in conclusion, Sergey and I are having a good\n",
      "time looking for new opportunities and managing\n",
      "and scaling our existing efforts. I still see amazing\n",
      "opportunities that just aren’t quite fully developed\n",
      "yet—and helping making them real is what I get\n",
      "excited about.\n",
      "\n",
      "Larry Page, CEO, Alphabet\n",
      "\n",
      "In June 2016, Marwan Fawaz became CEO of Nest.\n",
      "He has been doing great against their plan, and\n",
      "we have really been enjoying working with him!\n",
      "I recommend you buy all their excellent products\n",
      "including indoor and outdoor cameras, learning\n",
      "thermostats, and smoke alarms.\n",
      "Very recently Greg McCray started as CEO of\n",
      "Google Fiber. I’ve been enjoying working with\n",
      "him and his team and he has rapidly been getting\n",
      "up to speed. He visited all of our Fiber cities so\n",
      "quickly that I think he was still a bit out of breath\n",
      "when he got back to Mountain View! We have\n",
      "made significant investments in bringing gigabit\n",
      "fiber internet to lots of happy customers and I’m\n",
      "excited about our opportunities to do it better.\n",
      "We have many other efforts within Alphabet. Calico\n",
      "CEO Art Levinson, former CEO of Genentech, is\n",
      "building an amazing research and development\n",
      "company focused on aging. We also have newly\n",
      "branded investment arms, GV (formerly Google\n",
      "Ventures), and CapitalG (formerly Google Capital)\n",
      "which are doing well. Sergey is continuing to spend\n",
      "time working with the X moonshot factory. They\n",
      "have a number of efforts like Wing, which is doing\n",
      "drone delivery. I also can’t wait for them to launch!\n",
      "With the change to Alphabet, oversight has been\n",
      "easier because of increased visibility. We have\n",
      "streamlined efforts where it made sense and in\n",
      "other areas we have seen places to double down.\n",
      "I also think we have learned a lot about how\n",
      "to set up new companies with a structure for\n",
      "success. Our recent launch of Waymo was a\n",
      "great example of our learnings. In general we are\n",
      "taking a patient approach to investing our capital,\n",
      "especially significant uses. We’re not going to\n",
      "invest if we don’t see great opportunities and\n",
      "we feel like our track record for picking some\n",
      "important efforts long before others is pretty\n",
      "good. Machine learning and all the efforts around\n",
      "Google Brain and Deep Mind are good examples.\n",
      "Google Cloud led by Diane Greene is doing a\n",
      "fabulous job of getting our machine learning\n",
      "hardware and software out to everyone. We\n",
      "were early in machine learning and are already\n",
      "seeing significant dividends coming out. Many of\n",
      "the Alphabet companies are already using this\n",
      "technology and are planning to use it even more.\n",
      "-------------------------------------------------------------------------------\n",
      "Document 3\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2004_2021/20070331_google_10Q.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "IN WITNESS WHEREOF, the Parties have executed this Agreement on the date first above written.\n",
      "\n",
      "[Signature Page to Amended and Restated Agreement and Plan of Merger]\n",
      "\n",
      "GOOGLE INC.\n",
      "\n",
      "By: /s/ David C. Drummond\n",
      "Name: David C. Drummond\n",
      "Title: Senior Vice President, Corporate\n",
      "Development\n",
      "\n",
      "SNOWMASS HOLDINGS INC.\n",
      "\n",
      "By: /s/ David C. Drummond\n",
      "Name: David C. Drummond\n",
      "Title: President and Treasurer\n",
      "\n",
      "YOUTUBE, INC.\n",
      "By: /s/ Chad Hurley\n",
      "Name: Chad Hurley\n",
      "Title: CEO\n",
      "-------------------------------------------------------------------------------\n",
      "Document 4\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2004_2021/2006_google_annual_report.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "calling the SEC at 1-800-SEC-0330. The SEC maintains an internet site that contains reports, proxy and\n",
      "information statements and other information regarding our filings at www.sec.gov.\n",
      "Executive Officers of the Registrant\n",
      "The names of our executive officers and their ages, titles and biographies as of February 28, 2007 are set\n",
      "forth below:\n",
      "Name\n",
      "\n",
      "Age\n",
      "\n",
      "Position\n",
      "\n",
      "Eric Schmidt . .......... 51 Chairman of the Executive Committee, Chief Executive Officer and Director\n",
      "Sergey Brin . . . . . . . . . . . . 33 President of Technology and Director\n",
      "Larry Page . ............ 34 President of Products and Director\n",
      "Omid Kordestani . . . . . . . 43 Senior Vice President of Global Sales and Business Development\n",
      "David C. Drummond . . . . 43 Senior Vice President of Corporate Development, Chief Legal Officer and\n",
      "\n",
      "Secretary\n",
      "George Reyes .......... 52 Senior Vice President and Chief Financial Officer\n",
      "Jonathan J. Rosenberg . . . 45 Senior Vice President of Product Management\n",
      "Shona L. Brown . ....... 40 Senior Vice President of Business Operations\n",
      "Alan Eustace . .......... 50 Senior Vice President of Engineering\n",
      "\n",
      "Our executive officers are appointed by, and serve at the discretion of, our board of directors. Each\n",
      "executive officer is a full-time employee. There is no family relationship between any of our executive officers or\n",
      "directors.\n",
      "Eric Schmidt has served as our Chief Executive Officer since July 2001 and served as Chairman of our board\n",
      "of directors from March 2001 to April 2004. In April 2004, Eric was named Chairman of the Executive\n",
      "Committee of our board of directors. Prior to joining us, from April 1997 to November 2001, Eric served as\n",
      "Chairman of the board of Novell, a computer networking company, and, from April 1997 to July 2001, as the\n",
      "Chief Executive Officer of Novell. From 1983 until March 1997, Eric held various positions at Sun\n",
      "Microsystems, a supplier of network computing solutions, including Chief Technology Officer from February\n",
      "1994 to March 1997 and President of Sun Technology Enterprises from February 1991 until February 1994. Eric\n",
      "is also a director of Apple Inc., an electronic device company. Eric has a Bachelor of Science degree in electrical\n",
      "engineering from Princeton University, and a Masters degree and Ph.D. in computer science from the\n",
      "University of California at Berkeley.\n",
      "-------------------------------------------------------------------------------\n",
      "Document 5\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2004_2021/2007_google_annual_report.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "Executive Officers of The Registrant\n",
      "\n",
      "The names of our executive officers and their ages, titles and biographies as of January 31, 2008 are set forth below:\n",
      "\n",
      "Name\n",
      "\n",
      "Age\n",
      "\n",
      "Position\n",
      "\n",
      "Eric Schmidt . . . . . . . . . . . . . 52 Chairman of the Board of Directors, Chief Executive Officer and Director\n",
      "Sergey Brin\n",
      ". . . . . . . . . . . . . . 34 President of Technology and Director\n",
      "Larry Page . . . . . . . . . . . . . . . 35 President of Products and Director\n",
      "Omid Kordestani . . . . . . . . . 44 Senior Vice President of Global Sales and Business Development\n",
      "David C. Drummond . . . . . . 44 Senior Vice President of Corporate Development, Chief Legal Officer and\n",
      "\n",
      "Secretary\n",
      "George Reyes . . . . . . . . . . . . 53 Senior Vice President and Chief Financial Officer\n",
      "Jonathan J. Rosenberg . . . . . 46 Senior Vice President of Product Management\n",
      "Shona L. Brown . . . . . . . . . . 41 Senior Vice President of Business Operations\n",
      "Alan Eustace . . . . . . . . . . . . . 51 Senior Vice President of Engineering and Research\n",
      "\n",
      "Our executive officers are appointed by, and serve at the discretion of, our board of directors. Each executive officer\n",
      "\n",
      "is a full-time employee. There is no family relationship between any of our executive officers or directors.\n",
      "\n",
      "Eric Schmidt has served as our Chief Executive Officer since July 2001 and served as Chairman of our board of\n",
      "directors from March 2001 to April 2004 and again from April 2007 to the present. In April 2004, Eric was named\n",
      "Chairman of the Executive Committee of our board of directors. Prior to joining us, from April 1997 to November 2001,\n",
      "Eric served as Chairman of the board of Novell, a computer networking company, and, from April 1997 to July 2001, as\n",
      "the Chief Executive Officer of Novell. From 1983 until March 1997, Eric held various positions at Sun Microsystems, a\n",
      "supplier of network computing solutions, including Chief Technology Officer from February 1994 to March 1997 and\n",
      "President of Sun Technology Enterprises from February 1991 until February 1994. Eric is also a director of Apple Inc., an\n",
      "electronic device company. Eric has a Bachelor of Science degree in electrical engineering from Princeton University and a\n",
      "Masters degree and Ph.D. in computer science from the University of California at Berkeley.\n",
      "Sergey Brin, one of our founders, has served as a member of our board of directors since our inception in September\n",
      "1998 and as our President of Technology since July 2001. From September 1998 to July 2001, Sergey served as our\n",
      "President. Sergey holds a Masters degree in computer science from Stanford University and a Bachelor of Science degree\n",
      "with high honors in mathematics and computer science from the University of Maryland at College Park.\n"
     ]
    }
   ],
   "source": [
    "# from dt_gen_ai_hackathon_helper.formatter_helper import formatter_helper\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def format_results(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Formats and prints the results of a question-answering query.\n",
    "    Args:\n",
    "        results: The results of a question-answering query.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    counter = 1\n",
    "    print(\"*\" * 79)\n",
    "    print(f\"Answer: {results['result']}\")\n",
    "    print(f\"Used {len(results['source_documents'])} relevant documents.\")\n",
    "    print(\"*\" * 79)\n",
    "    for doc in results[\"source_documents\"]:\n",
    "        print(\"-\" * 79)\n",
    "        print(f\"Document {counter}\")\n",
    "        print(\"-\" * 79)\n",
    "        print(f\"Source of content: {doc.metadata['source']}\")\n",
    "        print(\"-\" * 79)\n",
    "        print(doc.page_content)\n",
    "        counter += 1\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "results = retrieval_qa({\"query\": search_query})\n",
    "format_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Task -> RetrievalQAWithSourcesChain\n",
    "This variant delivers both the answer to the question and the source documents used for generating that answer, doing so in a simpler manner than using `return_source_documents=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The CEO of DeepMind is Demis Hassabis.\\n',\n",
       " 'sources': '2016_google_annual_report.pdf'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "# MAKE THIS A TASK\n",
    "retrieval_qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "\n",
    "retrieval_qa_with_sources({\"question\": search_query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationalRetrievalChain\n",
    "The ConversationalRetrievalChain remembers and uses previous questions to enable a chat-like discovery process. To utilize this chain, we need to provide a memory class that stores and passes the previous messages to the LLM as context. For this purpose, we use the ConversationBufferMemory class that comes with Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet's revenues in 2021 were $257,637 million.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversational_retrieval = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, retriever=retriever, memory=memory\n",
    ")\n",
    "\n",
    "search_query = \"What were alphabet revenues in 2021?\"\n",
    "\n",
    "result = conversational_retrieval({\"question\": search_query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet's costs and expenses in 2021 were $13,200 million.\n"
     ]
    }
   ],
   "source": [
    "new_query = \"What about costs and expenses?\"\n",
    "result = conversational_retrieval({\"question\": new_query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Alphabet's revenues in 2021 were more than in 2020. In 2021, Alphabet's revenues were $257,637 million, while in 2020, Alphabet's revenues were $182,527 million. This represents a 39% increase in revenues from 2020 to 2021.\n"
     ]
    }
   ],
   "source": [
    "new_query = \"Is this more than in 2020?\"\n",
    "\n",
    "result = conversational_retrieval({\"question\": new_query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Prompt engineering\n",
    "\n",
    "![Q&A Chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/bca8120f1408be1895309517a7a4d693035b940b/assets/stuff-chain.jpeg?raw=true)\n",
    "\n",
    "As outlined before, the creation of prompts is essential to adapt LLMs for your given use case.\n",
    "**Prompt engineering** is a method of zero-shot fine-tuning for large language models.\n",
    "By prompting a LLM with contextual information about its purpose, the model can simulate a variety of situations, such as a customer assistant chatbot, a document summariser, a translator, etc.\n",
    "\n",
    "In this use case, we prompt our model to respond as a conversational Q&A chatbot.\n",
    "Prompt engineering can be especially useful for introducing guard rails to an application - in this template we tell the model to not respond to queries it lacks the information to answer, as users will trust the application to provide factual replies, so rejecting a query is preferable to outputting false information.\n",
    "\n",
    "You can use the prompt and code cells below for your knowledge worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_01_BASIC_TEMPLATE = \"\"\"\\\n",
    "You are a helpful chatbot designed to perform Q&A on a set of documents.\n",
    "Always respond to users with friendly and helpful messages.\n",
    "Your goal is to answer user questions using relevant sources.\n",
    "\n",
    "You were developed by Datatonic, and are powered by Google's PaLM-2 model.\n",
    "\n",
    "In addition to your implicit model world knowledge, you have access to the following data sources:\n",
    "- Company documentation.\n",
    "\n",
    "If a user query is too vague, ask for more information.\n",
    "If insufficient information exists to answer a query, respond with \"I don't know\".\n",
    "NEVER make up information.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task -> Add your own prompt\n",
    "Execute and study the code cell below. Pay attention to the prompt being defined.\n",
    "What elements do you notice in the prompt?\n",
    "How is the prompt used in the chain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have connected our Vertex AI Search data store and prompt, we can define our LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of the previous examples we used the default prompt that comes with langchain.\n",
    "\n",
    "We can inspect our chain object to discover the wording of the prompt template being used.\n",
    "\n",
    "We may find that this is not suitable for our purposes, and we may wish to customise the prompt, for example to present our results in a different format, or to specify additional constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RetrievalQA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alvaroazabal/Documents/gen-ai-hackathon/hackathon_juan.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alvaroazabal/Documents/gen-ai-hackathon/hackathon_juan.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Check LangChain's default prompt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alvaroazabal/Documents/gen-ai-hackathon/hackathon_juan.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m qa \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alvaroazabal/Documents/gen-ai-hackathon/hackathon_juan.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     llm\u001b[39m=\u001b[39mllm, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m, retriever\u001b[39m=\u001b[39mretriever, return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alvaroazabal/Documents/gen-ai-hackathon/hackathon_juan.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alvaroazabal/Documents/gen-ai-hackathon/hackathon_juan.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(qa\u001b[39m.\u001b[39mcombine_documents_chain\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39mtemplate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RetrievalQA' is not defined"
     ]
    }
   ],
   "source": [
    "# Check LangChain's default prompt\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "print(qa.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the prompt to return an answer in a single word (useful for yes/no questions). We will constrain the LLM to say 'I don't know' if it cannot answer.\n",
    "\n",
    "We create a new prompt_template and pass this in using the `template` argument.\n",
    "\n",
    "We want the answer to provide two things:\n",
    "```\n",
    "short_answer: Just one word\n",
    "detais: One sentence with short explanation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make this a task. Explain the objective\n",
    "\n",
    "TASK_01_A_TEMPLATE = \"\"\"\\\n",
    "The answer should consist of:\n",
    "Short answer: 1 word. Just a yes or no answer. Prefixed with Short answer:.\n",
    "Details: 1 sentence. A short explanation of the answer. Prefixed with Details:.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# TODO: Overwrite prompt\n",
    "prompt_a = PromptTemplate(\n",
    "    template=TASK_01_BASIC_TEMPLATE + \"\\n\" + TASK_01_A_TEMPLATE\n",
    "    , input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=llm, prompt=prompt_a, retriever=retriever, return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful chatbot designed to perform Q&A on a set of documents.\n",
      "Always respond to users with friendly and helpful messages.\n",
      "Your goal is to answer user questions using relevant sources.\n",
      "\n",
      "You were developed by Datatonic, and are powered by Google's PaLM-2 model.\n",
      "\n",
      "In addition to your implicit model world knowledge, you have access to the following data sources:\n",
      "- Company documentation.\n",
      "\n",
      "If a user query is too vague, ask for more information.\n",
      "If insufficient information exists to answer a query, respond with \"I don't know\".\n",
      "NEVER make up information.\n",
      "\n",
      "The answer should consist of:\n",
      "Short answer: 1 word. Just a yes or no answer. Prefixed with Short answer:.\n",
      "Details: 1 sentence. A short explanation of the answer. Prefixed with Details:.\n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Question: {question}\n",
      "Helpful Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Answer: Short answer: Yes.\n",
      "Details: EMEA revenues were $55,370 million in 2020, while APAC revenues were $32,550 million.\n",
      "Used 3 relevant documents.\n",
      "*******************************************************************************\n",
      "-------------------------------------------------------------------------------\n",
      "Document 1\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2004_2021/20201030_alphabet_10Q.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "The following table presents the foreign exchange effect on our international revenues and total revenues (in\n",
      "millions, except percentages, unaudited):\n",
      "\n",
      "Three Months Ended\n",
      "\n",
      "Nine Months Ended\n",
      "\n",
      "September 30,\n",
      "\n",
      "September 30,\n",
      "\n",
      "2019\n",
      "\n",
      "2020\n",
      "\n",
      "2019\n",
      "\n",
      "2020\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$ 12,565 $ 13,924 $ 36,546 $ 38,132\n",
      "\n",
      "Exclude foreign exchange effect on current period\n",
      "revenues using prior year rates\n",
      "\n",
      "456\n",
      "\n",
      "(250)\n",
      "\n",
      "2,034\n",
      "\n",
      "346\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "$ 13,021\n",
      "\n",
      "$ 13,674 $ 38,580 $ 38,478\n",
      "\n",
      "Prior period EMEA revenues\n",
      "\n",
      "$ 10,909 $ 12,565 $ 32,488 $ 36,546\n",
      "\n",
      "EMEA revenue percentage change\n",
      "\n",
      "15 %\n",
      "\n",
      "11 %\n",
      "\n",
      "12 %\n",
      "\n",
      "4 %\n",
      "\n",
      "EMEA constant currency revenue percentage change\n",
      "\n",
      "19 %\n",
      "\n",
      "9 %\n",
      "\n",
      "19 %\n",
      "\n",
      "5 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "$ 6,814 $ 8,458 $ 19,446 $ 22,641\n",
      "\n",
      "Exclude foreign exchange effect on current period\n",
      "revenues using prior year rates\n",
      "\n",
      "17\n",
      "\n",
      "1\n",
      "\n",
      "433\n",
      "\n",
      "167\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "$ 6,831\n",
      "\n",
      "$ 8,459\n",
      "$ 19,879\n",
      "$ 22,808\n",
      "\n",
      "Prior period APAC revenues\n",
      "\n",
      "$ 5,401\n",
      "\n",
      "$ 6,814\n",
      "$ 15,310\n",
      "$ 19,446\n",
      "\n",
      "APAC revenue percentage change\n",
      "\n",
      "26 %\n",
      "\n",
      "24 %\n",
      "\n",
      "27 %\n",
      "\n",
      "16 %\n",
      "\n",
      "APAC constant currency revenue percentage change\n",
      "\n",
      "26 %\n",
      "\n",
      "24 %\n",
      "\n",
      "30 %\n",
      "\n",
      "17 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "$ 2,290\n",
      "$ 2,371\n",
      "\n",
      "$ 6,320\n",
      "$ 6,367\n",
      "\n",
      "Exclude foreign exchange effect on current period\n",
      "revenues using prior year rates\n",
      "\n",
      "66\n",
      "\n",
      "304\n",
      "\n",
      "442\n",
      "\n",
      "640\n",
      "\n",
      "Other Americas constant currency revenues\n",
      "\n",
      "$ 2,356\n",
      "$ 2,675\n",
      "$ 6,762\n",
      "$ 7,007\n",
      "\n",
      "Prior period Other Americas revenues\n",
      "\n",
      "$ 1,827\n",
      "$ 2,290\n",
      "$ 5,407\n",
      "$ 6,320\n",
      "\n",
      "Other Americas revenue percentage change\n",
      "\n",
      "25 %\n",
      "\n",
      "4 %\n",
      "\n",
      "17 %\n",
      "\n",
      "1 %\n",
      "\n",
      "Other Americas constant currency revenue percentage\n",
      "change\n",
      "\n",
      "29 %\n",
      "\n",
      "17 %\n",
      "\n",
      "25 %\n",
      "\n",
      "11 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "$ 18,711\n",
      "\n",
      "$ 21,442 $ 53,106 $ 58,311\n",
      "\n",
      "United States revenue percentage change\n",
      "\n",
      "21 %\n",
      "\n",
      "15 %\n",
      "\n",
      "19 %\n",
      "\n",
      "10 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "$\n",
      "\n",
      "119\n",
      "\n",
      "$\n",
      "\n",
      "(22) $\n",
      "\n",
      "364 $\n",
      "\n",
      "178\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$ 40,499 $ 46,173 $ 115,782 $ 125,629\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$ 40,919 $ 46,250 $ 118,327 $ 126,604\n",
      "\n",
      "Prior period revenues, excluding hedging effect(1)\n",
      "\n",
      "$ 33,660 $ 40,380 $ 97,805 $ 115,418\n",
      "\n",
      "Total revenue percentage change\n",
      "\n",
      "20 %\n",
      "\n",
      "14 %\n",
      "\n",
      "19 %\n",
      "\n",
      "9 %\n",
      "\n",
      "Total constant currency revenue percentage change\n",
      "\n",
      "22 %\n",
      "\n",
      "15 %\n",
      "\n",
      "21 %\n",
      "\n",
      "10 %\n",
      "\n",
      "(1)\n",
      "-------------------------------------------------------------------------------\n",
      "Document 2\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2004_2021/2020Q2_alphabet_earnings_release.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "Total revenues and hedging gains for the quarter ended June 30, 2019 were $38,944 million and $108 million, respectively.\n",
      "Total revenues and hedging gains for the quarter ended March 31, 2020 were $41,159 million and $49 million, respectively.\n",
      "Non-GAAP constant currency revenues and percentage change: We define non-GAAP constant currency revenues as total\n",
      "revenues excluding the effect of foreign exchange rate movements and hedging activities, and we use it to determine the\n",
      "constant currency revenue percentage change on year-on-year and quarter-on-quarter basis. Non-GAAP constant currency\n",
      "revenues are calculated by translating current quarter revenues using prior period exchange rates and excluding any hedging\n",
      "effect recognized in the current quarter. Constant currency revenue percentage change is calculated by determining the increase\n",
      "in current quarter non-GAAP constant currency revenues over prior period revenues, excluding any hedging effect recognized in\n",
      "the prior period.\n",
      "\n",
      "8\n",
      "-------------------------------------------------------------------------------\n",
      "Document 3\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/20220202_alphabet_10K.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "The following table presents the foreign exchange effect on international revenues and total revenues (in millions,\n",
      "except percentages):\n",
      "\n",
      "Year Ended December 31,\n",
      "\n",
      "2020\n",
      "\n",
      "2021\n",
      "\n",
      "% Change from\n",
      "Prior Year\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$\n",
      "\n",
      "55,370 $\n",
      "\n",
      "79,107\n",
      "\n",
      "43 %\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "76,321\n",
      "\n",
      "38 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "32,550\n",
      "\n",
      "46,123\n",
      "\n",
      "42 %\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "45,666\n",
      "\n",
      "40 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "9,417\n",
      "\n",
      "14,404\n",
      "\n",
      "53 %\n",
      "\n",
      "Other Americas constant currency revenues\n",
      "\n",
      "14,317\n",
      "\n",
      "52 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "85,014\n",
      "\n",
      "117,854\n",
      "\n",
      "39 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "176\n",
      "\n",
      "149\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$\n",
      "\n",
      "182,527 $\n",
      "\n",
      "257,637\n",
      "\n",
      "41 %\n",
      "\n",
      "Revenues, excluding hedging effect\n",
      "\n",
      "$\n",
      "\n",
      "182,351 $\n",
      "\n",
      "257,488\n",
      "\n",
      "Exchange rate effect\n",
      "\n",
      "(3,330)\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$\n",
      "\n",
      "254,158\n",
      "\n",
      "39 %\n",
      "\n",
      "EMEA revenue growth from 2020 to 2021 was favorably affected by foreign currency exchange rates, primarily\n",
      "due to the U.S. dollar weakening relative to the Euro and British pound.\n",
      "APAC revenue growth from 2020 to 2021 was favorably affected by foreign currency exchange rates, primarily\n",
      "due to the U.S. dollar weakening relative to the Australian dollar, partially offset by the U.S. dollar strengthening relative\n",
      "to the Japanese yen.\n",
      "Other Americas growth change from 2020 to 2021 was favorably affected by changes in foreign currency\n",
      "exchange rates, primarily due to the U.S. dollar weakening relative to the Canadian dollar, partially offset by the U.S.\n",
      "dollar strengthening relative to the Argentine peso and the Brazilian real.\n",
      "Costs and Expenses\n",
      "Cost of Revenues\n",
      "The following tables present cost of revenues, including TAC (in millions, except percentages):\n",
      "Year Ended December 31,\n",
      "2020\n",
      "\n",
      "2021\n",
      "\n",
      "TAC\n",
      "\n",
      "$\n",
      "\n",
      "32,778 $\n",
      "\n",
      "45,566\n",
      "\n",
      "Other cost of revenues\n",
      "\n",
      "51,954\n",
      "\n",
      "65,373\n",
      "\n",
      "Total cost of revenues\n",
      "\n",
      "$\n",
      "\n",
      "84,732 $\n",
      "\n",
      "110,939\n",
      "\n",
      "Total cost of revenues as a percentage of revenues\n",
      "\n",
      "46.4 %\n",
      "\n",
      "43.1 %\n",
      "Cost of revenues increased $26.2 billion from 2020 to 2021. The increase was due to an increase in other cost of\n",
      "revenues and TAC of $13.4 billion and $12.8 billion, respectively.\n",
      "The increase in TAC from 2020 to 2021 was due to an increase in TAC paid to distribution partners and to Google\n",
      "Network partners, primarily driven by growth in revenues subject to TAC. The TAC rate decreased from 22.3% to\n",
      "21.8% from 2020 to 2021 primarily due to a revenue mix shift from Google Network properties to Google Search &\n",
      "other properties. The TAC rate on Google Search & other properties revenues and the TAC rate on Google Network\n",
      "revenues were both substantially consistent from 2020 to 2021.\n",
      "The increase in other cost of revenues from 2020 to 2021 was driven by increases in content acquisition costs\n",
      "primarily for YouTube, data center and other operations costs, and hardware costs. The increase in data center and\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Alphabet Inc.\n",
      "\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Were 2020 EMEA revenues higher than 2020 APAC revenues?\"\n",
    "\n",
    "results = qa_chain({\"query\": search_query})\n",
    "format_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Ingesting documents to Vertex AI Search\n",
    "\n",
    "Now, we are going to test querying our knowledge worker with a new set of documents. First, we will perform a query without the new documents to see what the results look like. Then, we will ingest the new documents and perform the same query again to see how the results change with the ingested data related to the query, which is from the year 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Answer: Short answer: Yes\n",
      "Details:\n",
      "EMEA revenues in 2022 were $82,062 million, while APAC revenues were $47,024 million.\n",
      "Used 5 relevant documents.\n",
      "*******************************************************************************\n",
      "-------------------------------------------------------------------------------\n",
      "Document 1\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/20220427_alphabet_10Q.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "Constant currency revenue percentage change is calculated by determining the change in current period\n",
      "revenues over prior year comparable period revenues where current period foreign currency revenues are\n",
      "translated using prior year comparable period exchange rates and hedging effects are excluded from revenues of\n",
      "both periods.\n",
      "These results should be considered in addition to, not as a substitute for, results reported in accordance with\n",
      "GAAP. Results on a constant currency basis, as we present them, may not be comparable to similarly titled\n",
      "measures used by other companies and are not a measure of performance presented in accordance with GAAP.\n",
      "The following table presents the foreign exchange effect on international revenues and total revenues (in\n",
      "millions, except percentages):\n",
      "\n",
      "Three Months Ended\n",
      "March 31,\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "% Change from\n",
      "Prior Year\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$\n",
      "\n",
      "17,031 $\n",
      "\n",
      "20,317\n",
      "\n",
      "19 %\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "21,628\n",
      "\n",
      "27 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "10,455\n",
      "\n",
      "11,841\n",
      "\n",
      "13 %\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "12,440\n",
      "\n",
      "19 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "2,905\n",
      "\n",
      "3,842\n",
      "\n",
      "32 %\n",
      "\n",
      "Other Americas constant currency revenues\n",
      "\n",
      "3,923\n",
      "\n",
      "35 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "25,032\n",
      "\n",
      "31,733\n",
      "\n",
      "27 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "(109)\n",
      "\n",
      "278\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$\n",
      "\n",
      "55,314 $\n",
      "\n",
      "68,011\n",
      "\n",
      "23 %\n",
      "\n",
      "Revenues, excluding hedging effect\n",
      "\n",
      "$\n",
      "\n",
      "55,423 $\n",
      "\n",
      "67,733\n",
      "\n",
      "Exchange rate effect\n",
      "\n",
      "1,991\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$\n",
      "\n",
      "69,724\n",
      "\n",
      "26 %\n",
      "\n",
      "EMEA revenue growth from the three months ended March 31, 2021 to the three months ended March 31,\n",
      "2022 was unfavorably affected by changes in foreign currency exchange rates, primarily due to the U.S. dollar\n",
      "strengthening relative to the Euro.\n",
      "APAC revenue growth from the three months ended March 31, 2021 to the three months ended March 31,\n",
      "2022 was unfavorably affected by changes in foreign currency exchange rates, primarily due to the U.S. dollar\n",
      "strengthening relative to the Japanese yen.\n",
      "Other Americas revenue growth from the three months ended March 31, 2021 to the three months ended\n",
      "March 31, 2022 was not materially affected by changes in foreign currency exchange rates.\n",
      "Costs and Expenses\n",
      "Cost of Revenues\n",
      "The following table presents cost of revenues, including TAC (in millions, except percentages):\n",
      "\n",
      "Three Months Ended\n",
      "March 31,\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "TAC\n",
      "\n",
      "$\n",
      "\n",
      "9,712\n",
      "\n",
      "$\n",
      "\n",
      "11,990\n",
      "\n",
      "Other cost of revenues\n",
      "\n",
      "14,391\n",
      "\n",
      "17,609\n",
      "\n",
      "Total cost of revenues\n",
      "\n",
      "$\n",
      "\n",
      "24,103\n",
      "\n",
      "$\n",
      "\n",
      "29,599\n",
      "\n",
      "Total cost of revenues as a percentage of revenues\n",
      "\n",
      "43.6 %\n",
      "\n",
      "43.5 %\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Alphabet Inc.\n",
      "\n",
      "36\n",
      "-------------------------------------------------------------------------------\n",
      "Document 2\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/20230203_alphabet_10K.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "Constant currency revenue percentage change is calculated by determining the change in current period\n",
      "revenues over prior year comparable period revenues where current period foreign currency revenues are translated\n",
      "using prior year comparable period exchange rates and hedging effects are excluded from revenues of both periods.\n",
      "These results should be considered in addition to, not as a substitute for, results reported in accordance with\n",
      "GAAP. Results on a constant currency basis, as we present them, may not be comparable to similarly titled measures\n",
      "used by other companies and are not a measure of performance presented in accordance with GAAP.\n",
      "\n",
      "The following table presents the foreign exchange effect on international revenues and total revenues (in millions,\n",
      "except percentages):\n",
      "\n",
      "Year Ended December 31, 2022\n",
      "% Change from Prior Period\n",
      "\n",
      "Year Ended December 31,\n",
      "\n",
      "Less FX\n",
      "Effect\n",
      "\n",
      "Constant\n",
      "Currency\n",
      "Revenues\n",
      "\n",
      "As\n",
      "Reported\n",
      "\n",
      "Less\n",
      "Hedging\n",
      "Effect\n",
      "\n",
      "Less FX\n",
      "Effect\n",
      "\n",
      "Constant\n",
      "Currency\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "Revenues\n",
      "\n",
      "United States\n",
      "\n",
      "$ 117,854 $ 134,814 $\n",
      "\n",
      "0 $ 134,814\n",
      "\n",
      "14 %\n",
      "\n",
      "0 %\n",
      "\n",
      "14 %\n",
      "\n",
      "EMEA\n",
      "\n",
      "79,107\n",
      "\n",
      "82,062\n",
      "\n",
      "(8,979)\n",
      "\n",
      "91,041\n",
      "\n",
      "4 %\n",
      "\n",
      "(11) %\n",
      "\n",
      "15 %\n",
      "\n",
      "APAC\n",
      "\n",
      "46,123\n",
      "\n",
      "47,024\n",
      "\n",
      "(3,915)\n",
      "\n",
      "50,939\n",
      "\n",
      "2 %\n",
      "\n",
      "(8) %\n",
      "\n",
      "10 %\n",
      "\n",
      "Other Americas\n",
      "\n",
      "14,404\n",
      "\n",
      "16,976\n",
      "\n",
      "(430)\n",
      "\n",
      "17,406\n",
      "\n",
      "18 %\n",
      "\n",
      "(3) %\n",
      "\n",
      "21 %\n",
      "\n",
      "Revenues, excluding hedging effect\n",
      "\n",
      "257,488\n",
      "\n",
      "280,876\n",
      "\n",
      "(13,324)\n",
      "\n",
      "294,200\n",
      "\n",
      "9 %\n",
      "\n",
      "(5) %\n",
      "\n",
      "14 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "149\n",
      "\n",
      "1,960\n",
      "\n",
      "Total revenues(1)\n",
      "\n",
      "$ 257,637 $ 282,836\n",
      "\n",
      "$ 294,200\n",
      "\n",
      "10 %\n",
      "\n",
      "1 %\n",
      "\n",
      "(5) %\n",
      "\n",
      "14 %\n",
      "\n",
      "(1) Total constant currency revenues of $294.2 billion for 2022 increased $36.7 billion compared to $257.5 billion in revenues,\n",
      "excluding hedging effect for 2021.\n",
      "EMEA revenue growth was unfavorably affected by changes in foreign currency exchange rates, primarily due to\n",
      "the U.S. dollar strengthening relative to the Euro and the British pound.\n",
      "APAC revenue growth was unfavorably affected by changes in foreign currency exchange rates, primarily due to\n",
      "the U.S. dollar strengthening relative to the Japanese yen and the Australian dollar.\n",
      "Other Americas growth was unfavorably affected by changes in foreign currency exchange rates, primarily due to\n",
      "the U.S. dollar strengthening relative to the Argentine peso.\n",
      "Costs and Expenses\n",
      "Cost of Revenues\n",
      "The following table presents cost of revenues, including TAC (in millions, except percentages):\n",
      "Year Ended December 31,\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "TAC\n",
      "\n",
      "$\n",
      "\n",
      "45,566 $\n",
      "\n",
      "48,955\n",
      "\n",
      "Other cost of revenues\n",
      "\n",
      "65,373\n",
      "\n",
      "77,248\n",
      "\n",
      "Total cost of revenues\n",
      "\n",
      "$\n",
      "\n",
      "110,939 $\n",
      "\n",
      "126,203\n",
      "\n",
      "Total cost of revenues as a percentage of revenues\n",
      "\n",
      "43 %\n",
      "-------------------------------------------------------------------------------\n",
      "Document 3\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/20220726_alphabet_10Q.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "EMEA revenue growth from the three and six months ended June 30, 2021 to the three and six months ended\n",
      "June 30, 2022 was unfavorably affected by changes in foreign currency exchange rates, primarily due to the U.S.\n",
      "dollar strengthening relative to the Euro and British pound.\n",
      "APAC revenue growth from the three and six months ended June 30, 2021 to the three and six months ended\n",
      "June 30, 2022 was unfavorably affected by changes in foreign currency exchange rates, primarily due to the U.S.\n",
      "dollar strengthening relative to the Japanese yen.\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Alphabet Inc.\n",
      "\n",
      "39\n",
      "-------------------------------------------------------------------------------\n",
      "Document 4\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/20221025_alphabet_10Q.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "The following table presents the foreign exchange effect on international revenues and total revenues (in\n",
      "millions, except percentages):\n",
      "\n",
      "Three Months Ended\n",
      "\n",
      "Nine Months Ended\n",
      "\n",
      "September 30,\n",
      "\n",
      "September 30,\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "% Change\n",
      "from Prior\n",
      "Year\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "% Change\n",
      "from Prior\n",
      "Year\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$ 19,839 $ 19,450\n",
      "\n",
      "(2) % $ 55,954 $ 60,300\n",
      "\n",
      "8 %\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "22,093\n",
      "\n",
      "11 %\n",
      "\n",
      "$ 66,210\n",
      "\n",
      "18 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "11,705\n",
      "\n",
      "11,494\n",
      "\n",
      "(2) % $ 33,391 $ 35,045\n",
      "\n",
      "5 %\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "12,604\n",
      "\n",
      "8 %\n",
      "\n",
      "$ 37,510\n",
      "\n",
      "12 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "3,688\n",
      "\n",
      "4,138\n",
      "\n",
      "12 % $ 9,957 $ 12,320\n",
      "\n",
      "24 %\n",
      "\n",
      "Other Americas constant currency\n",
      "\n",
      "revenues\n",
      "\n",
      "4,303\n",
      "\n",
      "17 %\n",
      "\n",
      "$ 12,536\n",
      "\n",
      "26 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "29,824\n",
      "\n",
      "33,372\n",
      "\n",
      "12 % $ 83,064 $ 97,832\n",
      "\n",
      "18 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "62\n",
      "\n",
      "638\n",
      "\n",
      "$\n",
      "\n",
      "(54) $ 1,291\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$ 65,118 $ 69,092\n",
      "\n",
      "6 % $ 182,312 $ 206,788\n",
      "\n",
      "13 %\n",
      "\n",
      "Revenues, excluding hedging effect $ 65,056 $ 68,454\n",
      "\n",
      "$ 182,366 $ 205,497\n",
      "\n",
      "Exchange rate effect\n",
      "\n",
      "3,918\n",
      "\n",
      "8,591\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$ 72,372\n",
      "\n",
      "11 %\n",
      "\n",
      "$ 214,088\n",
      "\n",
      "17 %\n",
      "\n",
      "EMEA revenue growth from the three and nine months ended September 30, 2021 to the three and nine\n",
      "months ended September 30, 2022 was unfavorably affected by changes in foreign currency exchange rates,\n",
      "primarily due to the U.S. dollar strengthening relative to the Euro and British pound.\n",
      "APAC revenue growth from the three and nine months ended September 30, 2021 to the three and nine\n",
      "months ended September 30, 2022 was unfavorably affected by changes in foreign currency exchange rates,\n",
      "primarily due to the U.S. dollar strengthening relative to the Japanese yen.\n",
      "\n",
      "Other Americas revenue growth from the three and nine months ended September 30, 2021 to the three and\n",
      "nine months ended September 30, 2022 was unfavorably affected by the general strengthening of the U.S. dollar.\n",
      "Costs and Expenses\n",
      "Cost of Revenues\n",
      "The following table presents cost of revenues, including TAC (in millions, except percentages):\n",
      "\n",
      "Three Months Ended\n",
      "\n",
      "Nine Months Ended\n",
      "\n",
      "September 30,\n",
      "\n",
      "September 30,\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "TAC\n",
      "\n",
      "$ 11,498\n",
      "$ 11,826\n",
      "$ 32,139\n",
      "$ 36,030\n",
      "\n",
      "Other cost of revenues\n",
      "\n",
      "16,123\n",
      "\n",
      "19,332\n",
      "\n",
      "45,812\n",
      "\n",
      "54,831\n",
      "\n",
      "Total cost of revenues\n",
      "\n",
      "$ 27,621\n",
      "\n",
      "$ 31,158\n",
      "$ 77,951\n",
      "\n",
      "$ 90,861\n",
      "\n",
      "Total cost of revenues as a percentage of revenues\n",
      "\n",
      "42.4 %\n",
      "\n",
      "45.1 %\n",
      "\n",
      "42.8 %\n",
      "-------------------------------------------------------------------------------\n",
      "Document 5\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/2022Q2_alphabet_earnings_release.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "Comparison from the Quarter Ended June 30, 2021 to the Quarter Ended June 30, 2022\n",
      "\n",
      "Quarter Ended\n",
      "\n",
      "June 30, 2021 June 30, 2022\n",
      "\n",
      "% Change\n",
      "from Prior\n",
      "Year\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$\n",
      "\n",
      "19,084 $\n",
      "\n",
      "20,533\n",
      "\n",
      "8 %\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "22,489\n",
      "\n",
      "18 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "11,231\n",
      "\n",
      "11,710\n",
      "\n",
      "4 %\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "12,466\n",
      "\n",
      "11 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "3,364\n",
      "\n",
      "4,340\n",
      "\n",
      "29 %\n",
      "\n",
      "Other Americas constant currency revenues\n",
      "\n",
      "4,310\n",
      "\n",
      "28 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "28,208\n",
      "\n",
      "32,727\n",
      "\n",
      "16 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "(7)\n",
      "\n",
      "375\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$\n",
      "\n",
      "61,880 $\n",
      "\n",
      "69,685\n",
      "\n",
      "13 %\n",
      "\n",
      "Revenues, excluding hedging effect\n",
      "\n",
      "$\n",
      "\n",
      "61,887 $\n",
      "\n",
      "69,310\n",
      "\n",
      "Exchange rate effect\n",
      "\n",
      "2,682\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$\n",
      "\n",
      "71,992\n",
      "\n",
      "16 %\n",
      "\n",
      "Comparison from the Quarter Ended March 31, 2022 to the Quarter Ended June 30, 2022\n",
      "\n",
      "Quarter Ended\n",
      "\n",
      "March 31, 2022 June 30, 2022\n",
      "\n",
      "% Change\n",
      "from Prior\n",
      "Quarter\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$\n",
      "\n",
      "20,317 $\n",
      "\n",
      "20,533\n",
      "\n",
      "1 %\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "21,164\n",
      "\n",
      "4 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "11,841\n",
      "\n",
      "11,710\n",
      "\n",
      "(1) %\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "12,044\n",
      "\n",
      "2 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "3,842\n",
      "\n",
      "4,340\n",
      "\n",
      "13 %\n",
      "\n",
      "Other Americas constant currency revenues\n",
      "\n",
      "4,231\n",
      "\n",
      "10 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "31,733\n",
      "\n",
      "32,727\n",
      "\n",
      "3 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "278\n",
      "\n",
      "375\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$\n",
      "\n",
      "68,011 $\n",
      "\n",
      "69,685\n",
      "\n",
      "2 %\n",
      "\n",
      "Revenues, excluding hedging effect\n",
      "\n",
      "$\n",
      "\n",
      "67,733 $\n",
      "\n",
      "69,310\n",
      "\n",
      "Exchange rate effect\n",
      "\n",
      "856\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$\n",
      "\n",
      "70,166\n",
      "\n",
      "4 %\n",
      "\n",
      "Non-GAAP constant currency revenues and percentage change: We define non-GAAP constant currency revenues as total\n",
      "revenues excluding the effect of foreign exchange rate movements and hedging activities, and we use it to determine the\n",
      "constant currency revenue percentage change on year-on-year and quarter-on-quarter basis. Non-GAAP constant currency\n",
      "revenues are calculated by translating current quarter revenues using prior period exchange rates and excluding any hedging\n",
      "effect recognized in the current quarter. Constant currency revenue percentage change is calculated by determining the increase\n",
      "in current quarter non-GAAP constant currency revenues over prior period revenues, excluding any hedging effect recognized in\n",
      "the prior period.\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Were 2022 EMEA revenues higher than 2022 APAC revenues?\"\n",
    "\n",
    "results = qa_chain({\"query\": search_query})\n",
    "format_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import logging\n",
    "import subprocess\n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "http.client.HTTPConnection.debuglevel = 1\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# def create_data_store(project_id: str, data_store_id: str, display_name: str):\n",
    "#     \"\"\"\n",
    "#     Create a data store in the default collection of the project.\n",
    "#     Args:\n",
    "#         project_id: The project ID of the project to create the data store in.\n",
    "#         data_store_id: The ID of the data store to create.\n",
    "#         display_name: The display name of the data store to create.\n",
    "#     Returns:\n",
    "#         None\n",
    "#     e.g.:\n",
    "#     from dt_gen_ai_hackathon_helper.vertex_ai_search import vertex_ai_search\n",
    "#     display_name = \"data_store_test_from_notebook\"\n",
    "#     data_store_name = \"alphabet_investor_pdfs\"\n",
    "#     hackathon_team_name = \"team_1\"\n",
    "#     vertex_ai_search.create_data_store(PROJECT_ID, data_store_name + \"_\" + hackathon_team_name, display_name)\n",
    "#     \"\"\"\n",
    "#     # Get the access token from gcloud\n",
    "#     process = subprocess.Popen([\"gcloud\", \"auth\", \"print-access-token\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#     out, err = process.communicate()\n",
    "#     access_token = out.decode('utf-8').strip()\n",
    "\n",
    "#     # Define headers and payload\n",
    "#     headers = {\n",
    "#         \"Authorization\": f\"Bearer {access_token}\",\n",
    "#         \"X-Goog-User-Project\": project_id,\n",
    "#         \"Content-Type\": \"application/json\"\n",
    "#     }\n",
    "\n",
    "#     payload = {\n",
    "#         \"displayName\": display_name,\n",
    "#         \"industryVertical\": \"GENERIC\",\n",
    "#         \"solutionTypes\": [\"SOLUTION_TYPE_SEARCH\"],\n",
    "#         \"contentConfig\": \"CONTENT_REQUIRED\",\n",
    "#         \"searchTier\": \"STANDARD\",\n",
    "#         \"searchAddOns\": [\"LLM\"]\n",
    "#     }\n",
    "\n",
    "#     parent = f\"projects/{project_id}/locations/global/collections/default_collection\"\n",
    "#     url = f\"https://discoveryengine.googleapis.com/v1alpha/{parent}/dataStores/{data_store_id}\"\n",
    "#     # Make the request\n",
    "#     response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "#     if response.status_code == 200 or response.status_code == 201:\n",
    "#         print(\"Data store created successfully.\")\n",
    "#         print(json.dumps(response.json(), indent=4))\n",
    "#     else:\n",
    "#         logging.info(f\"Failed to create data store. Status code: {response.status_code}\")\n",
    "#         logging.info(response.text)\n",
    "\n",
    "\n",
    "def ingest_documents(\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        data_store_id: str,\n",
    "        gcs_uri: Optional[str] = None,\n",
    "        bigquery_dataset: Optional[str] = None,\n",
    "        bigquery_table: Optional[str] = None,\n",
    "        data_schema: str = \"content\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Ingest documents into a data store.\n",
    "    Args:\n",
    "        project_id: The project ID of the project to import documents into.\n",
    "        location: The location of the data store to import documents into.\n",
    "        data_store_id: The ID of the data store to import documents into.\n",
    "        gcs_uri: The GCS URI of the documents to import.\n",
    "        bigquery_dataset: The BigQuery dataset of the documents to import.\n",
    "        bigquery_table: The BigQuery table of the documents to import.\n",
    "        data_schema: The data schema of the documents to import.\n",
    "    Returns:\n",
    "        The operation name of the import documents operation.\n",
    "    \"\"\"\n",
    "    #  For more information, refer to:\n",
    "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.DocumentServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the search engine branch.\n",
    "    # e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}\n",
    "    parent = client.branch_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        data_store=data_store_id,\n",
    "        branch=\"default_branch\",\n",
    "    )\n",
    "\n",
    "    print(parent)\n",
    "\n",
    "    if gcs_uri:\n",
    "        request = discoveryengine.ImportDocumentsRequest(\n",
    "            parent=parent,\n",
    "            gcs_source=discoveryengine.GcsSource(\n",
    "                input_uris=[gcs_uri], data_schema=data_schema\n",
    "            ),\n",
    "            # Options: `FULL`, `INCREMENTAL`\n",
    "            reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\n",
    "        )\n",
    "    else:\n",
    "        request = discoveryengine.ImportDocumentsRequest(\n",
    "            parent=parent,\n",
    "            bigquery_source=discoveryengine.BigQuerySource(\n",
    "                project_id=project_id,\n",
    "                dataset_id=bigquery_dataset,\n",
    "                table_id=bigquery_table,\n",
    "                data_schema=\"custom\",\n",
    "            ),\n",
    "            # Options: `FULL`, `INCREMENTAL`\n",
    "            reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\n",
    "        )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.import_documents(request=request)\n",
    "\n",
    "    print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
    "    response = operation.result()\n",
    "\n",
    "    # Once the operation is complete,\n",
    "    # get information from operation metadata\n",
    "    metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)\n",
    "\n",
    "    # Handle the response\n",
    "    print(response)\n",
    "    print(metadata)\n",
    "\n",
    "    return operation.operation.name\n",
    "\n",
    "\n",
    "# def create_search_ai_app(project_id: str, display_name: str, data_store_id: str, solution_type_search: str):\n",
    "#     \"\"\"\n",
    "#     Create a search AI app.\n",
    "#     Args:\n",
    "#         project_id: The project ID of the project to create the search AI app in.\n",
    "#         display_name: The display name of the search AI app to create.\n",
    "#         data_store_id: The ID of the data store to create the search AI app in.\n",
    "#         solution_type_search: The solution type of the search AI app to create.\n",
    "#     Returns:\n",
    "#         None\n",
    "#     e.g.:\n",
    "#     from dt_gen_ai_hackathon_helper.vertex_ai_search import vertex_ai_search\n",
    "#     display_name = \"app_team_1\"\n",
    "#     solution_type_search = \"SOLUTION_TYPE_SEARCH\"\n",
    "#     vertex_ai_search.create_search_ai_app(PROJECT_ID, display_name, data_store_id, solution_type_search)\n",
    "#     \"\"\"\n",
    "#     # Get the access token from gcloud\n",
    "#     process = subprocess.Popen([\"gcloud\", \"auth\", \"print-access-token\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#     out, err = process.communicate()\n",
    "#     access_token = out.decode('utf-8').strip()\n",
    "\n",
    "#     # Define headers and payload\n",
    "#     headers = {\n",
    "#         \"Authorization\": f\"Bearer {access_token}\",\n",
    "#         \"Content-Type\": \"application/json\",\n",
    "#         \"X-Goog-User-Project\": project_id,\n",
    "#     }\n",
    "\n",
    "#     payload = {\n",
    "#         \"displayName\": display_name,\n",
    "#         \"dataStoreIds\": [data_store_id],\n",
    "#         \"solutionType\": [solution_type_search]\n",
    "#     }\n",
    "\n",
    "#     url = f\"https://discoveryengine.googleapis.com/v1alpha/projects/{project_id}/locations/global/collections/default_collection/engines?engineId={data_store_id}\"\n",
    "\n",
    "#     try:\n",
    "#         response = requests.post(url, headers=headers, json=payload)\n",
    "#         response.raise_for_status()  # Raises HTTPError for bad responses (4xx and 5xx)\n",
    "#         logging.info(f\"Success: {response.json()}\")\n",
    "#     except requests.HTTPError as http_err:\n",
    "#         logging.error(f\"HTTP error occurred: {http_err}\")\n",
    "#     except Exception as err:\n",
    "#         logging.error(f\"An error occurred: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below and then go to the Vertex Search UI to see the original data store. Check in `Activity` to see how new data has been added to the store.\n",
    "\n",
    "Then, use the cells below to query this new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/dt-gen-ai-hackathon-dev/locations/global/dataStores/dt-gen-ai-hackathon-t1_1696592331863/branches/default_branch\n",
      "send: b'POST /token HTTP/1.1\\r\\nHost: oauth2.googleapis.com\\r\\nUser-Agent: python-requests/2.31.0\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nConnection: keep-alive\\r\\nContent-Type: application/x-www-form-urlencoded\\r\\nx-goog-api-client: gl-python/3.11.2 auth/2.23.0 cred-type/u\\r\\nContent-Length: 268\\r\\n\\r\\n'\n",
      "send: b'grant_type=refresh_token&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&client_secret=d-FL95Q19q7MQmFpd7hHD0Ty&refresh_token=1%2F%2F03WA67yybbuszCgYIARAAGAMSNwF-L9Ir2deDLhEKnZc1mpO3B4JMdV806w0W6O2-S6hc4J2G7N_ytpglcNR55GyKx8MqR8wtwiI'\n",
      "reply: 'HTTP/1.1 200 OK\\r\\n'\n",
      "header: Cache-Control: no-cache, no-store, max-age=0, must-revalidate\n",
      "header: Pragma: no-cache\n",
      "header: Date: Sun, 08 Oct 2023 11:12:32 GMT\n",
      "header: Expires: Mon, 01 Jan 1990 00:00:00 GMT\n",
      "header: Content-Type: application/json; charset=utf-8\n",
      "header: Vary: Origin\n",
      "header: Vary: X-Origin\n",
      "header: Vary: Referer\n",
      "header: Content-Encoding: gzip\n",
      "header: Server: scaffolding on HTTPServer2\n",
      "header: X-XSS-Protection: 0\n",
      "header: X-Frame-Options: SAMEORIGIN\n",
      "header: X-Content-Type-Options: nosniff\n",
      "header: Alt-Svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000\n",
      "header: Transfer-Encoding: chunked\n",
      "Waiting for operation to complete: projects/592911206780/locations/global/collections/default_collection/dataStores/dt-gen-ai-hackathon-t1_1696592331863/branches/0/operations/import-documents-2786547389668885060\n",
      "error_config {\n",
      "  gcs_prefix: \"gs://592911206780_eu_import_content/errors2786547389668885551\"\n",
      "}\n",
      "\n",
      "create_time {\n",
      "  seconds: 1696763555\n",
      "  nanos: 62320000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1696763559\n",
      "  nanos: 874485000\n",
      "}\n",
      "success_count: 15\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projects/592911206780/locations/global/collections/default_collection/dataStores/dt-gen-ai-hackathon-t1_1696592331863/branches/0/operations/import-documents-2786547389668885060'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dt_gen_ai_hackathon_helper.vertex_ai_search import vertex_ai_search\n",
    "location = \"global\"\n",
    "gcs_uri = \"gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/*.pdf\"\n",
    "\n",
    "ingest_documents(project_id=PROJECT_ID, location=location, data_store_id=data_store_id, gcs_uri=gcs_uri, data_schema= \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Answer: Short answer: Yes\n",
      "Details:\n",
      "EMEA revenues in 2022 were $82,062 million, while APAC revenues were $47,024 million.\n",
      "Used 5 relevant documents.\n",
      "*******************************************************************************\n",
      "-------------------------------------------------------------------------------\n",
      "Document 1\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/20220427_alphabet_10Q.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "Constant currency revenue percentage change is calculated by determining the change in current period\n",
      "revenues over prior year comparable period revenues where current period foreign currency revenues are\n",
      "translated using prior year comparable period exchange rates and hedging effects are excluded from revenues of\n",
      "both periods.\n",
      "These results should be considered in addition to, not as a substitute for, results reported in accordance with\n",
      "GAAP. Results on a constant currency basis, as we present them, may not be comparable to similarly titled\n",
      "measures used by other companies and are not a measure of performance presented in accordance with GAAP.\n",
      "The following table presents the foreign exchange effect on international revenues and total revenues (in\n",
      "millions, except percentages):\n",
      "\n",
      "Three Months Ended\n",
      "March 31,\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "% Change from\n",
      "Prior Year\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$\n",
      "\n",
      "17,031 $\n",
      "\n",
      "20,317\n",
      "\n",
      "19 %\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "21,628\n",
      "\n",
      "27 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "10,455\n",
      "\n",
      "11,841\n",
      "\n",
      "13 %\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "12,440\n",
      "\n",
      "19 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "2,905\n",
      "\n",
      "3,842\n",
      "\n",
      "32 %\n",
      "\n",
      "Other Americas constant currency revenues\n",
      "\n",
      "3,923\n",
      "\n",
      "35 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "25,032\n",
      "\n",
      "31,733\n",
      "\n",
      "27 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "(109)\n",
      "\n",
      "278\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$\n",
      "\n",
      "55,314 $\n",
      "\n",
      "68,011\n",
      "\n",
      "23 %\n",
      "\n",
      "Revenues, excluding hedging effect\n",
      "\n",
      "$\n",
      "\n",
      "55,423 $\n",
      "\n",
      "67,733\n",
      "\n",
      "Exchange rate effect\n",
      "\n",
      "1,991\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$\n",
      "\n",
      "69,724\n",
      "\n",
      "26 %\n",
      "\n",
      "EMEA revenue growth from the three months ended March 31, 2021 to the three months ended March 31,\n",
      "2022 was unfavorably affected by changes in foreign currency exchange rates, primarily due to the U.S. dollar\n",
      "strengthening relative to the Euro.\n",
      "APAC revenue growth from the three months ended March 31, 2021 to the three months ended March 31,\n",
      "2022 was unfavorably affected by changes in foreign currency exchange rates, primarily due to the U.S. dollar\n",
      "strengthening relative to the Japanese yen.\n",
      "Other Americas revenue growth from the three months ended March 31, 2021 to the three months ended\n",
      "March 31, 2022 was not materially affected by changes in foreign currency exchange rates.\n",
      "Costs and Expenses\n",
      "Cost of Revenues\n",
      "The following table presents cost of revenues, including TAC (in millions, except percentages):\n",
      "\n",
      "Three Months Ended\n",
      "March 31,\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "TAC\n",
      "\n",
      "$\n",
      "\n",
      "9,712\n",
      "\n",
      "$\n",
      "\n",
      "11,990\n",
      "\n",
      "Other cost of revenues\n",
      "\n",
      "14,391\n",
      "\n",
      "17,609\n",
      "\n",
      "Total cost of revenues\n",
      "\n",
      "$\n",
      "\n",
      "24,103\n",
      "\n",
      "$\n",
      "\n",
      "29,599\n",
      "\n",
      "Total cost of revenues as a percentage of revenues\n",
      "\n",
      "43.6 %\n",
      "\n",
      "43.5 %\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Alphabet Inc.\n",
      "\n",
      "36\n",
      "-------------------------------------------------------------------------------\n",
      "Document 2\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/20230203_alphabet_10K.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "Constant currency revenue percentage change is calculated by determining the change in current period\n",
      "revenues over prior year comparable period revenues where current period foreign currency revenues are translated\n",
      "using prior year comparable period exchange rates and hedging effects are excluded from revenues of both periods.\n",
      "These results should be considered in addition to, not as a substitute for, results reported in accordance with\n",
      "GAAP. Results on a constant currency basis, as we present them, may not be comparable to similarly titled measures\n",
      "used by other companies and are not a measure of performance presented in accordance with GAAP.\n",
      "\n",
      "The following table presents the foreign exchange effect on international revenues and total revenues (in millions,\n",
      "except percentages):\n",
      "\n",
      "Year Ended December 31, 2022\n",
      "% Change from Prior Period\n",
      "\n",
      "Year Ended December 31,\n",
      "\n",
      "Less FX\n",
      "Effect\n",
      "\n",
      "Constant\n",
      "Currency\n",
      "Revenues\n",
      "\n",
      "As\n",
      "Reported\n",
      "\n",
      "Less\n",
      "Hedging\n",
      "Effect\n",
      "\n",
      "Less FX\n",
      "Effect\n",
      "\n",
      "Constant\n",
      "Currency\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "Revenues\n",
      "\n",
      "United States\n",
      "\n",
      "$ 117,854 $ 134,814 $\n",
      "\n",
      "0 $ 134,814\n",
      "\n",
      "14 %\n",
      "\n",
      "0 %\n",
      "\n",
      "14 %\n",
      "\n",
      "EMEA\n",
      "\n",
      "79,107\n",
      "\n",
      "82,062\n",
      "\n",
      "(8,979)\n",
      "\n",
      "91,041\n",
      "\n",
      "4 %\n",
      "\n",
      "(11) %\n",
      "\n",
      "15 %\n",
      "\n",
      "APAC\n",
      "\n",
      "46,123\n",
      "\n",
      "47,024\n",
      "\n",
      "(3,915)\n",
      "\n",
      "50,939\n",
      "\n",
      "2 %\n",
      "\n",
      "(8) %\n",
      "\n",
      "10 %\n",
      "\n",
      "Other Americas\n",
      "\n",
      "14,404\n",
      "\n",
      "16,976\n",
      "\n",
      "(430)\n",
      "\n",
      "17,406\n",
      "\n",
      "18 %\n",
      "\n",
      "(3) %\n",
      "\n",
      "21 %\n",
      "\n",
      "Revenues, excluding hedging effect\n",
      "\n",
      "257,488\n",
      "\n",
      "280,876\n",
      "\n",
      "(13,324)\n",
      "\n",
      "294,200\n",
      "\n",
      "9 %\n",
      "\n",
      "(5) %\n",
      "\n",
      "14 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "149\n",
      "\n",
      "1,960\n",
      "\n",
      "Total revenues(1)\n",
      "\n",
      "$ 257,637 $ 282,836\n",
      "\n",
      "$ 294,200\n",
      "\n",
      "10 %\n",
      "\n",
      "1 %\n",
      "\n",
      "(5) %\n",
      "\n",
      "14 %\n",
      "\n",
      "(1) Total constant currency revenues of $294.2 billion for 2022 increased $36.7 billion compared to $257.5 billion in revenues,\n",
      "excluding hedging effect for 2021.\n",
      "EMEA revenue growth was unfavorably affected by changes in foreign currency exchange rates, primarily due to\n",
      "the U.S. dollar strengthening relative to the Euro and the British pound.\n",
      "APAC revenue growth was unfavorably affected by changes in foreign currency exchange rates, primarily due to\n",
      "the U.S. dollar strengthening relative to the Japanese yen and the Australian dollar.\n",
      "Other Americas growth was unfavorably affected by changes in foreign currency exchange rates, primarily due to\n",
      "the U.S. dollar strengthening relative to the Argentine peso.\n",
      "Costs and Expenses\n",
      "Cost of Revenues\n",
      "The following table presents cost of revenues, including TAC (in millions, except percentages):\n",
      "Year Ended December 31,\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "TAC\n",
      "\n",
      "$\n",
      "\n",
      "45,566 $\n",
      "\n",
      "48,955\n",
      "\n",
      "Other cost of revenues\n",
      "\n",
      "65,373\n",
      "\n",
      "77,248\n",
      "\n",
      "Total cost of revenues\n",
      "\n",
      "$\n",
      "\n",
      "110,939 $\n",
      "\n",
      "126,203\n",
      "\n",
      "Total cost of revenues as a percentage of revenues\n",
      "\n",
      "43 %\n",
      "-------------------------------------------------------------------------------\n",
      "Document 3\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/20220726_alphabet_10Q.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "EMEA revenue growth from the three and six months ended June 30, 2021 to the three and six months ended\n",
      "June 30, 2022 was unfavorably affected by changes in foreign currency exchange rates, primarily due to the U.S.\n",
      "dollar strengthening relative to the Euro and British pound.\n",
      "APAC revenue growth from the three and six months ended June 30, 2021 to the three and six months ended\n",
      "June 30, 2022 was unfavorably affected by changes in foreign currency exchange rates, primarily due to the U.S.\n",
      "dollar strengthening relative to the Japanese yen.\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Alphabet Inc.\n",
      "\n",
      "39\n",
      "-------------------------------------------------------------------------------\n",
      "Document 4\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/20221025_alphabet_10Q.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "The following table presents the foreign exchange effect on international revenues and total revenues (in\n",
      "millions, except percentages):\n",
      "\n",
      "Three Months Ended\n",
      "\n",
      "Nine Months Ended\n",
      "\n",
      "September 30,\n",
      "\n",
      "September 30,\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "% Change\n",
      "from Prior\n",
      "Year\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "% Change\n",
      "from Prior\n",
      "Year\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$ 19,839 $ 19,450\n",
      "\n",
      "(2) % $ 55,954 $ 60,300\n",
      "\n",
      "8 %\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "22,093\n",
      "\n",
      "11 %\n",
      "\n",
      "$ 66,210\n",
      "\n",
      "18 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "11,705\n",
      "\n",
      "11,494\n",
      "\n",
      "(2) % $ 33,391 $ 35,045\n",
      "\n",
      "5 %\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "12,604\n",
      "\n",
      "8 %\n",
      "\n",
      "$ 37,510\n",
      "\n",
      "12 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "3,688\n",
      "\n",
      "4,138\n",
      "\n",
      "12 % $ 9,957 $ 12,320\n",
      "\n",
      "24 %\n",
      "\n",
      "Other Americas constant currency\n",
      "\n",
      "revenues\n",
      "\n",
      "4,303\n",
      "\n",
      "17 %\n",
      "\n",
      "$ 12,536\n",
      "\n",
      "26 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "29,824\n",
      "\n",
      "33,372\n",
      "\n",
      "12 % $ 83,064 $ 97,832\n",
      "\n",
      "18 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "62\n",
      "\n",
      "638\n",
      "\n",
      "$\n",
      "\n",
      "(54) $ 1,291\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$ 65,118 $ 69,092\n",
      "\n",
      "6 % $ 182,312 $ 206,788\n",
      "\n",
      "13 %\n",
      "\n",
      "Revenues, excluding hedging effect $ 65,056 $ 68,454\n",
      "\n",
      "$ 182,366 $ 205,497\n",
      "\n",
      "Exchange rate effect\n",
      "\n",
      "3,918\n",
      "\n",
      "8,591\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$ 72,372\n",
      "\n",
      "11 %\n",
      "\n",
      "$ 214,088\n",
      "\n",
      "17 %\n",
      "\n",
      "EMEA revenue growth from the three and nine months ended September 30, 2021 to the three and nine\n",
      "months ended September 30, 2022 was unfavorably affected by changes in foreign currency exchange rates,\n",
      "primarily due to the U.S. dollar strengthening relative to the Euro and British pound.\n",
      "APAC revenue growth from the three and nine months ended September 30, 2021 to the three and nine\n",
      "months ended September 30, 2022 was unfavorably affected by changes in foreign currency exchange rates,\n",
      "primarily due to the U.S. dollar strengthening relative to the Japanese yen.\n",
      "\n",
      "Other Americas revenue growth from the three and nine months ended September 30, 2021 to the three and\n",
      "nine months ended September 30, 2022 was unfavorably affected by the general strengthening of the U.S. dollar.\n",
      "Costs and Expenses\n",
      "Cost of Revenues\n",
      "The following table presents cost of revenues, including TAC (in millions, except percentages):\n",
      "\n",
      "Three Months Ended\n",
      "\n",
      "Nine Months Ended\n",
      "\n",
      "September 30,\n",
      "\n",
      "September 30,\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "2021\n",
      "\n",
      "2022\n",
      "\n",
      "TAC\n",
      "\n",
      "$ 11,498\n",
      "$ 11,826\n",
      "$ 32,139\n",
      "$ 36,030\n",
      "\n",
      "Other cost of revenues\n",
      "\n",
      "16,123\n",
      "\n",
      "19,332\n",
      "\n",
      "45,812\n",
      "\n",
      "54,831\n",
      "\n",
      "Total cost of revenues\n",
      "\n",
      "$ 27,621\n",
      "\n",
      "$ 31,158\n",
      "$ 77,951\n",
      "\n",
      "$ 90,861\n",
      "\n",
      "Total cost of revenues as a percentage of revenues\n",
      "\n",
      "42.4 %\n",
      "\n",
      "45.1 %\n",
      "\n",
      "42.8 %\n",
      "-------------------------------------------------------------------------------\n",
      "Document 5\n",
      "-------------------------------------------------------------------------------\n",
      "Source of content: gs://dt-gen-ai-hackathon-pdf-datasets/alphabet_investor_pdfs_2022_2023/2022Q2_alphabet_earnings_release.pdf\n",
      "-------------------------------------------------------------------------------\n",
      "Comparison from the Quarter Ended June 30, 2021 to the Quarter Ended June 30, 2022\n",
      "\n",
      "Quarter Ended\n",
      "\n",
      "June 30, 2021 June 30, 2022\n",
      "\n",
      "% Change\n",
      "from Prior\n",
      "Year\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$\n",
      "\n",
      "19,084 $\n",
      "\n",
      "20,533\n",
      "\n",
      "8 %\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "22,489\n",
      "\n",
      "18 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "11,231\n",
      "\n",
      "11,710\n",
      "\n",
      "4 %\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "12,466\n",
      "\n",
      "11 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "3,364\n",
      "\n",
      "4,340\n",
      "\n",
      "29 %\n",
      "\n",
      "Other Americas constant currency revenues\n",
      "\n",
      "4,310\n",
      "\n",
      "28 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "28,208\n",
      "\n",
      "32,727\n",
      "\n",
      "16 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "(7)\n",
      "\n",
      "375\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$\n",
      "\n",
      "61,880 $\n",
      "\n",
      "69,685\n",
      "\n",
      "13 %\n",
      "\n",
      "Revenues, excluding hedging effect\n",
      "\n",
      "$\n",
      "\n",
      "61,887 $\n",
      "\n",
      "69,310\n",
      "\n",
      "Exchange rate effect\n",
      "\n",
      "2,682\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$\n",
      "\n",
      "71,992\n",
      "\n",
      "16 %\n",
      "\n",
      "Comparison from the Quarter Ended March 31, 2022 to the Quarter Ended June 30, 2022\n",
      "\n",
      "Quarter Ended\n",
      "\n",
      "March 31, 2022 June 30, 2022\n",
      "\n",
      "% Change\n",
      "from Prior\n",
      "Quarter\n",
      "\n",
      "EMEA revenues\n",
      "\n",
      "$\n",
      "\n",
      "20,317 $\n",
      "\n",
      "20,533\n",
      "\n",
      "1 %\n",
      "\n",
      "EMEA constant currency revenues\n",
      "\n",
      "21,164\n",
      "\n",
      "4 %\n",
      "\n",
      "APAC revenues\n",
      "\n",
      "11,841\n",
      "\n",
      "11,710\n",
      "\n",
      "(1) %\n",
      "\n",
      "APAC constant currency revenues\n",
      "\n",
      "12,044\n",
      "\n",
      "2 %\n",
      "\n",
      "Other Americas revenues\n",
      "\n",
      "3,842\n",
      "\n",
      "4,340\n",
      "\n",
      "13 %\n",
      "\n",
      "Other Americas constant currency revenues\n",
      "\n",
      "4,231\n",
      "\n",
      "10 %\n",
      "\n",
      "United States revenues\n",
      "\n",
      "31,733\n",
      "\n",
      "32,727\n",
      "\n",
      "3 %\n",
      "\n",
      "Hedging gains (losses)\n",
      "\n",
      "278\n",
      "\n",
      "375\n",
      "\n",
      "Total revenues\n",
      "\n",
      "$\n",
      "\n",
      "68,011 $\n",
      "\n",
      "69,685\n",
      "\n",
      "2 %\n",
      "\n",
      "Revenues, excluding hedging effect\n",
      "\n",
      "$\n",
      "\n",
      "67,733 $\n",
      "\n",
      "69,310\n",
      "\n",
      "Exchange rate effect\n",
      "\n",
      "856\n",
      "\n",
      "Total constant currency revenues\n",
      "\n",
      "$\n",
      "\n",
      "70,166\n",
      "\n",
      "4 %\n",
      "\n",
      "Non-GAAP constant currency revenues and percentage change: We define non-GAAP constant currency revenues as total\n",
      "revenues excluding the effect of foreign exchange rate movements and hedging activities, and we use it to determine the\n",
      "constant currency revenue percentage change on year-on-year and quarter-on-quarter basis. Non-GAAP constant currency\n",
      "revenues are calculated by translating current quarter revenues using prior period exchange rates and excluding any hedging\n",
      "effect recognized in the current quarter. Constant currency revenue percentage change is calculated by determining the increase\n",
      "in current quarter non-GAAP constant currency revenues over prior period revenues, excluding any hedging effect recognized in\n",
      "the prior period.\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Were 2022 EMEA revenues higher than 2022 APAC revenues?\"\n",
    "\n",
    "results = qa_chain({\"query\": search_query})\n",
    "format_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Building the user interface\n",
    "\n",
    "As building a UI is outside of the scope for this hackathon, a templated GUI using [Gradio](https://gradio.app/) is provided for your knowledge worker.\n",
    "\n",
    "**➡️ Your task:** Execute the cells below to launch the user interface.\n",
    "\n",
    "**❗ Note:** The following cell will run until manually stopped. Remember to halt it before moving onto the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_01_B_TEMPLATE = \"\"\"\\\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt_b = PromptTemplate.from_template(\n",
    "    template=TASK_01_BASIC_TEMPLATE + \"\\n\" + TASK_01_B_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "\n",
    "def create_qa_vertex_ai_search_chain(condense_question_prompt, k=10, temperature=0.0):\n",
    "    \"\"\"Create a Q&A conversation chain using the VertexAI LLM.\n",
    "\n",
    "    Arguments:\n",
    "        vector_store (object): The vectorstore containing our knowledge.\n",
    "        condense_question_prompt (PromptTemplate): The prompt template used to prompt engineer our LLM to respond in a certain tone, etc.\n",
    "        k (int): the 'k' value indicates the number of sources to use per query. 'k' as in 'k-nearest-neighbours' to the query in the embedding space.\n",
    "        temperature (float): the degree of randomness introduced into the LLM response.\n",
    "    \"\"\"\n",
    "    retriever = GoogleCloudEnterpriseSearchRetriever(\n",
    "        project_id=PROJECT_ID, search_engine_id=data_store_id, max_documents=k\n",
    "    )\n",
    "\n",
    "    # The selected Google model uses embedded documents related to the query\n",
    "    # It parses these documents in order to answer the user question.\n",
    "    # We use the VertexAI LLM, however other models can be substituted here\n",
    "    llm = VertexAI(model_name=MODEL, k=k, temperature=temperature)\n",
    "\n",
    "    # A conversation retrieval chain keeps a history of Q&A / conversation\n",
    "    # This allows for contextual questions such as \"give an example of that (previous response)\".\n",
    "    # The chain is also set to return the source documents used in generating an output\n",
    "    # This allows for explainability behind model output.\n",
    "    conversational_retrieval = ConversationalRetrievalChain.from_llm(\n",
    "        # condense_question_prompt=condense_question_prompt,\n",
    "        llm=llm, retriever=retriever,\n",
    "        return_source_documents=True\n",
    "        )\n",
    "    return conversational_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# from dt_gen_ai_hackathon_helper.chains.chains import create_qa_chain\n",
    "# from dt_gen_ai_hackathon_helper.prompts.prompts import TASK_01_PROMPT\n",
    "\n",
    "\n",
    "class View:\n",
    "    def __init__(self, qa_chain=None, vector_store=None):\n",
    "        if vector_store:\n",
    "            # condense_question_prompt = PromptTemplate.from_template(TASK_01_PROMPT)\n",
    "            self.qa_chain = create_qa_chain(\n",
    "                vector_store, \n",
    "                # condense_question_prompt\n",
    "                )\n",
    "        elif qa_chain:\n",
    "            self.qa_chain = qa_chain\n",
    "        else:\n",
    "            raise Exception(\"Must pass either qa_chain or vector_store as argument.\")\n",
    "\n",
    "    def q_a(self, question: str, history: list):\n",
    "        # map history (list of lists) to expected format of chat_history (list of tuples)\n",
    "        chat_history = map(tuple, history)\n",
    "\n",
    "        # Query the LLM to get a response\n",
    "        # First the Q&A chain will collect documents semantically similar to the question\n",
    "        # Then it will ask the LLM to use this data to answer the user question\n",
    "        # We also provide chat history as further context\n",
    "        response = self.qa_chain(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"chat_history\": chat_history,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Format source documents (sources of excerpts passed to the LLM) into links the user can validate\n",
    "        # Strip index.html so URLs terminate in the parent folder\n",
    "        # Strip https:// and http:// and replace with https:// to enforce https protocol and catch cases where https:// is present or not present\n",
    "        sources = [\n",
    "            \"[https://{0}](https://{0})\".format(\n",
    "                doc.metadata[\"source\"].replace(\"index.html\", \"\").replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "            )\n",
    "            for doc in response[\"source_documents\"]\n",
    "        ]\n",
    "\n",
    "        # Return the LLM answer, and list of sources used (formatted as a string)\n",
    "        return response[\"answer\"], \"\\n\\n\".join(sources)\n",
    "\n",
    "    def submit(self, msg, chatbot):\n",
    "        # First create a new entry in the conversation log\n",
    "        msg, chatbot = self.user(msg, chatbot)\n",
    "        # Then get the chatbot response to the user question\n",
    "        chatbot = self.bot(chatbot)\n",
    "        return msg, chatbot\n",
    "\n",
    "    def user(self, user_message, history):\n",
    "        # Return \"\" to clear the user input, and add the user question to the conversation history\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(self, history):\n",
    "        # Get the user question from conversation history\n",
    "        user_message = history[-1][0]\n",
    "        # Get the response and sources used to answer the user question\n",
    "        bot_message, bot_sources = self.q_a(user_message, history[:-1])\n",
    "\n",
    "        # Using a template, format the response and sources together\n",
    "        bot_template = (\n",
    "            \"{0}\\n\\n<details><summary><b>Sources</b></summary>\\n\\n{1}</details>\"\n",
    "        )\n",
    "        # Place the response into the conversation history and return\n",
    "        history[-1][1] = bot_template.format(bot_message, bot_sources)\n",
    "        return history\n",
    "\n",
    "    def launch_interface(self, share=True, debug=True):\n",
    "        # Build a simple GradIO app that accepts user input and queries the LLM\n",
    "        # Then displays the response in a ChatBot interface, with markdown support.\n",
    "        with gr.Blocks(theme=gr.themes.Base()) as demo:\n",
    "            # Set a page title\n",
    "            gr.Markdown(\"# Custom knowledge worker\")\n",
    "            # Create a chatbot conversation log\n",
    "            chatbot = gr.Chatbot(label=\"🤖 knowledge worker\")\n",
    "            # Create a textbox for user questions\n",
    "            msg = gr.Textbox(\n",
    "                label=\"👩‍💻 user input\",\n",
    "                info=\"Query information from the custom knowledge base.\",\n",
    "            )\n",
    "\n",
    "            # Align both buttons on the same row\n",
    "            with gr.Row():\n",
    "                send = gr.Button(value=\"Send\", variant=\"primary\", size=\"sm\")\n",
    "                clear = gr.Button(value=\"Clear History\", variant=\"secondary\", size=\"sm\")\n",
    "\n",
    "            # Submit message on <enter> or clicking \"Send\" button\n",
    "            msg.submit(self.submit, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "            send.click(self.submit, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "\n",
    "            # Clear chatbot history on clicking \"Clear History\" button\n",
    "            clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "        # Create a queue system so multiple users can access the page at once\n",
    "        demo.queue()\n",
    "        # Launch the webserver locally\n",
    "        demo.launch(share=share, debug=debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dt_gen_ai_hackathon_helper.view.view as demo_view\n",
    "\n",
    "qa_chain_ui = create_qa_vertex_ai_search_chain(prompt_b)\n",
    "demo = View(qa_chain=qa_chain_ui)\n",
    "demo.launch_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Build your own custom knowledge worker end-to-end\n",
    "\n",
    "We are now going to build a custom knowledge worker end-to-end using the Datatonic website as a knowledge source.\n",
    "\n",
    "In the previous section, you have learned how to use Vertex AI Search. This is Vertex AI's managed service version of a vector store. As such, Vertex AI manages the document loading, indexing, splitting, embedding and retrieval.\n",
    "\n",
    "However, in some cases, it is also benefitial to know how to do these steps yourself, using a fully custom solution. Therefore in this next section we are going to learn how to:\n",
    "* Load documents\n",
    "* Split documents\n",
    "* Embed documents\n",
    "* Ingest them in a vector store\n",
    "* Retrieve them based on similarity search\n",
    "\n",
    "In this first task, we will provide the pre-embedded vector store and explore the capabilities of a knowledge worker. Then we will let you try to load, split, embed and ingest your own documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @title Set the demo bucket name { run: \"auto\", display-mode: \"form\" }\n",
    "# # @markdown This variable can be left as default for this task.\n",
    "# DEMO_BUCKET = \"dt-gen-ai-hackathon-demo\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dt_gen_ai_hackathon_helper.widgets.widgets as widgets\n",
    "\n",
    "# DEMO_DROPDOWN = widgets.gcp_bucket_dropdown(\n",
    "#     DEMO_BUCKET, \"Select a demo from this dropdown: \"\n",
    "# )\n",
    "# display(DEMO_DROPDOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download website documents\n",
    "!gsutil cp gs: // {DEMO_BUCKET} / {DEMO_DROPDOWN.value}.tar.gz.& & tar -xzf {DEMO_DROPDOWN.value}.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dt_gen_ai_hackathon_helper.view.view as demo_view\n",
    "# import dt_gen_ai_hackathon_helper.embeddings.embeddings as demo_embeddings\n",
    "\n",
    "# demo_vector_store = demo_embeddings.load_embeddings(\n",
    "#     DEMO_DROPDOWN.value\n",
    "# )  # loads the vector DB from the local file system.\n",
    "\n",
    "# demo = demo_view.View(vector_store=demo_vector_store)\n",
    "# demo.launch_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Implementing a knowledge worker\n",
    "Here, you'll implement a knowledge worker for your company using a local vector store.\n",
    "\n",
    "We will:\n",
    "- Load documents with information about Datatonic's website\n",
    "- Create text embeddings from documents\n",
    "- Storing embedding in a local database\n",
    "- Use an LLM to answer queries about your company knowledge\n",
    "\n",
    "## Collecting documents\n",
    "First, we need to collect our data. To get started fast, we've already downloaded some sample website data upfront. Let's copy the website data from a public Cloud Storage bucket to your local file system\n",
    "\n",
    "**❗ Note:** Although PaLM supports multiple languages, text embeddings currently work best with English documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Select a webarchive { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown This variable can be left as default for this task.\n",
    "BUCKET = \"dt-gen-ai-hackathon-webarchive\"  # @param {type:\"string\"}\n",
    "LOCAL_FOLDER_DROPDOWN = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL_FOLDER_DROPDOWN = widgets.gcp_bucket_dropdown(\n",
    "#     BUCKET,\n",
    "#     \"Choose any of these web archives as the base knowledge of your worker: \",\n",
    "# )\n",
    "# display(LOCAL_FOLDER_DROPDOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs: // {BUCKET} / {LOCAL_FOLDER_DROPDOWN.value}.tar.gz.& & tar -xzf {LOCAL_FOLDER_DROPDOWN.value}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 Congratulations! 🎉 You've downloaded Datatonic's website data.\n",
    "\n",
    "If we browse the files we just downloaded, we can see the file structure contains folders and `HTML` files. This is because it is a local replica of the target website, meaning the file paths correlate with real webpages.\n",
    "\n",
    "**➡️ Your task:** Run the following cells to *create* the text embeddings based on your downloaded data.\n",
    "\n",
    "Now, lets embed these files so we can use them in our knowledge worker.\n",
    "\n",
    "LangChain supports [numerous methods](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) for loading documents.\n",
    "\n",
    "We will be using the `DirectoryLoader` and `UnstructuredHTMLLoader` in order to load a pre-compiled archive of your website. This method is similar to the [`RecursiveUrlLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/recursive_url_loader). This document loader searches for subpages of a website and loads each pages content as a document. Additionally, if we only wanted to download a list of URLs without searching for subpages, we could use a [`UnstructuredURLLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/url).\n",
    "\n",
    "**❗ Note:** We're using pre-compiled archives to avoid hitting rate limits during this session, however this content can also be programatically gathered routinely to collect new blog posts / press releases.\n",
    "\n",
    "**➡️ Your task:** Read the linked resources in the `Introduction to LangChain` step and study the following code cells as they provide reusable LangChain code for your knowledge worker.\n",
    "\n",
    "In the code cell below, we parse the directory to find `HTML` files and load their contents using `UnstructuredHTMLLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjaBy8CSs3R5"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, UnstructuredHTMLLoader\n",
    "\n",
    "\n",
    "def load_documents(source_dir):\n",
    "    # Load the documentation using a HTML parser\n",
    "    loader = DirectoryLoader(\n",
    "        source_dir,\n",
    "        glob=\"**/*.html\",\n",
    "        loader_cls=UnstructuredHTMLLoader,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    documents = loader.load()\n",
    "\n",
    "    print(f\"Loaded: {len(documents)} documents from {source_dir}.\")\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXhTVS9ws3R5"
   },
   "source": [
    "### Creating or loading embeddings\n",
    "\n",
    "Creating embeddings each time we use our app is time-consuming and expensive.\n",
    "By persisting the vector store database after embedding, we can load the saved embeddings for use in another session.\n",
    "\n",
    "**➡️ Your task:** Study and execute the following code cells. Note that after the documents have been loaded, they are split into shards using the `RecursiveCharacterTextSplitter` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Njknbfazs3R6"
   },
   "outputs": [],
   "source": [
    "# @title Set the name of your vectorstore { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown This variable can be left as default for this task.\n",
    "PERSIST_DIR = \"chromadb\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkO3NV73s3R6"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "\n",
    "def create_embeddings(source_dir):\n",
    "    documents = load_documents(source_dir=source_dir)\n",
    "\n",
    "    # We use Google embeddings model, however other models can be substituted here\n",
    "    embeddings = VertexAIEmbeddings(model_name='textembedding-gecko@001')\n",
    "\n",
    "    # Individual documents will often exceed the token limit.\n",
    "    # By splitting documents into chunks of 1000 token\n",
    "    # These chunks fit into the token limit alongside the user prompt\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=texts, embedding=embeddings, persist_directory=PERSIST_DIR\n",
    "    )\n",
    "\n",
    "    # Persist the ChromaDB locally, so we can reload the script without expensively re-embedding the database\n",
    "    vector_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings(\n",
    "    source_dir=LOCAL_FOLDER_DROPDOWN.value\n",
    ")  # creates the vector DB and saves it locally.\n",
    "print(f\"Created new vectorstore in dir {PERSIST_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**➡️ Your task:** Run the following cells to *load* the text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy4mX1JCs3R6"
   },
   "outputs": [],
   "source": [
    "def load_embeddings(persist_directory):\n",
    "    # We use VertexAI embeddings model, however other models can be substituted here\n",
    "    embeddings = VertexAIEmbeddings(model_name='textembedding-gecko@001')\n",
    "    \n",
    "    # Creating embeddings with each re-run is highly inefficient and costly.\n",
    "    # We instead aim to embed once, then load these embeddings from storage.\n",
    "    vector_store = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2r7APATs3R6"
   },
   "outputs": [],
   "source": [
    "vector_store = load_embeddings(\n",
    "    PERSIST_DIR\n",
    ")  # loads the vector DB from the local file system.\n",
    "print(f\"Loaded {PERSIST_DIR} as vectorstore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mXJKt22xc6N"
   },
   "source": [
    "**🎉 Congratulations! 🎉** You've created text embeddings from your company data and stored them successfully in a local vector database.\n",
    "Now, you'll shift your focus to implementing the actual LLM by creating a chain using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSsi93lgs3R7"
   },
   "source": [
    "## Creating the Conversational Q&A Chain\n",
    "\n",
    "In this section, you'll create a chain which will be able to provide an answer given a question from a user.\n",
    "To understand the purpose of chains, you can read about chains in the [LangChain documentation](https://docs.langchain.com/docs/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "\n",
    "def create_qa_chain(vector_store, condense_question_prompt: None, k=4, temperature=0.0):\n",
    "    \"\"\" Create a Q&A conversation chain using the VertexAI LLM.\n",
    "\n",
    "    Arguments:\n",
    "        vector_store (object): The vectorstore containing our knowledge.\n",
    "        condense_question_prompt (PromptTemplate): The prompt template used to prompt engineer our LLM to respond in a certain tone, etc.\n",
    "        k (int): the 'k' value indicates the number of sources to use per query. 'k' as in 'k-nearest-neighbours' to the query in the embedding space.\n",
    "        temperature (float): the degree of randomness introduced into the LLM response.\n",
    "    \"\"\"\n",
    "\n",
    "    # A vector store retriever relates queries to embedded documents\n",
    "    retriever = vector_store.as_retriever(k=k)\n",
    "\n",
    "    # The selected Google model uses embedded documents related to the query\n",
    "    # It parses these documents in order to answer the user question.\n",
    "    # We use the VertexAI LLM, however other models can be substituted here\n",
    "    model = VertexAI(model_name='text-bison@001',temperature=temperature)\n",
    "\n",
    "    # A conversation retrieval chain keeps a history of Q&A / conversation\n",
    "    # This allows for contextual questions such as \"give an example of that (previous response)\".\n",
    "    # The chain is also set to return the source documents used in generating an output\n",
    "    # This allows for explainability behind model output.\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=model,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        # condense_question_prompt=condense_question_prompt,\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the user interface\n",
    "\n",
    "As building a UI is outside of the scope for this hackathon, a templated GUI using [Gradio](https://gradio.app/) is provided for your knowledge worker.\n",
    "\n",
    "**➡️ Your task:** Execute the cells below to launch the user interface.\n",
    "\n",
    "**❗ Note:** The following cell will run until manually stopped. Remember to halt it before moving onto the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = create_qa_chain(\n",
    "    vector_store=vector_store #, condense_question_prompt=TASK_01_PROMPT\n",
    ")\n",
    "\n",
    "demo = View(qa_chain=qa_chain)\n",
    "demo.launch_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrsqbDyOs3R9"
   },
   "source": [
    "**➡️ Your task:** Use the user interface above (which you can also open in a separate tab given the shareable link above) to query your knowledge base.\n",
    "\n",
    "Try out a few questions from this example Q&A (using the Datatonic web archive):\n",
    "\n",
    "> 👩‍💻: What is Datatonic?\n",
    "> \n",
    "> 🦜: Datatonic is a data consultancy enabling companies to make better business decisions with the power of Modern Data Stack and MLOps.\n",
    "> \n",
    "> 👩‍💻: Summarise the web article on Greentonic.\n",
    "> \n",
    "> 🦜: Greentonic is Datatonic's sustainability initiative.\n",
    "> \n",
    "> 👩‍💻: How is Datatonic being sustainable?\n",
    "> \n",
    "> 🦜: Datatonic is committed to sustainability and has a number of initiatives in place to reduce its environmental impact. These include:\n",
    ">    * Using renewable energy sources\n",
    ">    * Reducing our carbon footprint\n",
    ">    * Promoting sustainable practices in our supply chain\n",
    ">    * Supporting environmental charities\n",
    ">\n",
    "> We believe that sustainability is essential for the future of our planet and we are committed to doing our part to make a difference.\n",
    "\n",
    "Since we used a `ConversationalRetrievalChain`, we can also correct the model when it gives the wrong response and prompt it to fix it’s mistake, or ask for further detail on a previous response.\n",
    "\n",
    "**🎉 Congratulations! 🎉** You've created your first chain using LangChain which you can query for general questions in a user interface.\n",
    "Let's continue extending your knowledge worker in the next task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ua3ttLS1_sO"
   },
   "source": [
    "# Task 2: Extending the knowledge base\n",
    "\n",
    "As mentioned, it is possible to extend the knowledge base with additional documents.\n",
    "This is useful for updating a knowledge base with new information without having to re-embed established knowledge from scratch.\n",
    "\n",
    "If you wanted to build a knowledge worker with another document type, for instance [Microsoft Word](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/microsoft_word.html) documents, you would update the `load_documents()` function according to the documentation for that document type loader.\n",
    "\n",
    "**❗ Note:** if you're looking to deploy a knowledge worker with several knowledge bases, an [Embedding Router Chain](https://python.langchain.com/docs/modules/chains/foundational/router#embeddingrouterchain), which combines several knowledge workers with discrete knowledge bases into a single chain which selects the best worker for the query.\n",
    "\n",
    "**➡️ Your task:** Extend the knowledge worker with new PDFs documents.\n",
    "1. Download some PDFs locally.\n",
    "2. Load the new documents.\n",
    "3. Add new documents to the existing vector store using the `.add_documents(documents=...)` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading documents locally\n",
    "!gsutil cp -r gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task -> Add new PDFs to the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LangChains's support for `PyPDFLoader` to load a few new PDFs into the same vector store. We suggest using these two:\n",
    "* `alphabet-investor-pdfs/2004_google_annual_report.pdf`\n",
    "* `alphabet-investor-pdfs/2010Q1_earnings_google.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "# TODO: Turn this into a task\n",
    "\n",
    "def load_pdf_documents(filepath):\n",
    "    \n",
    "    loader = PyPDFLoader(filepath)\n",
    "    pages = loader.load_and_split()\n",
    "\n",
    "    # add this document(s) to the vector store\n",
    "    vector_store.add_documents(pages)\n",
    "\n",
    "    # \"save\" the new vector store back to the file system\n",
    "    vector_store.persist()\n",
    "\n",
    "    print(\"Finished adding new documents to the vectorstore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkpb3xkq4nmu"
   },
   "source": [
    "**➡️ Your task:** Rerun the app and try asking questions using knowledge from your newly added documents.\n",
    "\n",
    "**❗ Note:** The following cell will run until manually stopped. Remember to halt it before moving onto the next task.\n",
    "\n",
    "Try asking questions about different topics coming from different sources. With this you will see the value of a custom vector store where you can ingest multiple data types and sources. For example, try asking:\n",
    "* About Datatonic's sustainability initiative\n",
    "* About Alphabet's revenue performance in 2004 and 2010\n",
    "* Then about some of Datatonic's case studies again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwUgYAAT4j-6"
   },
   "outputs": [],
   "source": [
    "qa_chain = create_qa_chain(\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "demo = View(qa_chain=qa_chain)\n",
    "demo.launch_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOkmXLdT31yE"
   },
   "source": [
    "**🎉 Congratulations! 🎉** You've extended your knowledge to creating text embedding from a variety of sources - whether it's public data from your company's website or unstructured documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsGLS0Xw2BKg"
   },
   "source": [
    "# Task 3: Generating text over a vector index\n",
    "\n",
    "We can utilise our embedded documents for more than just Q&A.\n",
    "In tasks 1 and 2, we used the embedded documents as context for answering user queries, but in this task we will use it to generate original content using this knowledge base as a source of information and style.\n",
    "\n",
    "The concept of this use case is to generate ideas for new blogs, utilising knowledge and style information contained in the existing company website data.\n",
    "We can use Generative AI for creative ideation, too!\n",
    "Let's demonstrates the possibilities for human-computer interaction (HCI) apps in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set LLM temperature { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown Temperature controls the degree of creativity / randomness introduced into the LLM.\n",
    "temperature = 0.7  # @param {type:\"slider\", min:0, max:1, step:0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_03_TEMPLATE = \"\"\"\\\n",
    "Using the provided context, write the outline of a company blog post.\n",
    "Include a bullet-point list of the main talking points, and a brief summary of the overall blog.\n",
    "\n",
    "Context: {context}\n",
    "Topic: {topic}\n",
    "\"\"\"\n",
    "\n",
    "TASK_03_PROMPT = PromptTemplate.from_template(TASK_03_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PzurMaEs3R-"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "model = VertexAI(model_name='text-bison@001',temperature=temperature)\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=TASK_03_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kVmZBg-s3R-"
   },
   "outputs": [],
   "source": [
    "def generate_blog_outline(topic: str, k: int):\n",
    "    # search for 'k' nearest documents related to our topic.\n",
    "    docs = vector_store.similarity_search(topic, k=k)\n",
    "\n",
    "    # associate topic with the content of each document to generate inputs\n",
    "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
    "\n",
    "    # generate blog outline\n",
    "    output = chain.apply(inputs)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJc3FB8WLs2-"
   },
   "source": [
    "**➡️ Your task:** Create ideas for a new blog post.\n",
    "Try adjusting the title of the post to generate new ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10THi9L2Kvzv"
   },
   "outputs": [],
   "source": [
    "# @title Set blog prompt / title { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown Be descriptive, as the LLM will collect semantically similar sources as inspiration.\n",
    "BLOG_TITLE = \"How we're making our business more sustainable\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho7Euwhks3R-"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# generate variations of blog posts on the topic provided, based on the 4 most relevant documents\n",
    "output = generate_blog_outline(BLOG_TITLE, k=4)\n",
    "markdown = \"\"\n",
    "\n",
    "for i, blog in enumerate(output):\n",
    "    markdown += f\"# #{i} {BLOG_TITLE}\\n{blog['text']}\\n\\n\"\n",
    "\n",
    "display(Markdown(markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt6qQQwELhm8"
   },
   "source": [
    "**➡️ Your task:** Update the `TASK_03_PROMPT`, `temperature` and `BLOG_TITLE` variables to create new types of content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIlaipnMMOMk"
   },
   "source": [
    "**🎉 Congratulations! 🎉** You've completed task 3 and generated ideas for future blog posts!\n",
    "Continue with the next section to explore more possibilities and ideas using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xooDGVw_Jwlg"
   },
   "source": [
    "# Bonus Track: Extending the chain\n",
    "\n",
    "So far you've created two types of chains:\n",
    "\n",
    "### LLMChain\n",
    "\n",
    "The `LLMChain` is a simple chain that adds some functionality around language models.\n",
    "It is used widely throughout LangChain, including in other chains and agents.\n",
    "\n",
    "An LLMChain consists of a **PromptTemplate** and a **language model** (either an LLM or chat model).\n",
    "It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "```python\n",
    "chain = LLMChain(llm=model, prompt=PROMPT)\n",
    "```\n",
    "\n",
    "### ConversationalRetrievalChain\n",
    "\n",
    "The `ConversationalRetrievalQA` chain builds on RetrievalQAChain to provide a chat history component.\n",
    "\n",
    "It first combines the chat history (either explicitly passed in or retrieved from the provided memory) and the question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question answering chain to return a response.\n",
    "\n",
    "To create one, you will need a retriever.\n",
    "In the below example, we will create one from a vector store, which can be created from embeddings.\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(k=k)\n",
    "model = VertexAI(temperature=temperature)\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    condense_question_prompt=TASK_01_PROMPT,\n",
    ")\n",
    "```\n",
    "\n",
    "### Explore more chains\n",
    "\n",
    "**➡️ Your task:** Firm up your knowledge about the two chains used in this notebook [here](https://python.langchain.com/docs/modules/chains/foundational/llm_chain) and [here](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db).\n",
    "In which scenarios should you apply either of them?\n",
    "What are their limitations?\n",
    "\n",
    "*The LLMChain is useful when ...*\n",
    "\n",
    "*It's limitations are ...*\n",
    "\n",
    "*The ConversationalRetrievalChain is useful when ...*\n",
    "\n",
    "*It's limitations are ...*\n",
    "\n",
    "**➡️ Your task:** Read about more types of chains in the [official LangChain documentation](https://python.langchain.com/docs/modules/chains/additional/).\n",
    "We recommend the **Sequential chain** and **Self-critique chain with constitutional AI**.\n",
    "How can you extend your conversational knowledge worker which is currently based on the `ConversationalRetrievalChain`?\n",
    "Summarise your idea either using pseudo code or actual code if you've time!\n",
    "Overall we would like to you to consider:\n",
    "\n",
    "**Idea + idea description:**\n",
    "\n",
    "- *The idea is ...*\n",
    "- *What it is ...*\n",
    "\n",
    "**Problem it solves + impact:**\n",
    "\n",
    "- *It would solve the following challenge ...*\n",
    "- *The volume or value of the impact would be ...*\n",
    "\n",
    "**Approach + Next steps:**\n",
    "\n",
    "- *Next steps would be ...*\n",
    "\n",
    "**❗ Note:** Do you have any other ideas (even outside of implementing a knowledge worker)?\n",
    "Feel free to ideate about another use case which is relevant to your industry or company!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jq_fu6wXKFNf"
   },
   "outputs": [],
   "source": [
    "# ❗ TODO: create pseudo code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXCL6Ri6s3R-"
   },
   "source": [
    "Try asking the same questions you used to query your knowledge worker. Does it answer the question in the same way? Does it give more or less detail? What are the immediate differences between solutions?\n",
    "\n",
    "**🎉 Congratulations! 🎉** You've gotten started with Enterprise Search. \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "## What have we built?\n",
    "\n",
    "In this session, we have built a knowledge worker use-case for accessing your complex information using Generative AI.\n",
    "This concept can be extended into a fully-fledged tool that can unlock the value of your data for customers or internal use.\n",
    "\n",
    "## Going further..\n",
    "\n",
    "This workshop has introduced all the LangChain knowledge required to create a knowledge worker.\n",
    "The next steps for moving this project from development to production are discussed below.\n",
    "\n",
    "## Decoupling LangChain from Gradio\n",
    "\n",
    "It is not necessary to run LangChain within a GradI/O app.\n",
    "Decoupling LangChain into a separate API has several benefits:\n",
    "1. We can deploy scalable servers / Docker containers\n",
    "2. Simplified code - a frontend-backend loose coupling can lead to simpler code, which is ease to update and maintain.\n",
    "3. If a more professional user interface is needed, such as a native React app.\n",
    "Replacing GradI/O is a straightforward process - FastAPI can be called from javascript, etc., allowing you to move beyond Python frontend frameworks.\n",
    "\n",
    "An example of this separation can be found on the GitHub repository, using FastAPI to create a simple LangChain API server and Poetry to manage separate server environments.\n",
    "\n",
    "## Deploying on Google Cloud\n",
    "\n",
    "Once we have decoupled our frontend and backend code, we can deploy the project onto Google Cloud.\n",
    "\n",
    "This reference architecture diagrams mirror the flow diagrams we first introducted in the workshop introduction. Using Google Cloud, we can create production pipelines for creating / updating vector databases, and deploy a knowledge worker API (which can be connected to a web UI, Slack bot, etc.).\n",
    "\n",
    "**Example architecture: Ingestion**\n",
    "![A typical ingestion chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/knowledge-worker-gcp-ingestion-pipeline.png?raw=true)\n",
    "\n",
    "By creating a pipeline for data ingestion, we can continue to extend the knowledge base of our knowledge worker as you produce new documents and documentation.\n",
    "\n",
    "**Example architecture: Inference**\n",
    "![A typical ingestion chain](https://github.com/teamdatatonic/gen-ai-hackathon/blob/7f37d477b18ace5912d34b0574512559d7a457ed/assets/knowledge-worker-gcp-inference-pipeline.png?raw=true)\n",
    "\n",
    "By creating a pipeline for inference, we can leverage the power of Google Cloud to provide a highly reliable and scalable API that can power a variety of applications.\n",
    "\n",
    "**🎉 Congratulations! 🎉** You've completed this notebook!\n",
    "Now it's time to embark your Generative AI journey and ideate about use cases which can benefit your company in conjunction or in addition to your first knowledge worker."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
