{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Generative AI Hackathon</h1>\n",
    "<table align=\"center\">\n",
    "    <!-- <td>\n",
    "        <a href=\"https://colab.research.google.com/github/teamdatatonic/gen-ai-hackathon/blob/feature/DBA-hackathon/notebook/analytics_hackathon.ipynb\">\n",
    "            <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\">\n",
    "            <span style=\"vertical-align: middle;\">Run in Colab</span>\n",
    "        </a>\n",
    "    </td> -->\n",
    "    <!-- <td>\n",
    "        <a href=\"https://github.com/teamdatatonic/gen-ai-hackathon/blob/DBA-hackathon/analytics_hackathon.ipynb\">\n",
    "            <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "            <span style=\"vertical-align: middle;\">View on GitHub</span>\n",
    "        </a>\n",
    "    </td> -->\n",
    "    <!-- <td>\n",
    "        <a href=\"http://127.0.0.1:8888/?token=30f0873aab701a416cc3cc4be5926caa89940d3778fcef47\n",
    "        \">\n",
    "            <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"> \n",
    "            <span style=\"vertical-align: middle;\">Open in Jupyter Notebook</span>\n",
    "        </a>\n",
    "    </td> -->\n",
    "</table>\n",
    "<hr>\n",
    "\n",
    "**➡️ Your task:** Learn about Generative AI by building your own Analytics Assistant using Python and LangChain!\n",
    "\n",
    "**❗ Note:** This workshop has been designed to be run in Jupyter Notebook. A credentials.json key will be shared with you for the purpose of running this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, let's first install some packages and dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip install package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet \"git+https://github.com/teamdatatonic/gen-ai-hackathon.git@feat/alvaro#egg=dt-gen-ai-analytics-helper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run jupyter notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❗ Note:** This notebook will keep running until it is shut down manually."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics Assistant Hackathon - Start Here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI Endpoint\n",
    "\n",
    "Currently, Vertex AI LLMs are accessible via Google Cloud projects. \n",
    "\n",
    "1. Set the env variables `project_id` and `dataset_id` with the filepath (**❗ Note:** the `/content/` folder is where uploaded files are stored by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your-project-id' with your Google Cloud project ID\n",
    "PROJECT_ID = 'dt-gen-ai-hackathon-dev'\n",
    "DATASET_ID = 'database_analytics_demo_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# @title Set project credentials. { run: \"auto\", display-mode: \"form\" }\n",
    "# @markdown Set the filepath to the `.json` credentials file.\n",
    "\n",
    "GOOGLE_APPLICATION_CREDENTIALS = \"credentials.json\"  # @param {type:\"string\"}\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GOOGLE_APPLICATION_CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set account dt-gen-ai-hackathon-sa@dt-gen-ai-hackathon-dev.iam.gserviceaccount.com\n",
    "!gcloud auth activate-service-account --key-file={GOOGLE_APPLICATION_CREDENTIALS}\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SQLDatabaseSequentialChain\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.agents import(create_pandas_dataframe_agent)\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain import LLMChain,PromptTemplate\n",
    "from langchain.agents import create_sql_agent \n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import create_engine\n",
    "from langchain.llms import VertexAI\n",
    "from langchain import SQLDatabase\n",
    "from tabulate import tabulate\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin the Hackathon\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective of the hackathon is to show how to use Generative AI to query BigQuery tables and extract useful insights from it. This is done by leveraging the following technologies:\n",
    "- PaLM: this is the Large Language Model (LLM) that generates the SQL code required to extract the data, as well as the text generated to provide an answer to the human\n",
    "- LangChain: this is an open-source package that allows you to easily use LLMs. LangChain introduces to concept of chains and agents. Chains are an abstraction of LLMs that already uses pre-built prompts for given tasks. For example, in this notebook you will use the `SQLDatabaseSequentialChain` (more detail on this later), which essentially orchestrates a combintation of prompts and LLM calls in sequence until the correct answer is extracted.\n",
    "- Gradio: this is friendly low-level UI tool, that allows users to create UIs and chatbots to interact with LLMs\n",
    "\n",
    "\n",
    "Throoughout this notebook, there will be a sequence of tasks for you to run. Some tasks will already be completed and other tasks will be open as a challenge for you to implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need an Analytics Assistant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Companies have a lot of data stored in data warehouses, such as BigQuery. A lot of insights can be generated from such data (information abotu customers, products, business performance, marketing campaigns, etc). However, often this data is not accessible to everybody, as it requires an understanding of writing SQL code. \n",
    "\n",
    "This means that while data might be available, most users will not be able to access it, or will depends on skilled engineers to write the queries to gain access to data. Having an Analytics Assistant removes this bottleneck and accelerates the \"time to insight\" metric. \n",
    "\n",
    "Additionally, in many cases the answer to a query is not enough to provide the required insight, and multiple back-and-forth questions & answers are needed. An Analytics Assistant enables you to properly \"chat\" with your data, ask follow-up questions, and even do tasks like write emails summarising the findings. Essentially, it allows users to not just generate SQL but go beyond that and generate insights that come out of the SQL answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What data do we have?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we will be using some mock data that aims to simulate real data that many companies would have. The dataset available has the following tables: Customer, employees, financial goals, inventory, orders, supplier orders. These tables are useful for this hackathon.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started with tasks, let's do Task 0 to ensure we can connect to BigQuery and already run some LLMs to query the data in the most basic way.\n",
    "\n",
    "We are going to do the following:\n",
    "- Connect to the Bigquery Database\n",
    "- Set up LLM Chain - this will be the `SQLDatabaseSequentialChain`\n",
    "- Example query with the above chain to extract an answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Bigquery Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to leverage the `sqlalchemy` package to create an engine, which acts as a connector to our BigQuery data. For this, lets create a `BigQueryDatabase` class. As you will see in the next section, an instance of this class will be passed as an input to the LLM, and in that way the LLM knows which BigQuery dataset it has access to and can query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Database:\n",
    "    \"\"\"Create a connector to your BigQuery dataset\"\"\"\n",
    "    def __init__(self, url: str, schema: str = None):\n",
    "        print(\"creating db engine...\")\n",
    "        self.engine = self.create_engine(url)\n",
    "        print(\"creating db session...\")\n",
    "        self.base = declarative_base()\n",
    "        self.sessionmaker = sessionmaker(\n",
    "            autocommit=True, autoflush=True, bind=self.engine\n",
    "        )\n",
    "        self.schema = schema\n",
    "        print(\"creating db connection...\")\n",
    "        self.connect = self.engine.connect()\n",
    "\n",
    "    def create_engine(self, url):\n",
    "        return create_engine(url)\n",
    "\n",
    "    @property\n",
    "    def dialect(self) -> str:\n",
    "        return self.engine.dialect.name\n",
    "\n",
    "    def create_session(self):\n",
    "        return self.sessionmaker()\n",
    "    \n",
    "    def create_connection(self):\n",
    "        return  self.connect\n",
    "    \n",
    "\n",
    "class BigQueryDatabase(Database):\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "    ):\n",
    "        super().__init__(f\"bigquery://{project_id}/{dataset_id}\")\n",
    "        self.schema = dataset_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LLM Chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use LangChain and some of the pre-built chains that let you easily connect to an LLM and use it. The chain we are going to use is called `SQLDatabaseSequentialChain`. \n",
    "\n",
    "Link to docs: https://api.python.langchain.com/en/latest/sql/langchain_experimental.sql.base.SQLDatabaseSequentialChain.html\n",
    "\n",
    "Github: https://github.com/langchain-ai/langchain/blob/3c7653bf0f68c15067839c5a2a8761bd04fd367f/libs/experimental/langchain_experimental/sql/base.py#L228\n",
    "\n",
    "Let's analyse what this chain is doing under the hood, as it will help us understand how to leverage to improve the LLM performance. The most important two arguments that this accepts as an input are `llm` and `db`. The former refers to which LLM you want to use. LangChain offers abstractions to multiple LLMs, both open-source and proprietary. For this notebook we will use the `VertexAI` LLM. This offers you to choose any LLM which is part of the Vertex Model Garden derived from PalM. This can be any of:\n",
    "- `text-bison` - text generation model, and any of its versions and tuned variants\n",
    "- `code-bison` - LLM tailored to write code, and any of its tuned variants\n",
    "- `chat-bison` - text generation model optimised for chat interactions\n",
    "- `embeddings-gecko` - this model lets you embed text for future use\n",
    "\n",
    "The `db` argument refers to the database that you want to query, in this case the `BigQueryDatabse` created beforehand.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the `SQLDatabaseSequentialChain` is doing the following things (once we execute it it will be more clear, so please refer to this section again):\n",
    "- List all tables that belong to the given database it has access to\n",
    "- Based on the user question and all tables available, the LLM will indentify which table (or combination of tables) it will need to query to generate the SQL code\n",
    "- A BigQuery job will be executed to extract the schema of the tables that have been identified as relevant\n",
    "- Given the user question and the table schemas, the LLM will try to generate the SQL code required to answer the question\n",
    "- The SQL code will be executed in BigQuery and the BigQuery answer will be returned\n",
    "- The user question and BigQuery answer will be used by the LLM to formulate an answer in natural language, which is then returned to the user\n",
    "\n",
    "This six steps are being executed in just one line of code, orchestrated in this order by the LangChain chain and powered by the PaLM LLM.\n",
    "\n",
    "Let's see it in action.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_sql_chain(llm, db, question):\n",
    "    \"\"\" Create a SQLDatabaseSequentialChain using the VertexAI LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    db_chain = SQLDatabaseSequentialChain.from_llm(\n",
    "        llm,\n",
    "        db,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "    )\n",
    "\n",
    "    output = db_chain(question)\n",
    "    sql_query = output[\"intermediate_steps\"][1]\n",
    "    response = output[\"result\"]\n",
    "    \n",
    "    return response, sql_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing the query we need to initialise the LLM from Vertex AI and the BigQuery database connector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex LLM\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID)\n",
    "\n",
    "# Initialize Vertex LLM\n",
    "llm = VertexAI(model_name='text-bison@001',\n",
    "               temperature=0, max_output_tokens=1024)\n",
    "\n",
    "# Initialise BigQuery datatbase connector\n",
    "db = BigQueryDatabase(project_id=PROJECT_ID, dataset_id=DATASET_ID)\n",
    "session = db.create_session()\n",
    "\n",
    "conn = db.create_connection()\n",
    "\n",
    "langchain_db = SQLDatabase(\n",
    "    db.engine, schema=db.schema, sample_rows_in_table_info=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to query the SQLDBChain\n",
    "def query_database(question, llm=llm ,db=langchain_db):\n",
    "    \n",
    "    # Call the SQLDBChain to get the answer based on the question\n",
    "    answer, sql_query = create_basic_sql_chain(llm=llm, db=langchain_db, question=question)\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try asking different questions and see what the LLM is doing under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_database('how many items did I sell in January?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Improve the Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Override default prompt\n",
    "\n",
    "As you could see, sometimes the LLM does not return the correct answer, or tries to generate a syntantically incorrect query. Additionally, you really don't have control over which tables the LLM can and cannot use. Also, you want the LLM to be able to know the current date so that the questions such as \"sales in the last three months\" are always up to date. \n",
    "\n",
    "All of this can be done by overrriding the default prompt provided by LangChain in the `SQLDatabaseSequentialChain`. Here is an example of how to do. Next, you can play with this prompt and create your own prompt too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SQL_PROMPT = \"\"\"\n",
    "You are a GoogleSQL expert. Given an input question, first create a syntactically\n",
    "correct GoogleSQL query to run, then look at the results of the query and return\n",
    "the answer to the input question.\n",
    "\n",
    "Unless the user specifies in the question a specific number of examples to obtain,\n",
    "query for at most {top_k} results using the LIMIT clause as per GoogleSQL. You can\n",
    "order the results to return the most informative data in the database.\n",
    "\n",
    "Never query for all columns from a table. You must query only the columns that are\n",
    "needed to answer the question. Wrap each column name and value in backticks (`)\n",
    "to denote them as delimited identifiers.\n",
    "\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful\n",
    "to not query for columns that do not exist. Also, pay attention to which column\n",
    "is in which table.\n",
    "\n",
    "Name all columns in the returned data appropriately. If a column does not have a\n",
    "matching name in the schema, create an appropriate name reflecting its content.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: \"Question here\"\n",
    "\n",
    "SQLQuery: \"SQL Query to run\"\n",
    "\n",
    "SQLResult: \"Result of the SQLQuery\"\n",
    "\n",
    "Answer: \"Final answer here\"\n",
    "\n",
    "\n",
    "Today''s date is {today_date}. When querying between dates, add the dates in quotes\n",
    "('')\n",
    "\n",
    "If someone asks for a specific month, use the range between the current month''s\n",
    "start date and the current month''s end date.\n",
    "\n",
    "If someone asks for a specific year, use the range between the first month of the\n",
    "current year and the current month''s end date.\n",
    "\n",
    "\n",
    "Remember to always use natural language when writing your final answer.\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "{table_info}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "table_names = [\"customers\",\"employees\",\"financial_goals\",\"inventory\",\"orders\",\"product_reviews\",\"supplier_orders\"]\n",
    "\n",
    "def create_sql_chain(question: str, table_info: str = table_names, top_k:int=100, llm: VertexAI = llm, db=langchain_db):\n",
    "    \"\"\" Create a Q&A conversation chain using the VertexAI LLM.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    db_chain = SQLDatabaseSequentialChain.from_llm(\n",
    "        llm,\n",
    "        db,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "    )\n",
    "    test_prompt = PromptTemplate(template=CUSTOM_SQL_PROMPT, input_variables=[\"question\", \"table_info\", \"today_date\", \"top_k\"])\n",
    "\n",
    "    today_date = datetime.now().strftime(\"%m/%d/%Y\")\n",
    "    output = db_chain(test_prompt.format(\n",
    "        question=question,\n",
    "        table_info=table_info,\n",
    "        today_date=today_date,\n",
    "        top_k=top_k\n",
    "        ))\n",
    "    sql_query = output[\"intermediate_steps\"][1]\n",
    "    response = output[\"result\"]\n",
    "    \n",
    "    return response, sql_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task - Part 1 -> Create your own prompt and remove tables from the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< TODO: Create a new prompt and replace the existing chain with that prompt>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_1_PROMPT = \"\"\"\n",
    "You are a GoogleSQL expert. Given an input question, first create a syntactically\n",
    "correct GoogleSQL query to run, then look at the results of the query and return\n",
    "the answer to the input question.\n",
    "\n",
    "Unless the user specifies in the question a specific number of examples to obtain,\n",
    "query for at most {top_k} results using the LIMIT clause as per GoogleSQL. You can\n",
    "order the results to return the most informative data in the database.\n",
    "\n",
    "Never query for all columns from a table. You must query only the columns that are\n",
    "needed to answer the question. Wrap each column name and value in backticks (`)\n",
    "to denote them as delimited identifiers.\n",
    "\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful\n",
    "to not query for columns that do not exist. Also, pay attention to which column\n",
    "is in which table.\n",
    "\n",
    "Name all columns in the returned data appropriately. If a column does not have a\n",
    "matching name in the schema, create an appropriate name reflecting its content.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: \"Question here\"\n",
    "\n",
    "SQLQuery: \"SQL Query to run\"\n",
    "\n",
    "SQLResult: \"Result of the SQLQuery\"\n",
    "\n",
    "Answer: \"Final answer here\"\n",
    "\n",
    "\n",
    "Today''s date is {today_date}. When querying between dates, add the dates in quotes\n",
    "('')\n",
    "\n",
    "If someone asks for a specific month, use the range between the current month''s\n",
    "start date and the current month''s end date.\n",
    "\n",
    "If someone asks for a specific year, use the range between the first month of the\n",
    "current year and the current month''s end date.\n",
    "\n",
    "\n",
    "Remember to always use natural language when writing your final answer.\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "{table_info}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_chain_task1(question: str, table_info: str = table_names, top_k:int=100, llm: VertexAI = llm, db=langchain_db):\n",
    "    \"\"\" Create a Q&A conversation chain using the VertexAI LLM.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    db_chain = SQLDatabaseSequentialChain.from_llm(\n",
    "        llm,\n",
    "        db,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "    )\n",
    "    test_prompt = PromptTemplate(template=TASK_1_PROMPT, input_variables=[\"question\", \"table_info\", \"today_date\", \"top_k\"])\n",
    "\n",
    "    today_date = datetime.now().strftime(\"%m/%d/%Y\")\n",
    "    output = db_chain(test_prompt.format(\n",
    "        question=question,\n",
    "        table_info=table_info,\n",
    "        today_date=today_date,\n",
    "        top_k=top_k\n",
    "        ))\n",
    "    sql_query = output[\"intermediate_steps\"][1]\n",
    "    response = output[\"result\"]\n",
    "    \n",
    "    return response, sql_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sql_chain_task1(\"how many items did I sell in January?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< TODO: Remove access to some tables>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = [\"customers\",\"employees\",\"financial_goals\",\"inventory\",\"orders\",\"product_reviews\",\"supplier_orders\"]\n",
    "langchain_db_subset = SQLDatabase(\n",
    "    db.engine, schema=db.schema, sample_rows_in_table_info=0, include_tables=table_names)\n",
    "create_sql_chain(question='how many items did I sell in January?', table_info=table_names, db=langchain_db_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task - Part 2 -> Chain multiple chains together for more powerful tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty of LLMs and chains is that you can combine multiple chains together, each of them using different LLMs, to achieve more powerful tasks, such as automatically writing emails to sumamrise the answer returned by the SQL generation code.\n",
    "\n",
    "For this task, first explore how to use the `LLMChain`, which is the simplest abstraction of a prompt + LLM pair. \n",
    "\n",
    "1. Implement the `LLMChain` and create a prompt to do any given task defined by you\n",
    "2. Combine the existing `SQLDatabaseSequentialChain` with the `LLMChain` to create a more powerful model. Possible ideas are:\n",
    "- Writing an email summarising the response\n",
    "- Suggest three good follow-up questions that a user might want to ask\n",
    "\n",
    "*Bonus track*: Can you try and use different LLMs for each of the tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 - Create a new LLMChain\n",
    "\n",
    "# TODO: Create a prompt for this new chain\n",
    "TASK_PROMPT = \"\"\"You are an Analytics Assistant. Your task is to assist users to better understand the insights from their BigQuery datasets.\n",
    "Users will ask a question to BigQuery and you will receive the question and the answer. \n",
    "\n",
    "Your task is to draft an email summarising the answer provided to the user.\n",
    "\n",
    "The user question was:\n",
    "```\n",
    "{question}\n",
    "```\n",
    "and the answer provided was \n",
    "```\n",
    "{answer}\n",
    "```\n",
    "\n",
    "Based on this, draft an email. Structure the email as follows:\n",
    "1. Start with a cordial introduction\n",
    "2. Remind the recipient as to what the user question was\n",
    "3. Provide a summary of the answer\n",
    "4. Send your best regards and say that you are happy to have a follow up\n",
    "\n",
    "Answer here:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "task_prompt = PromptTemplate(template=TASK_PROMPT, input_variables=[\"question\", \"answer\"])\n",
    "\n",
    "# TODO: Complete the chain below\n",
    "def get_task_chain(question, answer):\n",
    "    \n",
    "    task_chain = LLMChain(\n",
    "        llm=llm, prompt=task_prompt, output_key=\"output\")\n",
    "    \n",
    "    return task_chain.run(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Execute the new task chain to test it out\n",
    "get_task_chain(\"what is your name?\", \"My name is Alvaro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how to combine both chains to get an even more powerful model. To do so, we need to make sure we correctly get the output of the first chain (the `SQLDatabaseSequentialChain`) and pass it as input to the second chain (the `LLMChain`). Let's see how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def sql_and_task_chain(question, table_names=table_names):\n",
    "   # Call the SQLDBSequentialChain to get the answer based on the question\n",
    "    response, sql_query = create_sql_chain(question=question,table_info=table_names, db=langchain_db)\n",
    "\n",
    "    chatbot_history = []\n",
    "\n",
    "    chatbot_history.append(\n",
    "        (\n",
    "            question,\n",
    "            response\n",
    "        )\n",
    "    )\n",
    "   \n",
    "    output = get_task_chain(question=question,\n",
    "                           answer=response)\n",
    "    \n",
    "    chatbot_history.append(\n",
    "        (\n",
    "            output,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_and_task_chain(question='how many items did I sell in January?',table_names=table_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create Simple Gradio Interface "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that running LLMs in a notebook is not very user friendly nor interactive, and you cannot really chat with your data easily. \n",
    "\n",
    "To avoid this, let's see how to leverage Gradio to create a friendly UI that users can access to chat with their data.\n",
    "\n",
    "We are going to start with a basic Gradio `interface` and then look at how to create a more complex chatbot that allows for multiple chat interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a basic interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn= sql_and_task_chain,  # Function to execute when a query is received\n",
    "    inputs=\"text\",      # Input is a single text field\n",
    "    outputs=\"text\",     # Output will be a text response\n",
    "    title=\"Analytics Worker Demo\",\n",
    "    description=\"Enter a question, and the system will query the database and provide an answer.\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface on a specified port (e.g., 5000)\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's create an actual chatbot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use Gradio `chatbot`. `Gr.chatbots` are a way to build and show the chat history. Their core features are that they can have long-running conversations and have access to information that users want to know about. Read more about this: https://python.langchain.com/docs/use_cases/chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradio chatbot and interface\n",
    "with gr.Blocks(title=\"Analytics Assistant\") as demo:\n",
    "\n",
    "    with gr.Row():\n",
    "                with gr.Column(scale=1, variant=\"panel\"):\n",
    "                    with gr.Row():\n",
    "                        # Load Datatonic logo as .svg\n",
    "                        gr.Markdown(\n",
    "                            \"\"\"\\\n",
    "<svg width=\"177\" height=\"24\" viewBox=\"0 0 177 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M14.5548 14.596H9.37748V9.404H0V14.596H9.37748V24H14.5548V14.596H23.9323V9.404H14.5548V14.596Z\" fill=\"#2a5cff\"></path><path d=\"M14.5548 0H9.37748V9.404H14.5548V0Z\" fill=\"#2a5cff\"></path><path d=\"M59.6168 6.3732C55.4116 6.3732 52.3637 8.42451 51.9375 11.8749H56.6305C56.8133 10.6942 57.7879 9.60525 59.6168 9.60525C61.5972 9.60525 62.5728 10.9415 62.5728 12.6518V12.931L58.2151 13.3658C54.9541 13.6769 51.3588 14.6403 51.3588 18.4955C51.3588 21.7594 53.918 23.9981 57.1183 23.9981C60.3186 23.9981 61.6891 22.4753 62.6334 20.454V23.6252H67.2345V12.9938C67.2345 9.20136 64.7663 6.37226 59.6168 6.37226V6.3732ZM62.5728 16.4451C62.5728 19.0877 61.3235 21.139 58.9462 21.139C57.3617 21.139 56.1124 20.1448 56.1124 18.4964C56.1124 16.5997 58.0323 16.2586 59.9218 16.0403L62.5728 15.7601V16.4451Z\" fill=\"#2a5cff\"></path><path d=\"M76.7895 10.2275H80.416V6.74523H76.7895V2.76725H72.1277V6.74616H69.0496V10.2284H72.1277V18.622C72.1277 21.9796 74.5656 23.627 77.8257 23.627H80.4463V20.1457H78.8315C77.4904 20.1457 76.7895 19.7109 76.7895 18.249V10.2275Z\" fill=\"#2a5cff\"></path><path d=\"M90.6838 6.3732C86.4786 6.3732 83.4308 8.42451 83.0046 11.8749H87.6975C87.8803 10.6942 88.8549 9.60525 90.6838 9.60525C92.6643 9.60525 93.6398 10.9415 93.6398 12.6518V12.931L89.2821 13.3658C86.0212 13.6769 82.4259 14.6403 82.4259 18.4955C82.4259 21.7594 84.985 23.9981 88.1853 23.9981C91.3856 23.9981 92.7561 22.4753 93.7004 20.454V23.6252H98.3016V12.9938C98.3016 9.20136 95.8333 6.37226 90.6838 6.37226V6.3732ZM93.6398 16.4451C93.6398 19.0877 92.3905 21.139 90.0133 21.139C88.4287 21.139 87.1795 20.1448 87.1795 18.4964C87.1795 16.5997 89.0993 16.2586 90.9888 16.0403L93.6398 15.7601V16.4451Z\" fill=\"#2a5cff\"></path><path d=\"M108.022 10.2275H111.648V6.74523H108.022V2.76725H103.36V6.74616H100.282V10.2284H103.36V18.622C103.36 21.9796 105.798 23.627 109.058 23.627H111.679V20.1457H110.064C108.723 20.1457 108.022 19.7109 108.022 18.249V10.2275Z\" fill=\"#2a5cff\"></path><path d=\"M121.926 6.3732C116.624 6.3732 113.303 10.0101 113.303 15.2016C113.303 20.3931 116.624 23.9991 121.926 23.9991C127.228 23.9991 130.55 20.3622 130.55 15.2016C130.55 10.041 127.228 6.3732 121.926 6.3732ZM121.926 20.7661C119.397 20.7661 118.056 18.6211 118.056 15.2016C118.056 11.7821 119.397 9.60618 121.926 9.60618C124.455 9.60618 125.796 11.7512 125.796 15.2016C125.796 18.652 124.455 20.7661 121.926 20.7661Z\" fill=\"#2a5cff\"></path><path d=\"M143.121 6.3732C140.226 6.3732 138.61 8.08246 137.849 10.1966V6.74616H133.217V23.6261H137.88V14.0199C137.88 11.0353 139.007 9.69896 140.927 9.69896C142.847 9.69896 143.974 11.0353 143.974 13.9581V23.6261H148.637V12.9319C148.637 9.20136 146.991 6.3732 143.121 6.3732H143.121Z\" fill=\"#2a5cff\"></path><path d=\"M157.046 6.74616H152.383V23.6261H157.046V6.74616Z\" fill=\"#2a5cff\"></path><path d=\"M157.137 0H152.323V4.47651H157.137V0Z\" fill=\"#2a5cff\"></path><path d=\"M172.107 17.6268C171.589 19.5853 170.492 20.767 168.572 20.767C166.165 20.767 164.763 18.7148 164.763 15.1716C164.763 11.6284 166.104 9.60712 168.572 9.60712C170.492 9.60712 171.559 11.0371 171.955 12.8092H176.647C175.947 8.86119 172.93 6.37414 168.572 6.37414C163.514 6.37414 160.009 9.88731 160.009 15.1716C160.009 20.4559 163.453 24 168.572 24C172.717 24 176.007 21.7613 176.8 17.6268H172.107Z\" fill=\"#2a5cff\"></path><path d=\"M37.2505 0H28.7188V9.26789H33.533V4.10355H37.3727C41.6082 4.10355 44.0764 6.83894 44.0764 11.813C44.0764 16.7872 41.6082 19.5226 37.3727 19.5226H33.533V14.3666H28.7188V23.6261H37.2505C44.2895 23.6261 49.0431 19.7718 49.0431 11.813C49.0431 3.85428 44.2895 0 37.2505 0Z\" fill=\"#2a5cff\"></path><path d=\"M38.5356 9.26789H33.5376V14.3666H38.5356V9.26789Z\" fill=\"#2a5cff\"></path></svg>\"\"\"\n",
    "                        )\n",
    "                        gr.Markdown(\n",
    "                            \"# Datatonic Analytics Assistant\",\n",
    "                            elem_classes=\"title right\",\n",
    "                        )\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    with gr.Tab(\"Ask a question:\"):\n",
    "        # Create a textbox for user questions\n",
    "        msg = gr.Textbox(show_label=False)\n",
    "\n",
    "        with gr.Row().style(equal_height=False):\n",
    "            with gr.Column(scale=3):\n",
    "                with gr.Row():\n",
    "                    send_message = gr.Button(\n",
    "                        value=\"Submit\", variant=\"primary\"\n",
    "                    ).style(size=\"sm\")\n",
    "                    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(question, chat_history):\n",
    "        bot_message, _ = create_sql_chain(question)\n",
    "        chat_history.append((question, bot_message))\n",
    "        time.sleep(2)\n",
    "        return bot_message, chat_history\n",
    "    \n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    send_message.click(\n",
    "                respond, [msg, chatbot], [msg, chatbot])\n",
    "    \n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task -> Use chatbot history for LLM query\n",
    "\n",
    "Notice how although we are using the `gr.Chatbot` component, the chat history is not being passed to the `LLMChain`, which means the model is not really a chatbot. If you pass the chat history, and try again, you noticed it did not work? That is because the assistant is only using the `SQLDatabaseSequentialChain`, meaning its only task is to generate SQL.  If a lot of chat history is passed to solve this, it will only confuse the assistant but not generate the correct history. \n",
    "\n",
    "The solution to the problem is:\n",
    "\n",
    "- Create prompt with history\n",
    "- Pass prompt with history into LLM chain \n",
    "- Pass the Bigquery tables the model has access to LLM chain \n",
    "- If the answer requires SQL, use the `SQLDatabaseSequentialChain`\n",
    "- If the answer does not require SQL, the assumption is the model already generated a response and should return that answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json \n",
    "\n",
    "table_names = [\"customers\",\"employees\",\"financial_goals\",\"inventory\",\"orders\",\"product_reviews\",\"supplier_orders\"]\n",
    "\n",
    "PROMPT_WITH_HISTORY = \"\"\"You are a analytics assistant that understands human natural language business related questions and are able to convert this into relevant SQL code or answer the questions provided the relevant chat history is available.\n",
    "The user will write a prompt asking for you to generate an answer based on a BigQuery table.\n",
    "\n",
    "You will receive as an input the user question, the past chat history and a list of table_names.\n",
    "\n",
    "You have two tasks.\n",
    "First, you need to understand if the user question and chat history requires you to generate SQL code or if you just need to answer the question directly.\n",
    "Secondly, if the user inputs do not require SQL code to be generated then you will need to answer the user question using any relevant information provided in the chat history.\n",
    "\n",
    "Output a JSON to identify if SQL is required and the direct answer if SQL is not required. The JSON schema format is:\n",
    "'{{\n",
    "    \"requires_sql\": boolean \\\\ Value can be either true or false\n",
    "    \"direct_answer\": string \\\\ NOTE: only include this attribute if SQL code is not required\n",
    "}}'\n",
    "\n",
    "REMEMBER: \"require_sql\" is a boolean therefore can only be true or false\n",
    "REMEMBER: if require_sql is false then answer using the information in the chat history and user question only. DO NOT MAKE UP INFORMATION. If you don't have the necessary information to provide an answer, return \"I don't know, please ask the question differently\".\n",
    "\n",
    "You have access to the following past chat history:\n",
    "\n",
    "<< CHAT HISTORY >>\n",
    "{chat_history}\n",
    "\n",
    "You have access to the following BigQuery Tables:\n",
    "\n",
    "<< BigQuery TABLES >>\n",
    "{table_info}\n",
    "\n",
    "The user query is:\n",
    "\n",
    "<< USER QUESTION >>\n",
    "{question}\n",
    "\n",
    "Based on the query and chat history, return the answer below in the following JSON format:\n",
    "'{{\n",
    "    \"requires_sql\": boolean \\\\ Value can be either true or false\n",
    "    \"direct_answer\": string \\\\ NOTE: only include this attribute if SQL code is not required\n",
    "}}'\n",
    "\n",
    "\n",
    "<< EXAMPLES >>\n",
    "Chat History: [\"Hello how can i help you today?\"]\n",
    "BigQuery Tables: ['customers','employees','financial_goals','inventory','orders','product_reviews','supplier_orders']\n",
    "User question: What is revenue in 2023?\n",
    "Output JSON:'{{\n",
    "    \"requires_sql\": true,\n",
    "    \"direct_answer\": \"\"\n",
    "}}'\n",
    "\n",
    "Chat History: [\"Revenue is down 20 points last month\"]\n",
    "BigQuery Tables: [customers,orders,employees,inventory,product_reviews,supplier_orders,financial_goal]\n",
    "User question: Is this month better than last?\n",
    "Output JSON:'{{\n",
    "    \"requires_sql\": false,\n",
    "    \"direct_answer\": \"No, this month is worse as revenue is down by 20 points\"\n",
    "}}'\n",
    "\n",
    "\n",
    "<< ANSWER >>\n",
    "\"\"\"\n",
    "\n",
    "def create_sql_chain_with_history(question: str, history:str, table_info: str = table_names, top_k:int=100, llm: VertexAI = llm, db=langchain_db):\n",
    "    \"\"\" Create a Q&A conversation chain using the VertexAI LLM.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    chat_prompt = PromptTemplate(template=PROMPT_WITH_HISTORY, input_variables=[\"question\", \"table_info\",\"chat_history\"])\n",
    "\n",
    "    chat_chain = LLMChain(\n",
    "            llm=llm, prompt=chat_prompt, output_key=\"output\")\n",
    "        \n",
    "    output = chat_chain.run(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"chat_history\": history,\n",
    "            \"table_info\": table_info\n",
    "        }\n",
    "    )\n",
    "\n",
    "    json_output = json.loads(output)\n",
    "    \n",
    "    if json_output[\"requires_sql\"] == True:\n",
    "        response, _ = create_sql_chain(question)\n",
    "    elif json_output[\"requires_sql\"] == False:\n",
    "        response = json_output[\"direct_answer\"]\n",
    "    else:\n",
    "        response = \"Sorry but I could not identify your question, please ask again\"\n",
    "\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how inside the `respond` function we are creating a chatbot history and append every question and answer to it. However, this history is not being used as it is not passed as an input to the `LLMChain`. \n",
    "\n",
    "Your task is to modify the chain so that it accepts as an input the history and you can properly chat with your data with a model that contains memory and remembers past questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add chat history to chain\n",
    "\n",
    "# Gradio chatbot and interface\n",
    "with gr.Blocks(title=\"Analytics Assistant\") as demo:\n",
    "\n",
    "    with gr.Row():\n",
    "                with gr.Column(scale=1, variant=\"panel\"):\n",
    "                    with gr.Row():\n",
    "                        # Load Datatonic logo as .svg\n",
    "                        gr.Markdown(\n",
    "                            \"\"\"\\\n",
    "<svg width=\"177\" height=\"24\" viewBox=\"0 0 177 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M14.5548 14.596H9.37748V9.404H0V14.596H9.37748V24H14.5548V14.596H23.9323V9.404H14.5548V14.596Z\" fill=\"#2a5cff\"></path><path d=\"M14.5548 0H9.37748V9.404H14.5548V0Z\" fill=\"#2a5cff\"></path><path d=\"M59.6168 6.3732C55.4116 6.3732 52.3637 8.42451 51.9375 11.8749H56.6305C56.8133 10.6942 57.7879 9.60525 59.6168 9.60525C61.5972 9.60525 62.5728 10.9415 62.5728 12.6518V12.931L58.2151 13.3658C54.9541 13.6769 51.3588 14.6403 51.3588 18.4955C51.3588 21.7594 53.918 23.9981 57.1183 23.9981C60.3186 23.9981 61.6891 22.4753 62.6334 20.454V23.6252H67.2345V12.9938C67.2345 9.20136 64.7663 6.37226 59.6168 6.37226V6.3732ZM62.5728 16.4451C62.5728 19.0877 61.3235 21.139 58.9462 21.139C57.3617 21.139 56.1124 20.1448 56.1124 18.4964C56.1124 16.5997 58.0323 16.2586 59.9218 16.0403L62.5728 15.7601V16.4451Z\" fill=\"#2a5cff\"></path><path d=\"M76.7895 10.2275H80.416V6.74523H76.7895V2.76725H72.1277V6.74616H69.0496V10.2284H72.1277V18.622C72.1277 21.9796 74.5656 23.627 77.8257 23.627H80.4463V20.1457H78.8315C77.4904 20.1457 76.7895 19.7109 76.7895 18.249V10.2275Z\" fill=\"#2a5cff\"></path><path d=\"M90.6838 6.3732C86.4786 6.3732 83.4308 8.42451 83.0046 11.8749H87.6975C87.8803 10.6942 88.8549 9.60525 90.6838 9.60525C92.6643 9.60525 93.6398 10.9415 93.6398 12.6518V12.931L89.2821 13.3658C86.0212 13.6769 82.4259 14.6403 82.4259 18.4955C82.4259 21.7594 84.985 23.9981 88.1853 23.9981C91.3856 23.9981 92.7561 22.4753 93.7004 20.454V23.6252H98.3016V12.9938C98.3016 9.20136 95.8333 6.37226 90.6838 6.37226V6.3732ZM93.6398 16.4451C93.6398 19.0877 92.3905 21.139 90.0133 21.139C88.4287 21.139 87.1795 20.1448 87.1795 18.4964C87.1795 16.5997 89.0993 16.2586 90.9888 16.0403L93.6398 15.7601V16.4451Z\" fill=\"#2a5cff\"></path><path d=\"M108.022 10.2275H111.648V6.74523H108.022V2.76725H103.36V6.74616H100.282V10.2284H103.36V18.622C103.36 21.9796 105.798 23.627 109.058 23.627H111.679V20.1457H110.064C108.723 20.1457 108.022 19.7109 108.022 18.249V10.2275Z\" fill=\"#2a5cff\"></path><path d=\"M121.926 6.3732C116.624 6.3732 113.303 10.0101 113.303 15.2016C113.303 20.3931 116.624 23.9991 121.926 23.9991C127.228 23.9991 130.55 20.3622 130.55 15.2016C130.55 10.041 127.228 6.3732 121.926 6.3732ZM121.926 20.7661C119.397 20.7661 118.056 18.6211 118.056 15.2016C118.056 11.7821 119.397 9.60618 121.926 9.60618C124.455 9.60618 125.796 11.7512 125.796 15.2016C125.796 18.652 124.455 20.7661 121.926 20.7661Z\" fill=\"#2a5cff\"></path><path d=\"M143.121 6.3732C140.226 6.3732 138.61 8.08246 137.849 10.1966V6.74616H133.217V23.6261H137.88V14.0199C137.88 11.0353 139.007 9.69896 140.927 9.69896C142.847 9.69896 143.974 11.0353 143.974 13.9581V23.6261H148.637V12.9319C148.637 9.20136 146.991 6.3732 143.121 6.3732H143.121Z\" fill=\"#2a5cff\"></path><path d=\"M157.046 6.74616H152.383V23.6261H157.046V6.74616Z\" fill=\"#2a5cff\"></path><path d=\"M157.137 0H152.323V4.47651H157.137V0Z\" fill=\"#2a5cff\"></path><path d=\"M172.107 17.6268C171.589 19.5853 170.492 20.767 168.572 20.767C166.165 20.767 164.763 18.7148 164.763 15.1716C164.763 11.6284 166.104 9.60712 168.572 9.60712C170.492 9.60712 171.559 11.0371 171.955 12.8092H176.647C175.947 8.86119 172.93 6.37414 168.572 6.37414C163.514 6.37414 160.009 9.88731 160.009 15.1716C160.009 20.4559 163.453 24 168.572 24C172.717 24 176.007 21.7613 176.8 17.6268H172.107Z\" fill=\"#2a5cff\"></path><path d=\"M37.2505 0H28.7188V9.26789H33.533V4.10355H37.3727C41.6082 4.10355 44.0764 6.83894 44.0764 11.813C44.0764 16.7872 41.6082 19.5226 37.3727 19.5226H33.533V14.3666H28.7188V23.6261H37.2505C44.2895 23.6261 49.0431 19.7718 49.0431 11.813C49.0431 3.85428 44.2895 0 37.2505 0Z\" fill=\"#2a5cff\"></path><path d=\"M38.5356 9.26789H33.5376V14.3666H38.5356V9.26789Z\" fill=\"#2a5cff\"></path></svg>\"\"\"\n",
    "                        )\n",
    "                        gr.Markdown(\n",
    "                            \"# Datatonic Analytics Assistant\",\n",
    "                            elem_classes=\"title right\",\n",
    "                        )\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    with gr.Tab(\"Ask a question:\"):\n",
    "        # Create a textbox for user questions\n",
    "        msg = gr.Textbox(show_label=False)\n",
    "\n",
    "        with gr.Row().style(equal_height=False):\n",
    "            with gr.Column(scale=3):\n",
    "                with gr.Row():\n",
    "                    send_message = gr.Button(\n",
    "                        value=\"Submit\", variant=\"primary\"\n",
    "                    ).style(size=\"sm\")\n",
    "                    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(question, chat_history):\n",
    "        bot_message = create_sql_chain_with_history(question=question, history=chat_history)\n",
    "        chat_history.append((question, bot_message))\n",
    "        time.sleep(2)\n",
    "        return bot_message, chat_history\n",
    "    \n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    send_message.click(\n",
    "                respond, [msg, chatbot], [msg, chatbot])\n",
    "    \n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create SQL Agent with ToolKit\n",
    "\n",
    "Now that we have a more capable LLM and Gradio interface implemented, can we make it better?\n",
    "\n",
    "One alternative to chains are agents. Whereas chains have a very well defined workflow with clear steps to follow and a start and end, LLM agents are given more freedom to try different actions and go back to the starting point and try again different methods. This is done by using the `REACT` framework and providing LLMs with toolkits. What does this mean? \n",
    "\n",
    "First, let's looka at toolkits. A toolkit is, as the word implies, an external capability that you provide to the LLM. A potential toolkit might be the Google Search API. If the LLM agent is given this toolkit, the LLM could access the Google Search API to search for information in Google. Another toolkit is the SQL toolkit (used here) which gives it acces to a SQL database. Other common toolkits are the python REPL (that allows the LLM to execute python code) or a calculator, to do mathematical calculations.\n",
    "\n",
    "Now, let's understand the `REACT` framework. This essentially is a way that defines how an LLM agent should \"think\". While an LLM agent is given access to a tool and given the freedom to use it, the `REACT` framework (which is essentially just a prompt) tells the model what steps it should follow to know if it should use one tool or another, or just return an answer. This works by ensuring the LLM always follows the same four steps until it reaches a final answer:\n",
    "[use example from docs here] \n",
    "\n",
    "As you can see here, this means LLM agents are more powerful than chains, as they can do more actions the more tools you give it, but at the same time can be harder to control. Whether you use a chain or an agent is a trade-off that depends on each use case.\n",
    "\n",
    "Let's see how to create a SQL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(llm, db, question):\n",
    "    # Define our agent’s toolkit which will be used to answer the user question\n",
    "    toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "    qa_chain = create_sql_agent(\n",
    "    llm=llm,\n",
    "    db=db, \n",
    "    toolkit=toolkit,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    early_stopping_method=\"generate\")\n",
    "\n",
    "    answer = qa_chain.run(question)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Define an sql agent with SQLToolKit\n",
    "def sql_agent(question):\n",
    "    response = create_agent(llm=llm, db=langchain_db, question=question)\n",
    "\n",
    "    return response[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_agent(\"how many items did I sell in January 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task -> Use the agent in the Gradio chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the chain with the agent in the Gradio code\n",
    "\n",
    "# Gradio chatbot and interface\n",
    "with gr.Blocks(title=\"Analytics Assistant\") as demo:\n",
    "\n",
    "    with gr.Row():\n",
    "                with gr.Column(scale=1, variant=\"panel\"):\n",
    "                    with gr.Row():\n",
    "                        # Load Datatonic logo as .svg\n",
    "                        gr.Markdown(\n",
    "                            \"\"\"\\\n",
    "<svg width=\"177\" height=\"24\" viewBox=\"0 0 177 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M14.5548 14.596H9.37748V9.404H0V14.596H9.37748V24H14.5548V14.596H23.9323V9.404H14.5548V14.596Z\" fill=\"#2a5cff\"></path><path d=\"M14.5548 0H9.37748V9.404H14.5548V0Z\" fill=\"#2a5cff\"></path><path d=\"M59.6168 6.3732C55.4116 6.3732 52.3637 8.42451 51.9375 11.8749H56.6305C56.8133 10.6942 57.7879 9.60525 59.6168 9.60525C61.5972 9.60525 62.5728 10.9415 62.5728 12.6518V12.931L58.2151 13.3658C54.9541 13.6769 51.3588 14.6403 51.3588 18.4955C51.3588 21.7594 53.918 23.9981 57.1183 23.9981C60.3186 23.9981 61.6891 22.4753 62.6334 20.454V23.6252H67.2345V12.9938C67.2345 9.20136 64.7663 6.37226 59.6168 6.37226V6.3732ZM62.5728 16.4451C62.5728 19.0877 61.3235 21.139 58.9462 21.139C57.3617 21.139 56.1124 20.1448 56.1124 18.4964C56.1124 16.5997 58.0323 16.2586 59.9218 16.0403L62.5728 15.7601V16.4451Z\" fill=\"#2a5cff\"></path><path d=\"M76.7895 10.2275H80.416V6.74523H76.7895V2.76725H72.1277V6.74616H69.0496V10.2284H72.1277V18.622C72.1277 21.9796 74.5656 23.627 77.8257 23.627H80.4463V20.1457H78.8315C77.4904 20.1457 76.7895 19.7109 76.7895 18.249V10.2275Z\" fill=\"#2a5cff\"></path><path d=\"M90.6838 6.3732C86.4786 6.3732 83.4308 8.42451 83.0046 11.8749H87.6975C87.8803 10.6942 88.8549 9.60525 90.6838 9.60525C92.6643 9.60525 93.6398 10.9415 93.6398 12.6518V12.931L89.2821 13.3658C86.0212 13.6769 82.4259 14.6403 82.4259 18.4955C82.4259 21.7594 84.985 23.9981 88.1853 23.9981C91.3856 23.9981 92.7561 22.4753 93.7004 20.454V23.6252H98.3016V12.9938C98.3016 9.20136 95.8333 6.37226 90.6838 6.37226V6.3732ZM93.6398 16.4451C93.6398 19.0877 92.3905 21.139 90.0133 21.139C88.4287 21.139 87.1795 20.1448 87.1795 18.4964C87.1795 16.5997 89.0993 16.2586 90.9888 16.0403L93.6398 15.7601V16.4451Z\" fill=\"#2a5cff\"></path><path d=\"M108.022 10.2275H111.648V6.74523H108.022V2.76725H103.36V6.74616H100.282V10.2284H103.36V18.622C103.36 21.9796 105.798 23.627 109.058 23.627H111.679V20.1457H110.064C108.723 20.1457 108.022 19.7109 108.022 18.249V10.2275Z\" fill=\"#2a5cff\"></path><path d=\"M121.926 6.3732C116.624 6.3732 113.303 10.0101 113.303 15.2016C113.303 20.3931 116.624 23.9991 121.926 23.9991C127.228 23.9991 130.55 20.3622 130.55 15.2016C130.55 10.041 127.228 6.3732 121.926 6.3732ZM121.926 20.7661C119.397 20.7661 118.056 18.6211 118.056 15.2016C118.056 11.7821 119.397 9.60618 121.926 9.60618C124.455 9.60618 125.796 11.7512 125.796 15.2016C125.796 18.652 124.455 20.7661 121.926 20.7661Z\" fill=\"#2a5cff\"></path><path d=\"M143.121 6.3732C140.226 6.3732 138.61 8.08246 137.849 10.1966V6.74616H133.217V23.6261H137.88V14.0199C137.88 11.0353 139.007 9.69896 140.927 9.69896C142.847 9.69896 143.974 11.0353 143.974 13.9581V23.6261H148.637V12.9319C148.637 9.20136 146.991 6.3732 143.121 6.3732H143.121Z\" fill=\"#2a5cff\"></path><path d=\"M157.046 6.74616H152.383V23.6261H157.046V6.74616Z\" fill=\"#2a5cff\"></path><path d=\"M157.137 0H152.323V4.47651H157.137V0Z\" fill=\"#2a5cff\"></path><path d=\"M172.107 17.6268C171.589 19.5853 170.492 20.767 168.572 20.767C166.165 20.767 164.763 18.7148 164.763 15.1716C164.763 11.6284 166.104 9.60712 168.572 9.60712C170.492 9.60712 171.559 11.0371 171.955 12.8092H176.647C175.947 8.86119 172.93 6.37414 168.572 6.37414C163.514 6.37414 160.009 9.88731 160.009 15.1716C160.009 20.4559 163.453 24 168.572 24C172.717 24 176.007 21.7613 176.8 17.6268H172.107Z\" fill=\"#2a5cff\"></path><path d=\"M37.2505 0H28.7188V9.26789H33.533V4.10355H37.3727C41.6082 4.10355 44.0764 6.83894 44.0764 11.813C44.0764 16.7872 41.6082 19.5226 37.3727 19.5226H33.533V14.3666H28.7188V23.6261H37.2505C44.2895 23.6261 49.0431 19.7718 49.0431 11.813C49.0431 3.85428 44.2895 0 37.2505 0Z\" fill=\"#2a5cff\"></path><path d=\"M38.5356 9.26789H33.5376V14.3666H38.5356V9.26789Z\" fill=\"#2a5cff\"></path></svg>\"\"\"\n",
    "                        )\n",
    "                        gr.Markdown(\n",
    "                            \"# Datatonic Analytics Assistant\",\n",
    "                            elem_classes=\"title right\",\n",
    "                        )\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    with gr.Tab(\"Ask a question:\"):\n",
    "        # Create a textbox for user questions\n",
    "        msg = gr.Textbox(show_label=False)\n",
    "\n",
    "        with gr.Row().style(equal_height=False):\n",
    "            with gr.Column(scale=3):\n",
    "                with gr.Row():\n",
    "                    send_message = gr.Button(\n",
    "                        value=\"Submit\", variant=\"primary\"\n",
    "                    ).style(size=\"sm\")\n",
    "                    clear = gr.ClearButton([msg, chatbot])\n",
    "                    \n",
    "    def respond(question, chat_history):\n",
    "        bot_message = sql_agent(question)\n",
    "        chat_history.append((question, bot_message))\n",
    "        time.sleep(2)\n",
    "        return bot_message, chat_history\n",
    "    \n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    send_message.click(\n",
    "                respond, [msg, chatbot], [msg, chatbot])\n",
    "    \n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task -> Add memory to the SQL Agent\n",
    "\n",
    "Hint: Follow this page https://python.langchain.com/docs/modules/memory/agent_with_memory_in_db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Track: Create Pandas Agent\n",
    "\n",
    "If you've reached this step, well done! \n",
    "\n",
    "Now that we have made an SQLAgent and a Gradio chatbot, we can go further and make a Data Analytics agent that is able to perform analysis and plot relevant charts using pandas. For this, we can create an agent that also uses a SQL Chain, loads the answer of the chain to a pandas dataframe and then uses the pandas toolkit to solve the user query.\n",
    "\n",
    "Try to implement it below.\n",
    "\n",
    "*Hint*: Have a look at the `create_pandas_dataframe_agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_agent(question,llm, df):\n",
    "    \n",
    "    agent = create_pandas_dataframe_agent(llm, df, verbose=True)\n",
    "    response = agent.run(question)\n",
    "    return response\n",
    "\n",
    "def pd_sql_agent(question):\n",
    "\n",
    "    answer, sql_query = create_sql_chain(llm=llm, db=langchain_db, question=question)\n",
    "    df = pd.read_sql(sql_query, conn, index_col=None)\n",
    " \n",
    "    response = pandas_agent(question=question, llm=llm, df=df)\n",
    "\n",
    "    # check if response is a plot, then render that in UI else return response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it out in Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio chatbot and interface\n",
    "with gr.Blocks(title=\"Analytics Assistant\") as demo:\n",
    "\n",
    "    with gr.Row():\n",
    "                with gr.Column(scale=1, variant=\"panel\"):\n",
    "                    with gr.Row():\n",
    "                        # Load Datatonic logo as .svg\n",
    "                        gr.Markdown(\n",
    "                            \"\"\"\\\n",
    "<svg width=\"177\" height=\"24\" viewBox=\"0 0 177 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M14.5548 14.596H9.37748V9.404H0V14.596H9.37748V24H14.5548V14.596H23.9323V9.404H14.5548V14.596Z\" fill=\"#2a5cff\"></path><path d=\"M14.5548 0H9.37748V9.404H14.5548V0Z\" fill=\"#2a5cff\"></path><path d=\"M59.6168 6.3732C55.4116 6.3732 52.3637 8.42451 51.9375 11.8749H56.6305C56.8133 10.6942 57.7879 9.60525 59.6168 9.60525C61.5972 9.60525 62.5728 10.9415 62.5728 12.6518V12.931L58.2151 13.3658C54.9541 13.6769 51.3588 14.6403 51.3588 18.4955C51.3588 21.7594 53.918 23.9981 57.1183 23.9981C60.3186 23.9981 61.6891 22.4753 62.6334 20.454V23.6252H67.2345V12.9938C67.2345 9.20136 64.7663 6.37226 59.6168 6.37226V6.3732ZM62.5728 16.4451C62.5728 19.0877 61.3235 21.139 58.9462 21.139C57.3617 21.139 56.1124 20.1448 56.1124 18.4964C56.1124 16.5997 58.0323 16.2586 59.9218 16.0403L62.5728 15.7601V16.4451Z\" fill=\"#2a5cff\"></path><path d=\"M76.7895 10.2275H80.416V6.74523H76.7895V2.76725H72.1277V6.74616H69.0496V10.2284H72.1277V18.622C72.1277 21.9796 74.5656 23.627 77.8257 23.627H80.4463V20.1457H78.8315C77.4904 20.1457 76.7895 19.7109 76.7895 18.249V10.2275Z\" fill=\"#2a5cff\"></path><path d=\"M90.6838 6.3732C86.4786 6.3732 83.4308 8.42451 83.0046 11.8749H87.6975C87.8803 10.6942 88.8549 9.60525 90.6838 9.60525C92.6643 9.60525 93.6398 10.9415 93.6398 12.6518V12.931L89.2821 13.3658C86.0212 13.6769 82.4259 14.6403 82.4259 18.4955C82.4259 21.7594 84.985 23.9981 88.1853 23.9981C91.3856 23.9981 92.7561 22.4753 93.7004 20.454V23.6252H98.3016V12.9938C98.3016 9.20136 95.8333 6.37226 90.6838 6.37226V6.3732ZM93.6398 16.4451C93.6398 19.0877 92.3905 21.139 90.0133 21.139C88.4287 21.139 87.1795 20.1448 87.1795 18.4964C87.1795 16.5997 89.0993 16.2586 90.9888 16.0403L93.6398 15.7601V16.4451Z\" fill=\"#2a5cff\"></path><path d=\"M108.022 10.2275H111.648V6.74523H108.022V2.76725H103.36V6.74616H100.282V10.2284H103.36V18.622C103.36 21.9796 105.798 23.627 109.058 23.627H111.679V20.1457H110.064C108.723 20.1457 108.022 19.7109 108.022 18.249V10.2275Z\" fill=\"#2a5cff\"></path><path d=\"M121.926 6.3732C116.624 6.3732 113.303 10.0101 113.303 15.2016C113.303 20.3931 116.624 23.9991 121.926 23.9991C127.228 23.9991 130.55 20.3622 130.55 15.2016C130.55 10.041 127.228 6.3732 121.926 6.3732ZM121.926 20.7661C119.397 20.7661 118.056 18.6211 118.056 15.2016C118.056 11.7821 119.397 9.60618 121.926 9.60618C124.455 9.60618 125.796 11.7512 125.796 15.2016C125.796 18.652 124.455 20.7661 121.926 20.7661Z\" fill=\"#2a5cff\"></path><path d=\"M143.121 6.3732C140.226 6.3732 138.61 8.08246 137.849 10.1966V6.74616H133.217V23.6261H137.88V14.0199C137.88 11.0353 139.007 9.69896 140.927 9.69896C142.847 9.69896 143.974 11.0353 143.974 13.9581V23.6261H148.637V12.9319C148.637 9.20136 146.991 6.3732 143.121 6.3732H143.121Z\" fill=\"#2a5cff\"></path><path d=\"M157.046 6.74616H152.383V23.6261H157.046V6.74616Z\" fill=\"#2a5cff\"></path><path d=\"M157.137 0H152.323V4.47651H157.137V0Z\" fill=\"#2a5cff\"></path><path d=\"M172.107 17.6268C171.589 19.5853 170.492 20.767 168.572 20.767C166.165 20.767 164.763 18.7148 164.763 15.1716C164.763 11.6284 166.104 9.60712 168.572 9.60712C170.492 9.60712 171.559 11.0371 171.955 12.8092H176.647C175.947 8.86119 172.93 6.37414 168.572 6.37414C163.514 6.37414 160.009 9.88731 160.009 15.1716C160.009 20.4559 163.453 24 168.572 24C172.717 24 176.007 21.7613 176.8 17.6268H172.107Z\" fill=\"#2a5cff\"></path><path d=\"M37.2505 0H28.7188V9.26789H33.533V4.10355H37.3727C41.6082 4.10355 44.0764 6.83894 44.0764 11.813C44.0764 16.7872 41.6082 19.5226 37.3727 19.5226H33.533V14.3666H28.7188V23.6261H37.2505C44.2895 23.6261 49.0431 19.7718 49.0431 11.813C49.0431 3.85428 44.2895 0 37.2505 0Z\" fill=\"#2a5cff\"></path><path d=\"M38.5356 9.26789H33.5376V14.3666H38.5356V9.26789Z\" fill=\"#2a5cff\"></path></svg>\"\"\"\n",
    "                        )\n",
    "                        gr.Markdown(\n",
    "                            \"# Datatonic Analytics Assistant\",\n",
    "                            elem_classes=\"title right\",\n",
    "                        )\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "\n",
    "    with gr.Tab(\"Ask a question:\"):\n",
    "        # Create a textbox for user questions\n",
    "        msg = gr.Textbox(show_label=False)\n",
    "\n",
    "        with gr.Row().style(equal_height=False):\n",
    "            with gr.Column(scale=3):\n",
    "                with gr.Row():\n",
    "                    send_message = gr.Button(\n",
    "                        value=\"Submit\", variant=\"primary\"\n",
    "                    ).style(size=\"sm\")\n",
    "                    clear = gr.ClearButton([msg, chatbot])\n",
    "                    \n",
    "    def respond(question, chat_history):\n",
    "        bot_message = pd_sql_agent(question)\n",
    "        chat_history.append((question, bot_message))\n",
    "        time.sleep(2)\n",
    "        return bot_message, chat_history\n",
    "    \n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    send_message.click(\n",
    "                respond, [msg, chatbot], [msg, chatbot])\n",
    "    \n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ea5ffafec065a5b064a6fab7872630ad2c79ca3c7ae1da3bf097c85297990f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
